{
  "hash": "5644f72784bff390716deb0d309044c4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sampling from the poor man's posterior distribution of parameters from models fitted to claims data\"\ndate: \"2025-01-05\"\ndate-modified: \"2025-01-08\"\ncategories: [Bayesian, Bootstrap, GLM, Insurance, R, Tweedie]\nimage: \"tweedie_bootstrap_plot.png\"\ncode-fold: true\n---\n\n\nThis post is about fitting a handful of different models to a subset of a popular car-insurance claims data available online. Plausible future data values can be simulated from the fitted models and used for downstream tasks. Simulated values from the fitted models can also be compared with the actual observed data as a sanity check.\n\nI've never worked in the field of insurance, but I've been wanting to dive into Tweedie models for a while, since non-negative (response) variables with lots of zeroes and positive skew are pretty common and show up in many diverse disciplines such across the social sciences, insurance, biology, etc.\n\nWhat piqued my interest in insurance data is that the response variable can additionally have very large \"outliers\". In other fields, outliers resulting from corrupted data or measurement errors can be discarded from the analysis, or robust estimators / loss functions can be used for modelling. However, in such settings, it may not necessarily make sense to discard such values because they likely represent the true cost of damages from accidents[^1] — which an insurer may be on the hook for.\n\n[^1]: \"Well actually, ...\" - some actuary probably\n\nThis is also a bit of an unusual post in the sense that I'm using ideas I've encountered in Bayesian statistics but with frequentist methods. Sure, I could just fit Bayesian models, but that's not the point here. To wrap my head around the ideas utilized in this post, I'm keeping things relatively simple by eschewing more complex predictive models like (my favourite) boosted trees, and using a single model for the data instead of *hurdle* models that split the response variable into a zero and a non-zero part and model them separately.\n\nBefore going further, I'm going to load some packages and the claims data used for this post.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\n# functions from these packages are used via namespace::fun() \n# library(tweedie)\n# library(magrittr, include.only = \"%$%\") # importing the exposition pipe\n# library(glue)\n# library(scales)\n# library(broom)\n# library(boot)\n# library(ggExtra)\n# library(statmod)\n\ntheme_set(theme_bw())\n\nclaims <- read_csv(\"claims_subset.csv\", show_col_types = FALSE) %>%\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 60,000\nColumns: 13\n$ IDpol       <dbl> 1, 3, 5, 10, 11, 13, 15, 17, 18, 21, 25, 27, 30, 32, 35, 3…\n$ ClaimNb     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Exposure    <dbl> 0.10, 0.77, 0.75, 0.09, 0.84, 0.52, 0.45, 0.27, 0.71, 0.15…\n$ Area        <chr> \"D\", \"D\", \"B\", \"B\", \"B\", \"E\", \"E\", \"C\", \"C\", \"B\", \"B\", \"C\"…\n$ VehPower    <dbl> 5, 5, 6, 7, 7, 6, 6, 7, 7, 7, 7, 7, 4, 4, 4, 9, 6, 6, 6, 6…\n$ VehAge      <dbl> 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 1, 0, 9, 0, 2, 2, 2, 2…\n$ DrivAge     <dbl> 55, 55, 52, 46, 46, 38, 38, 33, 33, 41, 41, 56, 27, 27, 23…\n$ BonusMalus  <dbl> 50, 50, 50, 50, 50, 50, 50, 68, 68, 50, 50, 50, 90, 90, 10…\n$ VehBrand    <chr> \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B…\n$ VehGas      <chr> \"Regular\", \"Regular\", \"Diesel\", \"Diesel\", \"Diesel\", \"Regul…\n$ Density     <dbl> 1217, 1217, 54, 76, 76, 3003, 3003, 137, 137, 60, 60, 173,…\n$ Region      <chr> \"R82\", \"R82\", \"R22\", \"R72\", \"R72\", \"R31\", \"R31\", \"R91\", \"R…\n$ ClaimAmount <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n```\n\n\n:::\n:::\n\n\nThe most important variables are `IDpol` — which uniquely identifies individual policies (denoted with a subscript $i$), `Exposure` — which indicates the duration (in years) that the policy is in effect (denoted by $w_i$), and `ClaimAmount` — which is the total monetary amount of the claims filed by each policyholder (denoted by $z_i$). Some of the other variables will be later used for building richer models.\n\nLooking at the deciles of the exposure distribution for these 60,000 policies\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nclaims %>% \n  pull(Exposure) %>% \n  quantile(., probs = seq(0, 1, 0.1)) %>% \n  as_tibble(rownames = \"percentile\") %>% \n  print(n = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 × 2\n   percentile   value\n   <chr>        <dbl>\n 1 0%         0.00273\n 2 10%        0.07   \n 3 20%        0.16   \n 4 30%        0.27   \n 5 40%        0.42   \n 6 50%        0.57   \n 7 60%        0.76   \n 8 70%        1      \n 9 80%        1      \n10 90%        1      \n11 100%       1      \n```\n\n\n:::\n:::\n\n\nmany of the values are less than one, which means those policies were in effect for less than a year. All of these are non-zero as they should be, but more than 60% have a duration of less than one year. The smallest exposure is for a day, since $1/365 \\approx 0.02739$.\n\nAssuming a closed cohort — i.e., all these contracts get renewed the following year and no new contracts are issued — the goal is to predict the total claim amount for each policy for the following year. The distribution of individual claim amounts is highly skewed and has *a lot* of zeroes ($\\approx 94\\%$), as assessed by some quantiles\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nclaims %>% \n  pull(ClaimAmount) %>% \n  quantile(., probs = c(0, 0.5, 0.94, 0.95, 0.99, 0.999, 1.0)) %>% \n  round()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     0%     50%     94%     95%     99%   99.9%    100% \n      0       0       0      54    1636   10839 1404186 \n```\n\n\n:::\n:::\n\n\n## The simplest \"model\"\n\nSince I've worked in epidemiology these past few years, a natural quantity (i.e., estimand) similar to incidence rates (i.e., the number of events generated from some population divided by the total follow-up time for individuals with different follow-up durations or periods) seems to be a good starting step. This can be expressed as the sum of the individual claim amounts $z_i$ divided by the sum of policy durations $w_i$, and denoted by the expectation operator $\\mathbb{E}$.\n\n$$\n\\mathbb{E}\\big[\\text{Claim Amount}_i\\big] = \\frac{\\sum_i z_i}{\\sum_i w_i}\n$$It's a way of equally dividing the total claims generated from some population at risk among the individuals in that population. Each of the individual claim amounts is a random variable, so is expected to vary from person to person, as well as from year to year for the same person.\n\nTaking the regular (arithmetic) mean — which is the same as setting $w_i = 1$ for each individual in the formula above — underestimates the expected claim cost as many individuals are observed for less than a year. It is expected that had they been observed for the full year, their claim amounts would have been larger. A similar argument applies in the case of individuals with $w_i > 1$, although in this case we'd be overestimating instead of underestimating.\n\nFor the data above, this comes out to about 125 per person per year assuming $w_i = 1$, and to about 218 per person per year using the observed $w_i$'s.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nclaims %>% \n  summarise(\n    total_amount = sum(ClaimAmount), \n    max_claim_amount = max(ClaimAmount),\n    n_policies = n(), \n    person_time = sum(Exposure), \n    mean_person_time = mean(Exposure)\n  ) %>% \n  mutate(\n    mean = total_amount / n_policies, \n    avg_claim_cost = total_amount / person_time,\n    across(.cols = everything(), \n           .fns = ~ as.character(round(.x, 4)))\n  ) %>% \n  pivot_longer(cols = everything())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 2\n  name             value     \n  <chr>            <chr>     \n1 total_amount     7507466.02\n2 max_claim_amount 1404185.52\n3 n_policies       60000     \n4 person_time      34471.8546\n5 mean_person_time 0.5745    \n6 mean             125.1244  \n7 avg_claim_cost   217.7854  \n```\n\n\n:::\n:::\n\n\nSo now we have an estimate for the expected claim amount per person per year, even though we can expect most policies to generate zero claims, and a few policies to generate some very large claim amounts based on the observed claims distribution. This expected claim cost estimate will be used again, so it's assigned to a variable here.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nexpected_claim_amount <- sum(claims$ClaimAmount) / sum(claims$Exposure)\n```\n:::\n\n\nFor an insurance company, they need to ensure they have enough capital reserves to be able to pay out money for these claims, which they're legally liable for. For the current year, the total claim amount across the policies was about 7.5 million, from a population that was on averaged exposed for 0.57 years. The total expected claim amount for this population with each person being observed for a full year is $217.8 \\times 60,000$ which comes out to about 13 million.\n\nIn this case, the sample of customers constitutes the population of interest so it may not make sense to produce uncertainty estimates for this estimated total amount for the current year. However, the claim amounts can be seen as the result of a stochastic process, so superpopulation inference on this parameter can still make sense since next year's claim amounts may be expected to be similar, but not identical.\n\nThis is carried out here using a [weird Bayesian model](https://www.sumsar.net/blog/2015/04/the-non-parametric-bootstrap-as-a-bayesian-model/) (a.k.a. the nonparametric bootstrap). The following code chunk generates $B = 10,000$ resamples and calculates the expected claim amount on each resample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpected_claim_cost_fun <- function(data, indx, ...) {\n  data <- data[indx, ]\n\n  expected_value <- sum(data$ClaimAmount) / sum(data$Exposure)\n\n  # this is the single / largest outlier in the data\n  outlier_counts <- nrow(data[data$ClaimAmount == 1404185.52, ])\n\n  return(c(expected_value, outlier_counts))\n}\n\nboot_fun <- function(data, R = 100, parallel = \"snow\") {\n  stopifnot(parallel %in% c(\"no\", \"snow\"))\n\n  # TRUE if using parallelization, otherwise FALSE\n  simple <- parallel == \"snow\"\n\n  boot::boot(\n    data = data,\n    statistic = expected_claim_cost_fun,\n    R = R,\n    sim = \"ordinary\",\n    stype = \"i\",\n    simple = simple,\n    parallel = parallel,\n    ncpus = 18\n  )\n}\n\n# uncomment to run\n# boot_fit <- boot_fun(data = claims, R = 10000)\n\n# uncomment to save the results \n# saveRDS(boot_fit, file = \"bootstrap_expected_claim_cost.rds\")\n```\n:::\n\n\nThis can take a while to run (even in parallel), so saved results are read back in and used to produce the following plot of the sampling distribution of the expected claim amount.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboot_fit <- readRDS(\"bootstrap_expected_claim_cost.rds\")\n\n# convert the results into a data frame\nboot_dist <- tibble(\n  expected_claim_cost = boot_fit$t[, 1],\n  outlier_counts = boot_fit$t[, 2],\n  # used for coloring / facetting plots\n  #`Outlier counts` = paste0(boot_fit$t[, 2], \" replicates\")\n  `Outlier counts` = factor(boot_fit$t[, 2])\n)\n\nboot_dist %>%\n  ggplot(aes(x = expected_claim_cost)) +\n  geom_histogram(bins = 100) +\n  geom_vline(\n    xintercept = expected_claim_amount, \n    color = \"orange\", linewidth = 1.2, linetype = \"dashed\"\n  ) +\n  xlab(\n    glue::glue(\"Bootstrap distribution of expected \", \n               \"exposure-adjusted claim amount \", \n               \"per person per year\", \n               \"\\n(Estimate from the full sample in orange)\")\n  ) +\n  ylab(\"Count\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThat's an unusual looking bootstrap distribution. Googling led me to [this stackexchange thread](https://stats.stackexchange.com/questions/63999/how-to-interpret-multimodal-distribution-of-bootstrapped-correlation) which indicates similar behaviour arising due to outlier(s) and small sample sizes. Policies with the largest top-6 claim amounts are\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclaims %>% \n  select(ClaimAmount) %>% \n  arrange(desc(ClaimAmount)) %>% \n  slice_head(n = 6) %>% \n  pull(ClaimAmount)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1404185.52  183073.66  152666.39  116318.90  115232.88   96422.32\n```\n\n\n:::\n:::\n\n\nof which the largest at 1.4 million is roughly 8 times larger than the next largest value. It is interesting that despite having a large sample size of 60,000 (or 34,500 policy-years), this outlier is large enough relative to the sample size to cause the multimodality seen here.\n\nThe function used to calculate the expected claim amount on each bootstrap sample can be modified to also count the number of times the maximum value from the original sample shows up in each bootstrap sample. The bootstrap distribution is plotted again but this time colored by the number of times the maximum amount shows up in a replicate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboot_dist %>%\n  ggplot(aes(x = expected_claim_cost,\n             fill = `Outlier counts`)) +\n  geom_histogram(bins = 100) +\n  geom_vline(xintercept = expected_claim_amount, color = \"gray40\",\n             linewidth = 1.2, linetype = \"dashed\") +\n  xlab(\n    glue::glue(\"Bootstrap distribution of expected \", \n               \"exposure-adjusted claim amount \", \n               \"per person per year\", \n               \"\\n(Estimate from the full sample in gray)\")\n  ) +\n  ylab(\"Count\") + \n  theme(\n    legend.position = \"inside\",\n    #legend.background = element_blank(),\n    legend.position.inside = c(0.88, 0.65)\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nSo this makes sense. Resamples with more repeats of the maximum claim amount have higher expected claim cost amounts.\n\nSince the total claim amount for the following year is of interest, the bootstrap distribution can be multiplied by the number of policy holders to get a distribution of plausible values for the total claim amount for this population — which is expected to be around 13 million, but could be anywhere between 8 million to 28 million.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnonparametric_bootstrap_totals <- boot_dist %>%\n  mutate(\n    total_claim_amount = 60000 * expected_claim_cost,\n    total_claim_amount_in_millions = total_claim_amount / 1e6,\n    method = \"Nonparametric Bootstrap (Weighted mean)\"\n  ) %>%\n  select(method, total_claim_amount_in_millions)\n\nnonparametric_bootstrap_totals %>%\n  ggplot(aes(x = total_claim_amount_in_millions)) +\n  stat_ecdf(pad = FALSE) +\n  geom_vline(xintercept = (6e4 * expected_claim_amount) / 1e6,\n             color = \"orange\", linewidth = 1.2, linetype = \"dashed\") +\n  scale_x_continuous(breaks = seq(8, 30, 2)) +\n  scale_y_continuous(labels = scales::label_percent()) +\n  xlab(\n    glue::glue(\"Plausible values for the \", \n                \"following year's\\ntotal claim \", \n                \"amount assuming unit exposure (in millions)\", \n               \"\\n(Estimate from the full sample in orange)\")\n  ) +\n  ylab(\n    glue::glue(\"Empirical distribution function of the\\n\", \n               \"bootstrap distribution of total claim amounts\")\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThe empirical (cumulative) distribution function (EDF or eCDF) for the bootstrap distribution of the total claim amount from the population under unit exposure is shown above. The strong multimodality shows up as bumps in the eCDF but these seem pretty minor.\n\nThis approach ignores all the predictors in the data, and assumes that everyone has the same risk and spreads out that risk across every individual equally. However, it's expected that the risk may differ across factors such as driver's age, car model characteristics, region, etc. For this, we need more advanced approaches in the form of regression models.\n\n## Tweedie models\n\nA common probability model used to model claims data — the [Tweedie distribution](https://en.wikipedia.org/wiki/Tweedie_distribution) — is characterized by the power-law mean-variance relationship $\\text{Var}[Y_i] \\propto \\mathbb{E}[Y_i] ^ p$ where the variance $\\text{Var}[Y_i]$ is proportional to the mean $\\mathbb{E}[Y_i]$. The Tweedie distribution with $p \\in (1, 2)$ — also known as the compound Poisson(-gamma) distribution — is commonly used to model the total cost of claims from an insurance policy. In the insurance world, it can be derived at the policy level as a model of Poisson claim frequency with the sum of claim amounts (i.e., claim severity) coming from a gamma distribution.\n\nThe Tweedie distribution has three parameters — mean ($\\mu > 0$), dispersion ($\\sigma > 0$) which functions as the constant of proportionality, and variance function power $p \\in (-\\infty, 0] \\cup [1, \\infty)$. Restricting to $p \\in (1, 2)$ make sense here since we have policies with zero claim amounts in the data and $p \\notin(1, 2)$ is not suitable for modelling data with exact zeroes. The extreme of $p = 1$ corresponds to a Poisson distribution (which has support on non-negative integers), and $p = 2$ corresponds to a gamma distribution (which has support on positive real numbers). Special cases are also the Gaussian distribution ($p = 0$), and the inverse-Gaussian distribution ($p = 3$).\n\nSection 4 of [this paper](https://arxiv.org/abs/1508.06378) has the clearest distinction I've seen between three related random variables for the $i^{th}$ policy — $Z_i$ is the observed total claim amount with exposure $w_i$ (we have $z_i$'s in our data), $Y_i = Z_i / w_i \\sim \\text{Tweedie}(\\mu_i, \\phi / w_i, p)$ is a derived quantity known as the *pure premium* under exposure $w_i$, and $Y^*_i \\sim \\text{Tweedie}(\\mu_i, \\phi, p)$ is the pure premium under unit exposure (so $w_i = 1$) which satisfies the mean-variance relationship $\\text{Var}[Y_i^*] = \\phi \\mathbb{E}[Y_i^*]^p$. I'm treating $\\phi$ as a constant here, but it can be modelled as a function of covariates, as is done in [this paper](https://link.springer.com/article/10.1007/s13385-021-00264-3) for example.\n\nWhat tripped me up while writing a [previous post](../tweedie-with-identity-link-and-offset/index.qmd)[^2] on this topic was that I kept trying to take the weighted mean of the $z_i$ values instead of the $y_i$ values and getting different results compared with the ratio $\\sum_i z_i / \\sum_i w_i$ when they should've been identical. Taking the weighted mean of the $y_i$ values leads to the same estimate as the ratio of these sums because the $w_i$'s cancel out in the numerator.\n\n[^2]: To be fair, I'd read the Yang et al. paper for that post too, but had overlooked that (subtle?) distinction.\n\n### Intercept-only regression\n\nThere are two ways of accounting for the varying exposures in a model. The first method uses pure premium $y_i$ as the response variable and the exposure $w_i$'s are passed as weights to the fitting functions. The second method uses the observed amount $z_i$ with $w_i$ as an offset (i.e., a variable in the model with a fixed coefficient of 1). For the Tweedie model with $p \\in (1, 2)$, these two methods result in different parameter estimates (compared to the Poisson regression case where they give the same estimates). I'm sticking with the first approach here since more modelling packages across languages support model weights compared with offsets.\n\nThe following code fits an intercept-only Tweedie *generalized linear model* (GLM) with pure premium as the response variable and exposure as weights, and uses an identity link (`link.power = 1`). For this simple model, the link function shouldn't really matter for the parameter estimate for the mean $\\mu$. The variance power is taken to be $p = 1.6$ throughout. This is very close to the chosen value for $p$ from calling `tweedie::tweedie.profile()` using the model offset formulation described in the previous paragraph. I couldn't get this function to work for the pure premium model with weights because the log-likelihood estimates were (negative) infinite for all $p$, and $p = 1.8$ was the value that minimized the Tweedie deviance (including on simulated data with known $\\mu$, $\\phi$, and $p = 1.6$).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntweedie_intercept_only <- glm(\n  I(ClaimAmount / Exposure) ~ 1,\n  weights = Exposure,\n  data = claims,\n  # using link.power = 1 implies identity link for the mean parameter mu\n  family = statmod::tweedie(var.power = 1.6, link.power = 1)\n)\n\nsummary(tweedie_intercept_only)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = I(ClaimAmount/Exposure) ~ 1, family = statmod::tweedie(var.power = 1.6, \n    link.power = 1), data = claims, weights = Exposure)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    217.8       64.4   3.382 0.000721 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 25966.84)\n\n    Null deviance: 2144044  on 59999  degrees of freedom\nResidual deviance: 2144044  on 59999  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n```\n\n\n:::\n:::\n\n\nSo our best guess for the distribution of $Y_i^*$'s from the Tweedie class of models is $\\text{Tweedie}(\\mu = 217.78..., \\phi = 25966.83..., p = 1.6...)$ (where the ellipses indicate truncation). These estimates for $\\hat\\mu_{\\text{int}}$ and $\\hat\\phi_{\\text{int}}$ will be used again, so they're stored as variables.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmu <- coef(tweedie_intercept_only)\nphi <- tweedie_intercept_only %>% \n  summary() %>% \n  pluck(\"dispersion\")\n```\n:::\n\n\n### Parametric Bootstrap\n\nOne way of getting a distribution for the statistic of interest $T$ is the use of the parametric bootstrap. This involves drawing $B$ samples each of size $n$ from the fitted model, computing the statistic for each of the $B$ samples, and using this as an estimate of the sampling distribution from which summary statistics (e.g. mean, quantiles, sd, etc.) can be computed.\n\nThe observed statistics — such as the number of claims with amounts \\> 0, the total claim amounts, and the largest claim amount — from the one sample of $z_i$'s that we have can be compared with the sampling distribution from the parametric bootstrap using the observed exposures $w_i$.\n\nThe following code carries this out with $B = 10,000$ where for each $b$ we draw a vector of $n$ claim amounts $Z_{i, b}$ and the statistic $\\hat{T_b}$ is computed. $Z_{i, b}$ is drawn as $Z_{i, b} \\sim w_i \\times \\text{Tweedie}(\\hat\\mu_{\\text{int}}, \\hat\\phi_{\\text{int}} / w_i, 1.6)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how well does the model fit the data?\n# a form of posterior predictive checking\n# ppc_obs_exposure <- map(\n#   .x = 1:10000,\n#   .f = ~ {\n#     if(.x %% 100 == 0) {\n#       print(.x)\n#     }\n#     set.seed(.x)\n#     draw <- claims$Exposure * tweedie::rtweedie(60000, mu = mu,\n#                                                 phi = phi / claims$Exposure,\n#                                                 power = 1.6)\n#     tibble(prop_zero = mean(draw == 0), sample_total = sum(draw),\n#            sample_max = max(draw), n_nonzero = sum(draw > 0))\n#   }) %>%\n#   list_rbind()\n#\n# saveRDS(ppc_obs_exposure, file = \"ppc_obs_exposure.rds\")\n\n# code takes a while to run, so we can read in the saved results\nppc_obs_exposure <- readRDS(file = \"ppc_obs_exposure.rds\")\n\n# summary statistics on the observed data\nsample_statistics_obs_exposure <- claims %>%\n  summarise(\n    prop_zero = mean(ClaimAmount == 0),\n    sample_total = sum(ClaimAmount),\n    sample_max = max(ClaimAmount),\n    n_nonzero = sum(ClaimAmount > 0)\n  )\n\n# combine the ppc data and the original sample data\nplot_data_obs_exposure <- bind_rows(\n  ppc_obs_exposure %>% \n    mutate(group = \"sampled\", .before = 0),\n  sample_statistics_obs_exposure %>% \n    mutate(group = \"observed\", .before = 0)\n  ) %>%\n  mutate(\n    across(c(sample_total, sample_max), ~ .x / 1e6),\n    prop_zero = 100 * prop_zero\n  ) %>%\n  rename(\n    `% of policies with zero claims` = prop_zero,\n    `Number of policies with non zero claim amounts` = n_nonzero,\n    `Maximum claim amount (in millions)` = sample_max,\n    `Total claim amount (in millions)` = sample_total\n  ) %>%\n  pivot_longer(cols = -group, names_to = \"statistic\", values_to = \"values\")\n\n# mean of the posterior predictive distribution values\nppc_mean_obs_exposure <- plot_data_obs_exposure %>%\n  filter(group == \"sampled\") %>%\n  summarise(values = mean(values), .by = statistic) %>%\n  mutate(across(.cols = where(is.numeric), \n                .fns = ~ round(.x, 2)))\n\n# compare these visually\nplot_data_obs_exposure %>%\n  filter(group == \"sampled\") %>%\n  ggplot(aes(x = values, group = statistic)) +\n  stat_ecdf(pad = FALSE) +\n  # plot the sample statistic\n  geom_vline(data = filter(plot_data_obs_exposure,\n                           group == \"observed\"),\n             aes(xintercept = values, group = statistic),\n             color = \"orange\", linewidth = 1.2, linetype = \"dashed\") +\n  # plot the distribution means\n  geom_vline(data = ppc_mean_obs_exposure,\n             aes(xintercept = values, group = statistic),\n             color = \"red4\", linewidth = 1.2, linetype = \"dashed\") +\n  facet_wrap(~ statistic, scales = \"free\") +\n  scale_y_continuous(labels = scales::label_percent()) +\n  xlab(glue::glue(\"Statistics from the observed data (in orange);\",\n                  \"\\nmean of means from the simulated datasets (in red)\")) +\n  ylab(\"Empirical distribution function\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nWhat's really interesting about this plot is that the proportion (and number) of policies with zero claim amounts is really far from the observed proportion (number) of 95% (3000) vs the mean of the sampling distribution (99% and 29). On the other hand, the observed and average total claim amounts are virtually indistinguishable, and the average maximum claim amount isn't too far off from the observed maximum claim amount.\n\nGiven that the statistic of interest has been the total claim amount across the policies, this doesn't seem to be a bad approach, even though the EDFs of the sampled $Z_{i, b} > 0$'s from the model (10 datasets shown in gray) look pretty far off from the observed distribution of $Z_{i, \\text{obs}} > 0$'s (observed claim amounts in black)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot eCDFs of sampled datasets with the observed data\nmap(\n  .x = 1:10,\n  .f = ~ {\n    set.seed(.x)\n    draw <- claims$Exposure * tweedie::rtweedie(60000, mu = mu,\n                                                phi = phi / claims$Exposure,\n                                                power = 1.6)\n    tibble(sim_id = .x, y = draw, grp = \"Simulated\")\n  }) %>%\n  list_rbind() %>%\n  bind_rows(\n    .,\n    claims %>%\n      mutate(sim_id = 100, grp = \"Observed\") %>%\n      select(sim_id, y = ClaimAmount, grp)\n  ) %>%\n  filter(y > 0) %>%\n  ggplot(aes(x = y, group = sim_id, color = grp)) +\n  stat_ecdf() +\n  xlab(\"Policies with non-zero claim amounts (log10 scale)\") +\n  ylab(\"Empirical distribution function\") +\n  scale_x_log10(\n    breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n    labels = scales::trans_format(\"log10\", scales::math_format(10^.x))\n  ) +\n  annotation_logticks(sides = \"b\") +\n  scale_y_continuous(labels = scales::percent) +\n  scale_color_manual(\n    values = c(\"Simulated\" = \"gray70\", \"Observed\" = \"Black\")\n  ) +\n  theme(\n    legend.title = element_blank(), \n    legend.position = \"inside\",\n    legend.position.inside = c(0.9, 0.2)\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nSimulated claim amounts are orders of magnitude larger than the observed claim amounts, which is how the totals are very similar despite the very different sample sizes of policies with non-zero claim amounts. The very low number of non-zero claim amounts in the simulated datasets is what leads to the jaggedness of the distribution functions.\n\nIf the goal is to have accurate estimates for the proportion of non-zero claims, then a hurdle model mentioned earlier would be a better approach. The observed distribution has a pretty big jump at 1128.12 which shows up 1169 times in the data, so a mixture model might be more appropriate if want to have a model with approximately the same eCDF.\n\nThis approach of comparing the statistics on samples drawn from the model vs the statistics from the observed sample is the concept of [*posterior predictive checking*](https://mc-stan.org/docs/stan-users-guide/posterior-predictive-checks.html) (PPC). If the model adequately describes the data generating process, the distribution of all the statistics from sampled datasets should be approximately centered at the observed statistics.\n\nFor next year's claim amount, we can repeat the same step with a slight modification — sample $Y_{i, b}^* \\sim \\text{Tweedie}(\\hat\\mu_{\\text{int}}, \\hat\\phi_{\\text{int}}, p = 1.6)$ and calculate $\\hat{T_b} = \\sum_i Y_{i, b}^*$ which ranges from about 4 to 27.5 million.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# predicted_totals_for_unit_exposure <- map_dbl(.x = 1:10000, .f = ~ {\n#   if(.x %% 100 == 0) {\n#     print(.x)\n#   }\n#   set.seed(.x)\n#   sum(tweedie::rtweedie(60000, mu = mu, phi = phi, power = 1.6))\n# })\n#\n# saveRDS(predicted_totals_for_unit_exposure,\n#         file = \"predicted_totals_for_unit_exposure.rds\")\n\npredicted_totals_for_unit_exposure <- readRDS(\n  file = \"predicted_totals_for_unit_exposure.rds\"\n)\n\nsummary(predicted_totals_for_unit_exposure)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 3904985 11011626 12889186 13044586 14930666 27255385 \n```\n\n\n:::\n:::\n\n\n### Nonparametric Bootstrap\n\nThe parametric bootstrap uses the point estimate to simulate new samples. From a Bayesian point of view, this corresponds to using the Dirac delta posterior distribution with a spike at the point estimate $(\\hat\\mu_{\\text{int}}, \\hat\\phi_{\\text{int}})$ and ignoring the uncertainty in the parameter estimates by treating it as zero. This section extends the parametric bootstrap approach by accounting for the uncertainty in the $(\\hat\\mu_{\\text{int}}, \\hat\\phi_{\\text{int}})$ values.\n\nIn the first stage, a separate Tweedie model is fit to each bootstrapped sample from the original data and the estimated $(\\hat\\mu_{\\text{int}, b}, \\hat\\phi_{\\text{int}, b})_{b = 1}^{10,000}$ pairs are collected. These 10,000 pairs are a sample from the \"poor man's\" posterior distribution for these parameters and are visualized here\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# bootstrap and get the joint distribution of (mu, phi)\n# bootstrap_mu_phi <- map_dfr(\n#   .x = 1:10000,\n#   .f = ~ {\n#     if(.x %% 100 == 0) {\n#       print(.x)\n#     }\n#     set.seed(.x)\n#     data <- claims %>%\n#       slice_sample(n = 60000, replace = TRUE)\n#     outlier_counts <- nrow(data[data$ClaimAmount == 1404185.52, ])\n#     mod <- data %>%\n#       glm(\n#         I(ClaimAmount / Exposure) ~ 1,\n#         weights = Exposure,\n#         data = .,\n#         family = statmod::tweedie(var.power = 1.6, link.power = 0)\n#       )\n#\n#     tibble(\n#       mu = exp(coef(mod)),\n#       phi = summary(mod)$dispersion,\n#       outlier_counts = outlier_counts\n#     )\n#   }\n# )\n#\n# saveRDS(bootstrap_mu_phi, file = \"bootstrap_mu_phi.rds\")\n\nbootstrap_mu_phi <- readRDS(\"bootstrap_mu_phi.rds\")\n\nmu_phi_plot <- bootstrap_mu_phi %>%\n  mutate(`Outlier counts` = factor(outlier_counts)) %>%\n  ggplot(aes(x = mu, y = phi, color = `Outlier counts`)) +\n  geom_point() +\n  geom_point(data = tibble(mu = mu, phi = phi),\n             aes(x = mu, y = phi),\n             color = \"gray20\", size = 5, inherit.aes = FALSE) +\n  labs(\n    x = glue::glue(\"Mean (\\u03bc)\\n\", \n                   \"Point estimates for mean and\", \n                   \" dispersion from the full sample \",\n                   \"shown in black\"),\n    y = \"Dispersion (\\u03d5)\"\n  ) +\n  theme(\n    legend.position = \"inside\",\n    legend.background = element_blank(),\n    legend.position.inside = c(0.8, 0.18)\n  ) +\n  guides(color = guide_legend(nrow = 2))\n\n# this thorws warnings that bins are ignored, but \n# the correct behaviour is observed anyway\nggExtra::ggMarginal(\n  p = mu_phi_plot, type = \"densigram\", \n  xparams = list(bins = 100), \n  yparams = list(bins = 100)\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThe clustering of points was initially a bit puzzling. The marginal density for the mean (top) is the same as the multimodal bootstrap distribution from a few sections above. The density for dispersion seems to be a lot less well-behaved. Coloring by the number of times that the original sample maximum claim amount is sampled in the bootstrap datasets, it's clear that the frequency of occurrence is largely responsible for the clustering observed here.\n\nIn the second stage, a random pair $(\\hat\\mu_{\\text{int}, b}, \\hat\\phi_{\\text{int}, b})$ is drawn first, followed by drawing $n$ values $Y_{i, b}^* \\sim \\text{Tweedie}(\\hat\\mu_{\\text{int}, b}, \\hat\\phi_{\\text{int}, b}, p = 1.6)$, and summing these to get an estimate of the total claim amount $\\hat{T_b}$.\n\nIt is expected that accounting for uncertainty in the estimation of $(\\hat\\mu_{\\text{int}}, \\hat\\phi_{\\text{int}})$ should lead to thicker tails for the distribution of $\\hat{T_b}$'s. The right tail now extends beyond 28 million and goes up to 34 million.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sample Y_i from the posterior predictive distribution and sum them\n# do this 10k times to get the posterior distribution\n# posterior_distribution_samples_for_total_claims <- map(\n#   .x = 1:10000,\n#   .f = ~ {\n#     if(.x %% 100 == 0) {\n#       print(.x)\n#     }\n#     set.seed(.x)\n#     draw <- bootstrap_mu_phi %>%\n#       slice_sample(n = 1)\n#\n#     set.seed(.x)\n#     sum_values <- draw %$%\n#       tweedie::rtweedie(n = 60000, mu = mu, phi = phi, power = 1.6) %>%\n#       sum()\n#\n#     draw %>%\n#       mutate(total = sum_values)\n#   }\n# ) %>%\n#   list_rbind()\n#\n# saveRDS(posterior_distribution_samples_for_total_claims,\n#         file = \"posterior_distribution_samples_for_total_claims.rds\")\n\nposterior_distribution_samples_for_total_claims <- readRDS(\n  file = \"posterior_distribution_samples_for_total_claims.rds\"\n)\n\nsummary(posterior_distribution_samples_for_total_claims)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       mu             phi              total         \n Min.   :144.3   Min.   :  883.9   Min.   : 3244481  \n 1st Qu.:182.1   1st Qu.:16455.5   1st Qu.:10286795  \n Median :213.6   Median :24801.9   Median :12234351  \n Mean   :218.2   Mean   :23607.4   Mean   :13061045  \n 3rd Qu.:243.8   3rd Qu.:31349.8   3rd Qu.:15142596  \n Max.   :452.7   Max.   :68572.1   Max.   :34585477  \n```\n\n\n:::\n:::\n\n\n### Using properties of EDMs\n\nThis section uses two properties — sampling distribution of the weighted average, and scale invariance — of [exponential dispersion models](https://en.wikipedia.org/wiki/Exponential_dispersion_model) (EDMs) of which Tweedie distributions are a special case.\n\nThe main statistic of interest has been the weighted mean $T_{wm}$ of the pure premium values ($Y_i = Z_i / w_i$)\n\n$$\nT_{wm} = w_{\\bullet}^{-1} \\sum_{i = 1}^n w_i Y_i\n$$\n\nwhere $w_{\\bullet} = \\sum_i w_i$ is the sum of the exposures. If $Y_i \\sim \\text{Tweedie}(\\mu, \\phi / w_i, p)$, then the sampling distribution of the weighted mean is also a Tweedie distribution with the same mean but with the dispersion $\\phi$ scaled by the total exposure, i.e., $T_{wm} \\sim \\text{Tweedie}(\\mu, \\phi / w_{\\bullet}, p)$.\n\nThe scale invariance property says that\n\n$$\nc \\text{Tweedie}(\\mu, \\phi, p) = \\text{Tweedie}(c \\mu, c^{2-p} \\phi, p)\n$$\n\nCombining these two and setting $c = 60,000$ since we're interested in the total claim amount across the policies, we can write\n\n$$\nT_\\text{total} \\sim \\text{Tweedie}(60000 \\mu, 60000^{2-p} \\phi / w_{\\bullet}, p)\n$$\n\nThis is much simpler and faster than the parametric bootstrap (for the weighted mean $T_{wm}$) since we don't need to sample the vector of $Y_{i, b}^*$'s as an intermediate step[^3].\n\n[^3]: Of course the parametric bootstrap is much more general, so it's not really a fair comparison.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntotal_exposure <- claims %>% \n  pull(Exposure) %>% \n  sum()\n\nset.seed(43)\npredicted_totals_tweedie_sampling_dist <- tweedie::rtweedie(\n  n = 10000,\n  mu = 60000 * mu,\n  phi = ((60000 ^ (2 - 1.6)) * phi) / total_exposure,\n  power = 1.6\n)\n\npredicted_totals_tweedie_sampling_dist %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 2848259 10276333 12713295 13030839 15472733 32389786 \n```\n\n\n:::\n:::\n\n\n### GLM with predictors\n\nSo far, we've ignored all other variables in the data that provide information at the level of the policyholder, and modelled the marginal distribution of the pure premiums. Using this so-called *collective* model can be contrasted with the *individual* model where information at the policy level can be used for modelling individual risk.\n\nThe simplest extension to the intercept-only model is the *main effects* model which includes all the additional variables at the policy level in the model\n\n$$\n\\begin{align*}\nY_i &\\sim \\text{Tweedie}(\\mu_i = \\text{exp}(\\text{log}(E[Y_i])), \\phi, p) \\\\\n\\text{log}(E[Y_i]) &= \\beta_0 + \\beta_1 x_{i, 1} + \\dots + \\beta_p x_{i, p}\n\\end{align*}\n$$\n\nFor simplicity, linearity and additivity of the predictors are assumed on the log scale, which leads to a multiplicative model on the original scale. This has the additional advantage of ensuring that the expected values can never be less than 0 (since $\\mu > 0$). Using boosted trees (e.g. LightGBM) would be a natural next step for improvement as they can model interactions, carry out variable selection by dropping terms that don't impact risk, don't impose functional form restrictions, etc. and can lead to more accurate predictions.\n\nThe following code fits a Tweedie GLM to the pure premium values $y_i$ with exposure weights $w_i$ using the log link (`link.power = 0`) and fixing $p = 1.6$.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nclaims_modelling <- claims %>%\n  mutate(\n    across(c(Area, VehPower, VehBrand, VehGas, Region), ~ as.factor(.x)),\n    pure_premium = ClaimAmount / Exposure\n  )\n\nmain_effects_glm <- glm(\n  pure_premium ~ 1 + Area + VehPower + VehAge + DrivAge\n  + BonusMalus + VehBrand + VehGas + Density + Region,\n  weights = Exposure,\n  data = claims_modelling,\n  family = statmod::tweedie(var.power = 1.6, link.power = 0)\n)\n```\n:::\n\n\nGiven the increase in complexity of this model and to eventually try more complex models, it would be good to switch to cross-validation to see how well this model performs on unseen data. I'll get to this in a future post.\n\nIt can be informative to explore the fitted values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted_values <- main_effects_glm %>%\n  broom::augment(newdata = claims_modelling, type.predict = \"response\")\n\nmean_fitted <- fitted_values %>%\n  pull(\".fitted\") %>%\n  mean()\n\nfitted_values %>%\n  pull(.fitted) %>%\n  quantile(\n    probs = c(0, 0.25, 0.5, 0.75, 0.95, 0.99, 1.0)\n  ) %>%\n  c(., \"Mean\" = mean_fitted) %>%\n  sort() %>%\n  round(., 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       0%       25%       50%       75%      Mean       95%       99%      100% \n     6.20     87.99    138.68    247.29    256.30    764.44   1763.00 306296.00 \n```\n\n\n:::\n:::\n\n\nThe least risky policy has an expected value of 6.2 and 50% of the policies have an expected value less than 140. Fewer than 1% of the policies have a risk larger than 2000, but the riskiest policy has an expected value of about 300,000. Since this is a simple main effects model, it's easy enough to see which term(s) contribute towards these very high values for the top-10 policies with the largest pure premium values\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# get the term-wise contribution to the prediction on the\n# link scale for the top k largest predictions\ntop_10_largest_policy_predictions <- fitted_values %>%\n  arrange(desc(.fitted)) %>%\n  slice_head(n = 10)\n\ntop_10_largest_policy_predictions %>% \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|  IDpol| ClaimNb| Exposure|Area |VehPower | VehAge| DrivAge| BonusMalus|VehBrand |VehGas  | Density|Region | ClaimAmount| pure_premium|   .fitted|\n|------:|-------:|--------:|:----|:--------|------:|-------:|----------:|:--------|:-------|-------:|:------|-----------:|------------:|---------:|\n|  41847|       1|     0.24|C    |7        |     12|      60|        228|B1       |Regular |     300|R53    |     1128.12|     4700.500| 306296.00|\n|  70558|       0|     0.30|E    |6        |     12|      24|        195|B1       |Regular |    3103|R82    |        0.00|        0.000|  75259.17|\n|  40910|       0|     0.60|B    |12       |     19|      39|        156|B11      |Regular |      95|R82    |        0.00|        0.000|  23306.65|\n|  89044|       0|     0.39|E    |4        |     10|      26|        173|B1       |Regular |    3744|R93    |        0.00|        0.000|  22336.77|\n|  62898|       1|     1.00|E    |7        |     10|      80|        173|B2       |Regular |    3688|R82    |     1128.12|     1128.120|  20961.85|\n| 113887|       1|     0.26|D    |6        |      2|      28|        156|B1       |Regular |    1943|R24    |     1128.12|     4338.923|  18147.85|\n|  41513|       2|     1.00|D    |5        |      3|      30|        196|B2       |Regular |    1284|R25    |     1830.85|     1830.850|  17029.99|\n| 102276|       0|     0.75|E    |6        |      8|      25|        156|B3       |Diesel  |    3021|R53    |        0.00|        0.000|  16095.04|\n|  32880|       0|     1.00|C    |4        |     17|      47|        177|B2       |Regular |     105|R24    |        0.00|        0.000|  14837.72|\n|  58572|       0|     0.05|E    |7        |      5|      21|        156|B1       |Regular |    4762|R93    |        0.00|        0.000|  14193.21|\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\ntop_10_largest_policy_predictions <- predict(\n  object = main_effects_glm, \n  newdata = top_10_largest_policy_predictions, \n  type = \"terms\"\n)\n\n# convert the 'terms' data frame to tibble and add constant value\n# then pivot to get the exp(cumsum()) for each policy (i.e. row)\n# then pivot back\n# there's probably some neat function that does \n# this in fewer lines of code\ntop_10_largest_policy_predictions %>%\n  as_tibble() %>%\n  mutate(\n    id = row_number(),\n    # for the calculation of the constant value, see this\n    # https://stackoverflow.com/questions/37963904/what-does-predict-glm-type-terms-actually-do\n    constant = attr(top_10_largest_policy_predictions, \"constant\"),\n    .before = 0\n  ) %>%\n  pivot_longer(cols = -id) %>%\n  group_by(id) %>%\n  mutate(value = exp(cumsum(value))) %>%\n  ungroup() %>%\n  pivot_wider(id_cols = id, names_from = name, values_from = value) %>%\n  mutate(across(.cols = everything(), .fns = round)) %>% \n  select(-id) %>%\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n| constant| Area| VehPower| VehAge| DrivAge| BonusMalus| VehBrand| VehGas| Density| Region|\n|--------:|----:|--------:|------:|-------:|----------:|--------:|------:|-------:|------:|\n|      157|  169|      141|    125|     142|     188272|   216443| 218766|  230584| 306296|\n|      157|  216|      311|    274|     234|      75132|    86374|  87300|   81568|  75259|\n|      157|  116|      275|    201|     193|      11582|    23503|  23755|   25260|  23307|\n|      157|  216|      143|    134|     116|      14426|    16585|  16763|   15236|  22337|\n|      157|  216|      180|    168|     223|      27859|    24670|  24935|   22719|  20962|\n|      157|  153|      220|    254|     224|      13433|    15443|  15609|   15330|  18148|\n|      157|  153|      126|    142|     127|      42714|    37826|  38231|   38627|  17030|\n|      157|  216|      311|    306|     262|      15756|    13100|  12922|   12117|  16095|\n|      157|  169|      112|     87|      89|      13175|    11667|  11792|   12534|  14838|\n|      157|  216|      180|    192|     160|       9577|    11010|  11128|    9681|  14193|\n\n\n:::\n:::\n\n\nLooking at the contribution of each term for a given policy shows that high values of `BonusMalus` (with a coefficient of 0.043 on the log scale) has the largest impact on pushing up the predicted pure premiums. Obviously, for more complex models, SHAP plots would provide similar information in terms of identifying features that contribute to very high (or low) predicted values for the policies of interest.\n\nThe distribution of fitted values can be visualized too\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted_values %>%\n  ggplot(aes(x = .fitted)) +\n  stat_ecdf() +\n  geom_vline(xintercept = mu, color = \"orange\", linetype = \"dashed\") +\n  annotate(geom = \"text\", x = 200, y = 0.8, color = \"orange\",\n           label = \"Marginal mean: 217.8\", hjust = \"right\") +\n  geom_vline(xintercept = mean_fitted, color = \"red4\", linetype = \"dashed\") +\n  annotate(geom = \"text\", x = 275, y = 0.5, color = \"red4\",\n           label = \"Fitted values mean: 256.3\", hjust = \"left\") +\n  scale_x_log10(\n    breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n    labels = scales::trans_format(\"log10\", scales::math_format(10^.x))\n  ) +\n  annotation_logticks(sides = \"b\") +\n  scale_y_continuous(labels = scales::percent) +\n  xlab(\"Fitted pure premium values\") +\n  ylab(\"Empirical distribution function\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nThe overall mean from the intercept only model and the mean of the fitted values don't coincide, which I wasn't expecting given the [*law of iterated expectation*](https://en.wikipedia.org/wiki/Law_of_total_expectation) (LIE) where $E[Y] = E[E[Y | X]]$ holds. After puzzling over this for a bit and looking around on the internet, this seems to be due to the use of the non-canonical log-link function $g(\\mu) = \\text{log}(\\mu)$. Using the canonical link function for a Tweedie distribution $g(\\mu) = \\mu^{(1 - p)} / (1-p)$ results in LIE holding, but attempting to fit the main effects model with the canonical link fails in R.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmain_effects_glm %>% \n  update(\n    # the default link function is the canonical link function\n    family = statmod::tweedie(var.power = 1.6)\n  )\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: no valid set of coefficients has been found: please supply starting values\n```\n\n\n:::\n:::\n\n\nThe disadvantages of using the canonical link for Tweedie models are numerical instability, and interpretation issues (since risks are not multiplicative as a function of predictors).\n\nFor simulating the sampling distribution of total claims across the policies, the same approach as the one from the parametric bootstrap section can be applied using the usual Pearson estimate of scale $\\phi$.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nphi_glm <- main_effects_glm %>% \n  summary() %>% \n  pluck(\"dispersion\") %>% \n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4529.461\n```\n\n\n:::\n:::\n\n\nThe estimated value of $\\hat\\phi_{\\text{main}} \\approx 4529$ is much smaller compared to the intercept-only model with value $\\hat\\phi_{\\text{int}} \\approx 25966$. Sampling 10,000 datasets of size 60,000 with policy-specific conditional means from the individual model $\\hat\\mu_i$'s and $\\hat\\phi_{\\text{main}}$ and summing the values for each dataset now gives estimates ranging from 9 million to 52 million. The maximum possible claim amount is much much higher here compared to the 35 million from the collective model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate 10,000 times the total claim amounts for the full year of exposure\n# using the fitted means\n# predicted_totals_from_glm <- map_dbl(.x = 1:10000, .f = ~ {\n#   if(.x %% 100 == 0) {\n#     print(.x)\n#   }\n#   set.seed(.x)\n#   sum(tweedie::rtweedie(60000, mu = fitted_values$.fitted, \n#                         phi = phi_glm, power = 1.6))\n# })\n#\n# saveRDS(predicted_totals_from_glm,\n#         file = \"predicted_totals_from_glm.rds\")\n\npredicted_totals_from_glm <- readRDS(\n  file = \"predicted_totals_from_glm.rds\"\n)\n\nsummary(predicted_totals_from_glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 9179960 13812465 15097242 15442905 16590660 52011640 \n```\n\n\n:::\n:::\n\n\nSample statistics for the observed data with exposure $w_i$ can be compared with the statistics from the intercept-only and main-effects GLMs as a form of PPC. Looking at the summary statistics for the main effects model, there are more policies with non-zero claims (266 on average) compared with the PPC from the intercept only model (28 on average), but still very far off from the 3051 that are in the observed data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# quick check of sampling dist of statistics\n# ppc_glm_obs_exposure <- map(\n#   .x = 1:1000,\n#   .f = ~ {\n#     if(.x %% 50 == 0) {\n#       print(.x)\n#     }\n#     set.seed(.x)\n#     draw <- (fitted_values$Exposure *\n#                tweedie::rtweedie(60000,\n#                                  mu = fitted_values$.fitted,\n#                                  phi = phi_glm, power = 1.6))\n#\n#     tibble(prop_zero = mean(draw == 0), sample_total = sum(draw),\n#            sample_max = max(draw), n_nonzero = sum(draw > 0))\n#   }) %>%\n#   list_rbind()\n#\n# saveRDS(ppc_glm_obs_exposure, file = \"ppc_glm_obs_exposure.rds\")\n\nppc_glm_obs_exposure <- readRDS(file = \"ppc_glm_obs_exposure.rds\")\n\n# function to convert the output of summary.data.frame() to a tibble\ntidy_df_summary <- function(data) {\n  data %>%\n    summary() %>%\n    as.data.frame() %>%\n    separate(col = Freq, into = c(\"Statistic\", \"Value\"), sep = \":\") %>%\n    select(-Var1) %>%\n    rename(Column = Var2) %>%\n    pivot_wider(names_from = Statistic, values_from = Value) %>%\n    mutate(\n      Column = str_trim(Column, side = \"left\"),\n      across(\n        .cols = where(is.character),\n        .fns = ~ str_trim(.x, side = \"both\")\n      )\n    )\n}\n\nlist(\n  \"Intercept\" = ppc_obs_exposure,\n  \"Main effects\" = ppc_glm_obs_exposure,\n  \"Observed\" = sample_statistics_obs_exposure\n) %>%\n  imap(\n    .f = ~ {\n      .x %>%\n        tidy_df_summary() %>%\n        mutate(Method = .y, .before = 1)\n    }\n  ) %>%\n  list_rbind() %>%\n  mutate(\n    Method = factor(Method, \n                    levels = c(\"Observed\", \"Intercept\", \"Main effects\"))\n  ) %>%\n  arrange(Column, Method) %>% \n  relocate(Column, .before = Method) %>% \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|Column       |Method       |Min.    |1st Qu. |Median  |Mean    |3rd Qu. |Max.     |\n|:------------|:------------|:-------|:-------|:-------|:-------|:-------|:--------|\n|n_nonzero    |Observed     |3051    |3051    |3051    |3051    |3051    |3051     |\n|n_nonzero    |Intercept    |12.00   |25.00   |28.00   |28.61   |32.00   |50.00    |\n|n_nonzero    |Main effects |218.0   |255.8   |265.0   |266.1   |277.0   |312.0    |\n|prop_zero    |Observed     |0.9492  |0.9492  |0.9492  |0.9492  |0.9492  |0.9492   |\n|prop_zero    |Intercept    |0.9992  |0.9995  |0.9995  |0.9995  |0.9996  |0.9998   |\n|prop_zero    |Main effects |0.9948  |0.9954  |0.9956  |0.9956  |0.9957  |0.9964   |\n|sample_max   |Observed     |1404186 |1404186 |1404186 |1404186 |1404186 |1404186  |\n|sample_max   |Intercept    |265332  |918290  |1172482 |1248649 |1486689 |4658137  |\n|sample_max   |Main effects |165327  |348857  |479910  |633731  |693709  |4569095  |\n|sample_total |Observed     |7507466 |7507466 |7507466 |7507466 |7507466 |7507466  |\n|sample_total |Intercept    |1515008 |5966878 |7341226 |7506283 |8903769 |17569471 |\n|sample_total |Main effects |4374107 |6851961 |7530698 |7630616 |8274916 |12929669 |\n\n\n:::\n:::\n\n\nThe maximum claim amounts are much smaller — mean of 633k vs 1.4 mil in the observed data and 1.24 mil on average from the intercept-only PPC model.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlist(\n  \"Intercept\" = ppc_obs_exposure, \n  \"Main effects\" = ppc_glm_obs_exposure\n) %>% \n  map(.f = ~ {\n    .x %>%\n      mutate(sample_gt_obs_max = sample_max <= 1404185.52) %>% \n      pull(sample_gt_obs_max) %>%\n      mean()\n  })\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Intercept\n[1] 0.6975\n\n$`Main effects`\n[1] 0.94\n```\n\n\n:::\n:::\n\n\nFor the intercept-only GLM, the observed maximum is larger than 70% of the maxima from the sampled datasets, compared to 94% of the maxima from the sampled datasets using the fitted values from the main effects GLM. In an ideal scenario, this should be about 50%.\n\nThe sample totals are all within about +/- 150k of each other.\n\nTen datasets are simulated, and only the policies with non-zero claim amounts are retained. The eCDF for each of these datasets are visually compared with the observed data. Since the model predicts a larger number of policies with non-zero claims, the distribution functions are smoother, and these functions are closer to the function for the observed sample (but still a very poor fit for the observed data).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmap(\n  .x = 1:10,\n  .f = ~ {\n    set.seed(.x)\n    draw <- (fitted_values$Exposure *\n               tweedie::rtweedie(60000,\n                                 mu = fitted_values$.fitted,\n                                 phi = phi_glm, power = 1.6))\n    tibble(sim_id = .x, y = draw, grp = \"Simulated\")\n  }) %>%\n  list_rbind() %>%\n  bind_rows(\n    .,\n    claims %>%\n      mutate(sim_id = 100, grp = \"Observed\") %>%\n      select(sim_id, y = ClaimAmount, grp)\n  ) %>%\n  filter(y > 0) %>%\n  ggplot(aes(x = y, group = sim_id, color = grp)) +\n  stat_ecdf() +\n  xlab(\"Policies with non-zero claim amounts (log10 scale)\") +\n  ylab(\"Empirical distribution function\") +\n  scale_x_log10(\n    breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n    labels = scales::trans_format(\"log10\", scales::math_format(10^.x))\n  ) +\n  annotation_logticks(sides = \"b\") +\n  scale_y_continuous(labels = scales::percent) +\n  scale_color_manual(\n    values = c(\"Simulated\" = \"gray70\", \"Observed\" = \"Black\")\n  ) +\n  theme(\n    legend.title = element_blank(), \n    legend.position = \"inside\",\n    legend.position.inside = c(0.9, 0.2)\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n## All the distributions together\n\nFinally, all the different estimates for the distribution of next year's total claim amounts can be visualized together. First, the eCDFs are plotted together\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndistributions_of_total_claims <- bind_rows(\n  tibble(\n    method = \"Parametric Bootstrap (Tweedie MLE)\",\n    total_claim_amount_in_millions = predicted_totals_for_unit_exposure\n  ),\n  tibble(\n    method = \"Nonparametric Bootstrap (Tweedie GLM)\",\n    total_claim_amount_in_millions = posterior_distribution_samples_for_total_claims$total\n  ),\n  tibble(\n    method = \"Closed-form Tweedie Distribution for Weighted Mean\",\n    total_claim_amount_in_millions = predicted_totals_tweedie_sampling_dist\n  ),\n  tibble(\n    method = \"Main Effects Tweedie GLM\",\n    total_claim_amount_in_millions = predicted_totals_from_glm\n  )\n) %>%\n  mutate(\n    total_claim_amount_in_millions = total_claim_amount_in_millions / 1e6\n  ) %>%\n  bind_rows(nonparametric_bootstrap_totals, .)\n\n# order the methods by increasing values of the ranges\nmethod_order <- distributions_of_total_claims %>%\n  group_by(method) %>%\n  summarise(range = diff(range(total_claim_amount_in_millions))) %>%\n  arrange(range) %>%\n  #print() %>%\n  pull(method)\n\ndistributions_of_total_claims <- distributions_of_total_claims %>% \n  mutate(\n    method = factor(method, levels = method_order)\n  )\n\ntotals_plot <- distributions_of_total_claims %>%\n  ggplot() +\n  stat_ecdf(aes(x = total_claim_amount_in_millions,\n                linetype = method, group = method), pad = FALSE) +\n  geom_vline(xintercept = (6e4 * expected_claim_amount) / 1e6,\n             color = \"orange\", linewidth = 1.2, linetype = \"dashed\") +\n  scale_y_continuous(labels = scales::label_percent()) +\n  xlab(glue::glue(\"Plausible values for the following \", \n                  \"year's total claim amount in millions, \", \n                  \"assuming unit exposure\", \n                  \"\\n(Estimate from the full sample in orange)\",\n                  \"\\n(Mean of each distribution in black)\")) +\n  ylab(\"Empirical distribution function\")+\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  )\n\ntotals_plot + \n  scale_x_continuous(breaks = seq(3, 54, 3)) + \n  guides(linetype = guide_legend(nrow = 3))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\nand then plotted separately as well, with the expected total (217.8 x 60,000) in orange and the mean of the totals for each method overlaid as a black vertical line\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntotals_plot +\n  facet_wrap(~factor(method, levels = method_order), ncol = 1) +\n  theme(legend.position = \"none\") +\n  geom_vline(\n    data = distributions_of_total_claims %>%\n      summarize(m = mean(total_claim_amount_in_millions), .by = \"method\"),\n    aes(xintercept = m, group = method, linetype = method)\n  ) +\n  scale_x_continuous(breaks = seq(3, 54, 6))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-30-1.png){width=1050}\n:::\n:::\n\n\nIf every policy is to have the same premium, then the closed-form Tweedie distribution or the joint bootstrap distribution $(\\hat\\mu_{\\text{int}, b}, \\hat\\phi_{\\text{int}, b})_{b = 1}^{10,000}$ can be used for simulating plausible values for the following year's total claim amounts.\n\nFor setting premiums at the policy level, plausible values for next year's claim amount for a specific policy can be sampled using the fitted means $\\hat\\mu_i$ from the main-effects GLM.\n\n## References\n\nI've skimmed (parts of some of) these references, but I'm including all the ones I encountered in case I need to come back to them in the future.\n\n### Auto Claims Data\n\n-   [CASdatasets - freMTPL](https://dutangc.github.io/CASdatasets/reference/freMTPL.html){.uri}\n\n-   Noll, Alexander and Salzmann, Robert and Wuthrich, Mario V., Case Study: French Motor Third-Party Liability Claims (March 4, 2020). Available at SSRN: <https://ssrn.com/abstract=3164764> or [http://dx.doi.org/10.2139/ssrn.3164764](https://dx.doi.org/10.2139/ssrn.3164764)\n\n### Bootstrapping\n\n-   [Cross Validated - how to interpret multimodal distribution of bootstrapped correlation](https://stats.stackexchange.com/questions/63999/how-to-interpret-multimodal-distribution-of-bootstrapped-correlation)\n\n-   [Cross Validated - bootstrapping vs bayesian bootstrapping conceptually](https://stats.stackexchange.com/questions/181350/bootstrapping-vs-bayesian-bootstrapping-conceptually){.uri}\n\n-   <https://mc-stan.org/docs/stan-users-guide/posterior-predictive-checks.html>\n\n-   [Cross Validated - difference between sampling a population vs bootstrapping](https://stats.stackexchange.com/questions/475631/difference-between-sampling-a-population-vs-bootstrapping){.uri}\n\n-   <https://www.sumsar.net/blog/2015/04/the-non-parametric-bootstrap-as-a-bayesian-model/>\n\n-   [Bootstrapping - Wikipedia](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)){.uri}\n\n### Tweedie Models\n\n-   Denuit, Michel, Arthur Charpentier, and Julien Trufin. \"Autocalibration and Tweedie-dominance for insurance pricing with machine learning.\" *Insurance: Mathematics and Economics* 101 (2021): 485-497. ArXiv link - <https://arxiv.org/abs/2103.03635>\n\n    -   This paper is for calibrating boosted tree models fit by minimizing deviance instead of maximizing likelihood\n\n-   Yang, Yi, Wei Qian, and Hui Zou. \"Insurance premium prediction via gradient tree-boosted Tweedie compound Poisson models.\" *Journal of Business & Economic Statistics* 36.3 (2018): 456-470. ArXiv link - <https://arxiv.org/abs/1508.06378>\n\n-   Delong, Ł., Lindholm, M. & Wüthrich, M.V. Making Tweedie’s compound Poisson model more accessible. *Eur. Actuar. J.* **11**, 185–226 (2021). https://doi.org/10.1007/s13385-021-00264-3\n\n-   Zhang, Y. Likelihood-based and Bayesian methods for Tweedie compound Poisson linear mixed models. *Stat Comput* **23**, 743–757 (2013). https://doi.org/10.1007/s11222-012-9343-7\n\n-   [These slides](https://www.casact.org/sites/default/files/old/affiliates_bace_0419_loss_cost_modeling.pdf) comparing Tweedie vs Quasi-Poisson models\n\n-   <https://lorentzen.ch/index.php/2024/06/03/a-tweedie-trilogy-part-i-frequency-and-aggregration-invariance/>\n\n-   <https://lorentzen.ch/index.php/2024/06/10/a-tweedie-trilogy-part-ii-offsets/>\n\n-   [Tweedie distribution - Wikipedia](https://en.wikipedia.org/wiki/Tweedie_distribution){.uri}\n\n-   [Exponential dispersion model - Wikipedia](https://en.wikipedia.org/wiki/Exponential_dispersion_model)\n\n### Books\n\n-   Dunn, Peter K., and Gordon K. Smyth. *Generalized linear models with examples in R*. Vol. 53. New York: Springer, 2018.\n\n-   Davison, Anthony Christopher, and David Victor Hinkley. *Bootstrap methods and their application*. No. 1. Cambridge university press, 1997.\n\n-   Kaas, R. *Modern Actuarial Risk Theory*. Springer, 2008.\n\n-   Ohlsson, Esbjörn, and Björn Johansson. *Non-life insurance pricing with generalized linear models*. Vol. 174. Berlin: Springer, 2010.\n\n-   Klugman, Stuart A., Harry H. Panjer, and Gordon E. Willmot. *Loss models: from data to decisions*. Vol. 715. John Wiley & Sons, 2012.\n\n## Appendix: Claims Data\n\nThe data I'm using for most of this post — except for the last section — is a subset of 60,000 policies from the full claims dataset, which contains about 680,000 policies. The full data here is the version from OpenML ([frequency](https://www.openml.org/d/41214), [severity](https://www.openml.org/d/41215)), and information on this dataset can be found in the [documentation](https://dutangc.github.io/CASdatasets/reference/freMTPL.html) for the R package `CASdatasets`, where it's called `freMTPL2freq` (frequency) and `freMTPL2sev` (severity). For reasons I haven't figured out, the number of rows differ very slightly between these two datasets. The R script I used for combining the full dataset can be found [here](https://github.com/ad1729/ad1729.github.io/tree/master/posts/fitting-tweedie-models-to-claims-data/process_claims_data.R).\n\nAn exploratory data analysis on the full dataset can be found in [this paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3164764).\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}