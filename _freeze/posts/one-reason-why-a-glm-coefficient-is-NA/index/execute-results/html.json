{
  "hash": "fcec63b61aedd2de7693dfee250ab90f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"One reason why a (g)lm coefficient is NA\"\ndate: \"2023-03-31\"\ncategories: [R, GLM, Multicollinearity]\ncode-fold: false\nreference-location: margin\nimage: \"aliased-effect.png\"\n---\n\n\nTL;DR: Use the *alias* function in R to check if you have *nested factors* (predictors) in your data\n\nRecently while fitting a logistic regression model, some of the coefficients estimated by the model were `NA`. Initially I thought it was due to *separation*[^1], as that's the most common issue I usually face when fitting unregularized models on data.\n\n[^1]: see [this](https://en.wikipedia.org/wiki/Separation_(statistics)) Wikipedia article, or [this](https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression) stats.stackexchange.com thread (and the associated links in the sidebar)\n\nHowever, googling[^2] threw up many threads on multicollinearity and anyway, separation usually leads to nonsensical estimates like $1.5 \\times 10^8$ instead of `NA`.\n\n[^2]: in this day and age of *ChatGPT*, I know\n\nAfter combing through many stackexchange threads, I discovered the [*alias*](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/alias.html) function in R from [this](https://stats.stackexchange.com/questions/13465/how-to-deal-with-an-error-such-as-coefficients-14-not-defined-because-of-singu) thread, which was pretty handy at identifying the problematic column(s).\n\nIt's interesting that the alias documentation doesn't mention anything about GLMs (`glm()`) but this does work on `glm(..., family = \"binomial\")` model objects[^3].\n\n[^3]: possibly since the `class` of a `glm` object is `c(\"glm\", \"lm\")`\n\nThe rest of this post explores this issue and its resolution using aggregated test data, where the city variable is intentionally nested within the country variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\nsimulated_data <- tribble(\n  ~age, ~city, ~country, ~y, ~N,\n  \"< 30\", \"Paris\", \"France\", 30, 100,\n  \"< 30\", \"Nice\", \"France\", 20, 100,\n  \"< 30\", \"Berlin\", \"Germany\", 23, 100,\n  \"30+\", \"Paris\", \"France\", 12, 100,\n  \"30+\", \"Nice\", \"France\", 11, 100,\n  \"30+\", \"Berlin\", \"Germany\", 27, 100\n) %>% \n  mutate(y = y / N)\n\nmodel <- glm(y ~ age + city + country, weights = N, data = simulated_data, family = \"binomial\")\n\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ age + city + country, family = \"binomial\", \n    data = simulated_data, weights = N)\n\nCoefficients: (1 not defined because of singularities)\n               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)     -0.8733     0.1876  -4.656 3.23e-06 ***\nage30+          -0.4794     0.2062  -2.325   0.0201 *  \ncityNice        -0.6027     0.2558  -2.356   0.0185 *  \ncityParis       -0.2286     0.2395  -0.954   0.3399    \ncountryGermany       NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 19.2591  on 5  degrees of freedom\nResidual deviance:  8.0953  on 2  degrees of freedom\nAIC: 43.492\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\nThe estimate for Germany is `NA`. Calling the alias function on this GLM model shows that the dummy variable of Germany is *linearly dependent* on (a subset of) the other columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalias(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel :\ny ~ age + city + country\n\nComplete :\n               (Intercept) age30+ cityNice cityParis\ncountryGermany  1           0     -1       -1       \n```\n\n\n:::\n:::\n\n\nThis means that the column for Germany is redundant in this *design matrix* (or the *model matrix*), as the values of Germany (the pattern of 0s and 1s) can be perfectly predicted / recreated by combining the Intercept, Nice, and Paris columns using the coefficients from the output of `alias()`. This is why the perfect [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) in this case leads to an `NA` coefficient.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# countryGermany and Germany are identical\nmodel.matrix(~ ., data = simulated_data) %>% \n  as_tibble() %>% \n  select(-y, -N, -`age30+`) %>% \n  mutate(Germany = `(Intercept)` - cityNice - cityParis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 5\n  `(Intercept)` cityNice cityParis countryGermany Germany\n          <dbl>    <dbl>     <dbl>          <dbl>   <dbl>\n1             1        0         1              0       0\n2             1        1         0              0       0\n3             1        0         0              1       1\n4             1        0         1              0       0\n5             1        1         0              0       0\n6             1        0         0              1       1\n```\n\n\n:::\n:::\n\n\nAnother interesting observation is that changing the order of the variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# here country comes before city\nglm(y ~ age + country + city, weights = N, data = simulated_data, family = \"binomial\") %>% \n  alias()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel :\ny ~ age + country + city\n\nComplete :\n          (Intercept) age30+ countryGermany cityNice\ncityParis  1           0     -1             -1      \n```\n\n\n:::\n:::\n\n\nleads to different estimates being `NA`, i.e., the estimate for Paris is now `NA` and is linearly dependent on the Intercept, country (Germany), and another city (Nice). This depends on which term enters the model first.\n\nThe simplest solution here is to drop one of the city or country columns, or to build two separate models -- one without country, and one without city.\n\nIf the goal is to estimate coefficients for both the city and country variables, then a mixed model with nested effects might be the right rabbit hole to go down, assuming they both have more than 10 levels or so. See the following links:\n\n-   <https://stats.stackexchange.com/questions/197977/analyzing-nested-categorical-data>\n-   <https://stats.stackexchange.com/questions/79360/mixed-effects-model-with-nesting>\n-   <https://stats.stackexchange.com/questions/372257/how-do-you-deal-with-nested-variables-in-a-regression-model>\n-   Second bullet point from the answer: <https://stats.stackexchange.com/questions/243811/how-to-model-nested-fixed-factor-with-glmm>\n-   <https://stackoverflow.com/questions/70537291/lmer-model-failed-to-converge-with-1-negative-eigenvalue>\n-   <https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html>\n-   <https://stackoverflow.com/questions/40723196/why-do-i-get-na-coefficients-and-how-does-lm-drop-reference-level-for-interact>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}