{
  "hash": "e46f5b6e5a9aa01592d7f1c56d1139a1",
  "result": {
    "markdown": "---\ntitle: \"How should the untreated in the treatment group from an experiment be analyzed?\"\ndate: \"2023-09-28\"\ncategories: [Causal Inference, Instrumental Variables, Advertising, R]\ncode-fold: false\ntoc-expand: true\nreference-location: margin\nimage: \"image.png\"\n---\n\n\nThis (longer-than-expected) post compares intention-to-treat, per-protocol, as-treated, and instrumental variable analyses on a simulated dataset. Along the way, it goes off on fun tangents like 1) comparing results from different estimators for risk difference (a.k.a. uplift), and 2) comparing the bootstrap distribution with the Bayesian posterior distribution and the normal approximation for the risk difference. Finally, it uses the estimated probabilities to get the posterior predictive distribution for the total profit under different scenarios. This was written largely for me to learn about IV methods in order to deal with the problem of estimating the effect of a treatment under non-compliance in randomized experiments.\n\n## Introduction\n\nOne of the first problems I ever cut my teeth on as a data scientist many years ago was to analyze data from a marketing / advertising campaign to assess the impact of an advertisement (ad) on sales of a certain product.\n\nIt was a simple experimental dataset from a two-arm[^1] randomized controlled trial. The most challenging part of that analysis was how to analyze individuals -- the so-called *non-compliers*[^2] *--* who were assigned to the treatment group eligible to view an ad, but who did not actually end up seeing a single ad.\n\n[^1]: i.e., an experiment with two groups -- treatment and control\n\n[^2]: in the usual terminology used in the instrumental variable methods literature\n\nAll details below are of a simplified version of the problem similar to the one I worked on, and have no resemblance to the real constraints / numbers / settings from the actual problem. This exact same problem shows up in pretty much every field (marketing, advertising, pharma, tech, psychology, economics, etc.) where experiments are regularly conducted, and the analyses in this post are applicable to those situations as well.\n\n## Problem description\n\nLet's say a company has been working on updating an existing version of a product, and wants to know whether they should invest in an advertising campaign on a specific channel (say YouTube / Instagram ads) to promote the new product. Since buying ads cost money, the campaign would have to be profitable enough in terms of increased sales to justify the ad spend[^3].\n\n[^3]: The advertising company, on the other hand, is more interested in assessing the effectiveness of advertising, so would be happier to see a strong positive effect of advertising.\n\nThe main interest is in (functions of) two quantities -- $\\text{Pr}[Y^{a=0} = 1]$, which is the proportion of sales in a population not shown the ad; and $\\text{Pr}[Y^{a=1} = 1]$, which is the proportion of sales in a population which is shown the ad. These can be used to calculate the risk difference / uplift ($\\text{Pr}[Y^{a=1} = 1] - \\text{Pr}[Y^{a=0} = 1]$), the risk ratio / relative risk / lift ($\\text{Pr}[Y^{a=1} = 1] / \\text{Pr}[Y^{a=0} = 1]$), or any other quantity listed in the tables [here](https://en.wikipedia.org/wiki/Relative_risk_reduction). The risk difference can then be used to calculate the additional profit from showing the ad compared to doing nothing.\n\nThe cleanest way of estimating these probabilities is through an experiment. First, a target population -- say, users of a given product -- is identified, and then, this group is randomly split into a treatment and a control group. Individuals assigned to the treatment group are eligible to see an ad for the newer, updated version of the product, and the control group is ineligible to view the ad. The campaign runs for a two week period, where individuals in the treatment group see an ad multiple times. The outcome for this campaign is sales of the newer product in each of the two groups in the six-week period from the start of the campaign.\n\nBefore writing any code, the `tidyverse` packages is loaded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n:::\n\n\nA DAG, or a *directed acyclic graph* for this problem can be visualized as follows\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndag <- ggdag::dagify(\n  A ~ Z + U, \n  Y ~ A + U, \n  exposure = \"A\", outcome = \"Y\", \n  coords = list(x = c(Z = 1, A = 2, U = 2.5, Y = 3), \n                y = c(Z = 1, A = 1, U = 1.2, Y = 1))\n) %>% \n  ggdag::ggdag() +\n  ggdag::theme_dag()\n\nplot(dag)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nwhere $Y$ is a binary outcome (bought the newer version of the product or not during the follow-up period), $A$ is a binary treatment variable (saw the ad or not), $Z$ is a binary variable indicating group assignment (i.e., randomly assigned to the treatment or control group), and U is the set of (unmeasured) confounder(s), i.e., one or more variables that both impact whether an individual ends up seeing the ad, and whether they end up buying the newer product or not.\n\nThis DAG encodes several assumptions:\n\n-   Group assignment $Z$ only affects whether someone sees an ad or not (variable $A$)\n\n-   There are some unmeasured common causes (confounders) $U$ that make our life difficult by potentially distorting the relationship between $A$ and $Y$\n\n-   $Z$ has no direct effect on Y; only an indirect effect entirely (mediated) via $A$\n\n-   $Z$ has no relationship with $U$, since $Z$ is randomly assigned and should be balanced with respect to $U$ (unless we're unlucky and there's some imbalance by chance)\n\nAfter the campaign is over, it is observed that a subset of the treatment group never received the treatment, i.e., didn't see the ad at all. The main question for the analysis is then -- how should these non-compliers be analyzed? In this post I look at three possible choices:\n\n-   Should they be analysed as part of the treatment group, despite not receiving the treatment?\n\n-   Should they be excluded from the analysis altogether?\n\n-   Should they be included in the control group since they were untreated?\n\n## Simulate data\n\nBefore the question in the previous section can be answered, we need to generate hypothetical data[^4] for this fictional ad campaign.\n\n[^4]: Inspired by a comment on [this post](https://statmodeling.stat.columbia.edu/2019/06/20/how-to-simulate-an-instrumental-variables-problem/), and the Imbens, Rubin 1997 paper linked therein.\n\nWhat the following code does is it first partitions the population of 100,000 individuals into an 80-20% mix of *compliers* and *non-compliers*. Compliers are the individuals that would watch the ad if they were assigned to the target group. Non-compliers are the individuals who would not comply with the group they're randomized to. More on this in the IV section below. Then, the potential outcomes for each individual under the two treatments depending on their compliance status are simulated.\n\nIf $C_i = 1$ denotes whether individual $i$ is a complier, and $C_i = 0$ otherwise, then $\\text{Pr}[Y^{a = 0}_i = 1| C_i = 0] = \\text{Pr}[Y^{a = 1}_i = 1| C_i = 0] = 0.1\\%$, where the probability of purchasing the new product is very small among the non-compliers independent of whether they're assigned to the treatment or the control group.\n\nAmong the compliers, $\\text{Pr}[Y^{a = 0}_i = 1| C_i = 1] = 1\\%$, i.e., 1% of the individuals would buy the new product if nobody was shown the ad. If the ad is rolled out to the full population, then $\\text{Pr}[Y^{a = 1}_i = 1| C_i = 1] = 11\\%$, which leads to an *average treatment effect (ATE)* among the compliers of +10 percentage points.\n\nThese individuals are randomly assigned to the treatment (70%) or control (30%) group. Their actual treatment status (i.e., treated or untreated) is a product of the group they're assigned to and their compliance. Their realized outcome is the outcome corresponding to the group they're assigned to.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsimulate_data <- function(n = 100000L,\n                          pY0 = 0.01,\n                          risk_diff = 0.1, \n                          seed = 23, \n                          p_compliers = 0.8, \n                          p_treatment = 0.7,\n                          pY_non_compliers_factor = 0.1) {\n  \n  pY1 <- pY0 + risk_diff\n  pY_nc <- pY_non_compliers_factor * pY0\n  \n  set.seed(seed)\n  data <- tibble(\n    id = 1:n,\n    # the underlying population can be stratified into \n    # never-takers and compliers\n    complier = rbinom(n, 1, prob = p_compliers), \n    # generate individual potential outcomes\n    # under control and treatment, i.e., Pr[Y^0 = 1]\n    Y0 = case_when(\n      complier == 0 ~ rbinom(n, 1, pY_nc),\n      complier == 1 ~ rbinom(n, 1, pY0),\n    ),\n    # assuming a constant effect of +10 percentage points\n    # among the compliers, and no average effect under the never-takers\n    Y1 = case_when(\n      complier == 0 ~ rbinom(n, 1, pY_nc),\n      complier == 1 ~ rbinom(n, 1, pY1)\n    ), \n    # treatment assigned at random\n    # 70-30 split into treatment / control\n    Z = rbinom(n, 1, prob = p_treatment),\n    # treatment uptake depends on \n    # being assigned to treatment (Z = 1)\n    # AND being a complier (C = 1)\n    A = Z * complier,\n    # generate observed response using the \n    # consistency equation\n    Y = (1 - Z) * Y0 + Z * Y1\n  )\n\n  return(data)\n}\n\n# creating these variables as they'll be helpful later on\n# population size\nn <- 100000L\n\n# P[Y^0 = 1]\npY0 <- 0.01\n# P[Y^1 = 1], ATE of +10 pct. pts.\npY1 <- pY0 + 0.1\n\ndata <- simulate_data(n = n, pY0 = pY0, risk_diff = 0.1)\n\nglimpse(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 100,000\nColumns: 7\n$ id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ complier <int> 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1…\n$ Y0       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Y1       <int> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ Z        <int> 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1…\n$ A        <int> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1…\n$ Y        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n```\n:::\n:::\n\n\nIn this simulated dataset, we've got information on compliance and the potential outcome under each treatment at the individual level. However, from a real experiment, only the $Z, A, Y$ columns would be observed.\n\nThe [*exposition pipe*](https://magrittr.tidyverse.org/reference/exposition.html) `%$%` operator -- similar to the pipe `%>%` operator -- is exported from the *magrittr* package and used with `base::table()` to expose the variables in the data frame to the table function to produce contingency tables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(magrittr, include.only = \"%$%\")\n\ndata %$% \n  table(complier, Z) %>% \n  prop.table(margin = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Z\ncomplier         0         1\n       0 0.1974162 0.2003373\n       1 0.8025838 0.7996627\n```\n:::\n:::\n\n\nSince the treatment $Z$ is randomly assigned, the proportion of compliers in each group is nearly the same as expected.\n\nThere are several effects that can be estimated for this problem each with its own advantages and disadvantages.\n\n## Intention-to-treat (ITT)\n\nThe *intention-to-treat* (ITT) effect is the *effect of* *being assigned to the treatment* instead of the *effect of the treatment itself*. These are identical when treatment compliance / adherence is perfect, i.e, when all the individuals only take the treatment they are assigned to, but not otherwise.\n\nFor this problem, the ITT analysis would analyse individuals based on the group they were assigned to, and not the treatment they ultimately received. Those in the treatment group who didn't see a single ad would be analysed as part of the treatment group rather than being excluded from the analysis, or being analysed as part of the control group.\n\nAn advantage of an ITT analysis is that randomization preserves the balance of confounders in both the treatment and control groups, so the $Z-Y$ association remains unconfounded and a valid, albeit conservative[^5] effect of assigning the treatment at the population level. However, this validity would be affected[^6] if this ad is rolled out to another target population -- a different period in time, or another geographical location -- with a different proportion of (non-)compliers[^7].\n\n[^5]: Section 22.1 of the what-if book mentions that while the ITT estimate is usually conservative, it's not guaranteed to be attenuated compared to the *per-protocol effect* (described in the next section) for *1)* *non-inferiority trials, or 2) if the per-protocol effect is not monotonic for all individuals and non-compliance is high*. It also lists other counterarguments (e.g., lack of transportability) against the ITT effect.\n\n[^6]: Rerunning the data simulation chunk above with `simulate_data(p_compliers = 0.4) %>% mutate(diff = Y1 - Y0) %>% pull(diff) %>% mean()` instead of 0.8 leads to an ITT estimate of +4 instead of +8 percentage points, even though the effect of the treatment itself is still +10 percentage points.\n\n[^7]: For a garden-variety ad campaign, the proportion of compliers could be effectively random, unless the ad in question is visually appealing, or contentious. On the contrary, for something like a vaccine in a global pandemic, compliance could vary wildly between the trial and rollout at the population level. Source: reading internet discussions between 2021-2022.\n\n### Estimators / models\n\nSince both $Y$ and $Z$ are binary variables, an estimate of the ITT effect can be obtained by fitting a simple logistic regression model, and using the `marginaleffects` package to perform *g-computation / (marginal / model-based) standardization* to get the risk difference (and the associated CIs)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n  glm(Y ~ Z, family = binomial(link = \"logit\"), data = .) %>%\n  marginaleffects::avg_comparisons(variables = \"Z\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n Term Contrast Estimate Std. Error  z Pr(>|z|)   S  2.5 % 97.5 %\n    Z    1 - 0   0.0807     0.0012 67   <0.001 Inf 0.0783  0.083\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n```\n:::\n:::\n\n\nor by fitting an *identity-link logistic regression* where the estimated coefficient for $Z$ can be directly interpreted as the risk difference\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n  glm(Y ~ Z, family = binomial(link = \"identity\"), data = .) %>% \n  broom::tidy(conf.int = TRUE) %>% \n  mutate(across(.cols = where(is.numeric), .fns = ~ round(.x, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n1 (Intercept)   0.0087    0.0005      16.2       0   0.0077    0.0098\n2 Z             0.0807    0.0012      67.0       0   0.0783    0.083 \n```\n:::\n:::\n\n\nThe use of glm + g-computation here is a bit overkill, as the same estimate can be obtained by looking at the 2x2 table of proportions scaled to sum to 1 within each column\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %$%\n  table(Y, Z) %>% \n  print() %>% \n  prop.table(margin = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Z\nY       0     1\n  0 29772 63715\n  1   261  6252\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n   Z\nY             0           1\n  0 0.991309559 0.910643589\n  1 0.008690441 0.089356411\n```\n:::\n:::\n\n\nand taking the difference of $P[Y = 1 | Z]$ in the two groups to give the risk difference $\\hat{p}_1 - \\hat{p}_0$, where $\\hat{p}_0 = \\text{Pr}[Y = 1 | Z = 0]$ and $\\hat{p}_1 = \\text{Pr}[Y = 1 | Z = 1]$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(0.089356411 - 0.008690441, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0807\n```\n:::\n:::\n\n\nAdditionally, a wald-type CI can be obtained using the formula for variance mentioned (using counts) [here](https://en.wikipedia.org/wiki/Risk_difference) or (using proportions) [here](https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_confidence_intervals/bs704_confidence_intervals7.html)\n\n$$\nSE(\\hat{p}_1 - \\hat{p}_0) = \\sqrt{\\frac{\\hat{p}_0 (1 - \\hat{p}_0)}{n_0} + \\frac{\\hat{p}_1 (1 - \\hat{p}_1)}{n_1}}\n$$\n\nwhich results in the same standard error\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nvar_p0 <- (0.008690441 * (1 - 0.008690441)) / (261 + 29772)\nvar_p1 <- (0.089356411 * (1 - 0.089356411)) / (6252 + 63715)\n\nround(sqrt(sum(var_p0, var_p1)), digits = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0012\n```\n:::\n:::\n\n\n[This paper](https://academic.oup.com/aje/article/189/6/508/5812650) mentions other ways of estimating the risk difference from different models, e.g. linear regression + sandwich estimator for the standard errors, which are again identical to the ones from the other methods above\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_mod <- lm(Y ~ Z, data = data) \n\nlm_mod %>% \n  broom::tidy() %>% \n  select(term, estimate, model_SE = std.error) %>% \n  mutate(robust_SE = sqrt(diag(sandwich::vcovHC(lm_mod))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  term        estimate model_SE robust_SE\n  <chr>          <dbl>    <dbl>     <dbl>\n1 (Intercept)  0.00869  0.00141  0.000536\n2 Z            0.0807   0.00168  0.00120 \n```\n:::\n:::\n\n\nHowever, getting a distribution of this effect as opposed to just a 95% CI can be more informative from a decision making point of view. This distribution can be obtained three ways -- 1) plugging the estimate and its standard error into a normal distribution and simulating effect sizes from that; 2) using the (nonparametric) bootstrap to resample the data and getting the bootstrap distribution of effect sizes; 3) using a Bayesian model to get the posterior distributions of the parameters from the model.\n\nFor the rest of the analyses, I pick option 3, where the simple (Bayesian) [*Beta-Binomial* model](https://en.wikipedia.org/wiki/Beta-binomial_distribution#Beta-binomial_in_Bayesian_statistics) is fit to this data separately within the treatment and the control groups. Taking the difference of these two (Beta) posterior distributions produces the posterior distribution of the risk difference. The `bayesAB` package in R can quickly fit Beta-Binomial models, or the `brms` package can be used for more general models.\n\n### Back to ITT analysis\n\nA $\\text{Beta}(\\alpha = 1, \\beta = 9)$ prior is specified for the probability of success in each arm reflecting the knowledge that the we can expect the conversion in each group to be somewhere between 0%-70%. Normally this depends on the product in question -- expensive and / or infrequently bought products are not expected to lead to very large conversion rates.\n\nThe following shows a summary of 10,000 draws from the prior distribution for the parameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(rbeta(1e5, 1, 9))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0000017 0.0313705 0.0740948 0.1002186 0.1430362 0.7405391 \n```\n:::\n:::\n\n\nwhich implies the following prior distribution for the risk difference\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(rbeta(1e5, 1, 9) - rbeta(1e5, 1, 9))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.6858270 -0.0709874 -0.0000165  0.0000020  0.0699090  0.6885754 \n```\n:::\n:::\n\n\nA priori, we expect the difference to be pretty small with mean of close to 0, but allowing for large differences of +/- 60-70%. We can also visualize these distributions\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbind_rows(\n  tibble(x = rbeta(1e5, 1, 9), cat = \"Prior conversion probability\"), \n  tibble(x = rbeta(1e5, 1, 9) - rbeta(1e5, 1, 9), \n         cat = \"Prior risk difference\")\n) %>% \n  ggplot(aes(x = x, group = cat, color = cat)) + \n  geom_density() + \n  theme_classic() + \n  xlab(\"\") + \n  scale_x_continuous(labels = scales::percent) +\n  facet_wrap(~ cat, scales = \"free\") + \n  guides(color = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nTo get the posterior distribution for conversion in each arm, the counts from the 2x2 contingency table can be plugged into the formula for Beta posterior distribution $\\text{Beta}(y + \\alpha, n - y + \\beta)$, where $y$ corresponds to the count for $Y = 1$, and $n-y$ for $Y = 0$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %$% table(Y, Z)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Z\nY       0     1\n  0 29772 63715\n  1   261  6252\n```\n:::\n:::\n\n\nThis gives $p (\\alpha_0, \\beta_0|y) \\sim \\text{Beta}(261 + 1, 29772 + 9) = \\text{Beta}(262, 29781)$ for the control group, and $p (\\alpha_1, \\beta_1|y) \\sim \\text{Beta}(6252 + 1, 63715 + 9) = \\text{Beta}(6253, 63724)$ for the treatment group, visualized as follows\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nposterior_categories <- c(\n  \"Conversion probability \\n(control group)\", \n  \"Conversion probability \\n(treatment group)\",\n  \"Risk difference\"\n)\n\nsample_from_beta_posterior <- function(theta_0, theta_1, \n                                       seed = 23, n_samples = 1e5) {\n  set.seed(seed)\n  posterior_control <- rbeta(n_samples, theta_0[1], theta_0[2])\n  posterior_treatment <- rbeta(n_samples, theta_1[1], theta_1[2])\n  risk_difference <- posterior_treatment - posterior_control\n  \n  tibble(\n    id = rep(1:n_samples, times = 3),\n    cat = rep(posterior_categories, each = n_samples),\n    x = c(posterior_control, posterior_treatment, risk_difference)\n  )\n}\n\ntrue_effect <- tibble(\n  x = c(NA_real_, NA_real_, pY1 - pY0), \n  cat = posterior_categories\n)\n\nplot_posteriors <- function(posterior, effect = true_effect) {\n  posterior %>% \n    ggplot(aes(x = x, group = cat, color = cat)) + \n    geom_density(trim = TRUE) + \n    geom_vline(data = effect, \n               aes(xintercept = x), \n               color = \"gold3\", \n               linetype = \"dashed\",\n               linewidth = 1.3,\n               na.rm = TRUE) + \n    theme_classic() + \n    theme(plot.margin = margin(0.5, 0.5, 0.5, 0.5, \"cm\")) + \n    xlab(\"Posterior distribution\") + \n    scale_x_continuous(labels = scales::label_percent(accuracy = 0.1)) +\n    facet_wrap(~ cat, scales = \"free\", ncol = 3) + \n    guides(color = \"none\")\n}\n\nITT_posterior <- sample_from_beta_posterior(\n  theta_0 = c(262, 29781), theta_1 = c(6253, 63724)\n)\n\nplot_posteriors(ITT_posterior)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nThe posterior for the risk difference completely fails to capture the underlying true effect size of +10 percentage points. However, since the dataset is simulated with the potential outcomes $\\{Y_{i}^{z = 0},\\ Y_{i}^{z = 1}\\}$ at the individual level, we can see that the estimate of +8 percentage points is correct for *this population*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrue_ITT_effect <- data %>% \n  mutate(diff = Y1 - Y0) %>% \n  pull(diff) %>% \n  mean() %>% \n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.08069\n```\n:::\n:::\n\n\nThe posterior distribution is summarized via the posterior mean, mode, and 95% (equal-tailed) credible intervals, which are nearly identical to the ones above.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nITT_risk_difference <- ITT_posterior %>% \n  filter(cat == \"Risk difference\") \n\n# get the posterior density to find the mode\n# based on this SE answer: \n# https://stackoverflow.com/a/13874750\nITT_risk_difference_density <- density(ITT_risk_difference$x)\nITT_risk_difference_mode <- ITT_risk_difference_density$x[which.max(ITT_risk_difference_density$y)]\n\nITT_risk_difference %>% \n  summarize(\n    mean = mean(x), \n    lower = quantile(x, probs = 0.025),\n    upper = quantile(x, probs = 0.975)\n  ) %>% \n  mutate(mode = ITT_risk_difference_mode, .after = mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n    mean   mode  lower  upper\n   <dbl>  <dbl>  <dbl>  <dbl>\n1 0.0806 0.0808 0.0783 0.0830\n```\n:::\n:::\n\n\nJust for fun, the Bayesian posterior distribution is visually compared to the normal approximation and the bootstrap distribution for the risk difference\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nboot_ITT_fun <- function(data, indx, ...) {\n  data <- data[indx, ]\n  data %>% \n    count(Z, Y) %>% \n    # get P[Y = 1] within levels of Z\n    group_by(Z) %>% \n    mutate(p = n / sum(n)) %>% \n    ungroup() %>% \n    # only keep P[Y = 1 | Z]\n    filter(Y == 1) %>% \n    pull(p) %>% \n    # get |p1 - p0|\n    reduce(.f = `-`) %>% \n    abs()\n} \n\nITT_boot <- boot::boot(data = data, \n                       statistic = boot_ITT_fun, \n                       R = 999, \n                       # multicore only works on linux\n                       parallel = \"multicore\")\n\n# plot the three distributions\nset.seed(23)\nbind_rows(\n  tibble(x = rnorm(999, 0.0807, 0.0012), y = \"Normal approximation\"), \n  tibble(x = ITT_boot$t[, 1], y = \"Bootstrap distribution\"), \n  ITT_posterior %>% \n    filter(cat == \"Risk difference\") %>% \n    rename(y = cat) %>% \n    slice_sample(n = 999) %>% \n    mutate(y = \"Beta posterior\")\n) %>% \n  ggplot(aes(x = x, y = y, group = y, fill = y)) + \n  ggdist::stat_halfeye() +\n  theme_classic() + \n  theme(legend.position = \"none\", \n        plot.margin = margin(0.5, 0.5, 0.5, 0.5, \"cm\")) + \n  xlab(\"Distribution of risk difference / uplift (999 samples)\") + \n  ylab(\"\") + \n  scale_x_continuous(labels = scales::label_percent(accuracy = 0.1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nAs expected, despite being philosophically different, all of these methods give nearly identical results (increasing the samples from 999 to 100,000[^8] would lead to them being virtually identical), but all of these underestimate the correct treatment effect of +10 percentage points.\n\n[^8]: Using the boot package here complains about memory issues when trying to set R = 100,000, and I'm too lazy to write a (parallel) map here myself.\n\n## Per-protocol (PP)\n\nThe *per-protocol* (PP) effect is generated by including all the people who adhere to the *protocol*, i.e., those with treatment status $A$ identical to the assignment treatment $Z$. Contrary to the ITT effect from the previous section, this effect is a measure of the effectiveness of the actual treatment itself, although this effect estimate is prone to bias.\n\nSince it's not possible for us to measure[^9] an individual seeing an ad if they were assigned to the control group, i.e, individuals with $Z = 0, A = 1$, this would only exclude all the individuals with $Z = 1$ and $A = 0$, i.e., about 14,000 individuals from the treatment group who didn't see an ad.\n\n[^9]: An individual in the control group cannot see an ad, if the ad is not shown to the individual.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %$% table(A, Z)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Z\nA       0     1\n  0 30033 14017\n  1     0 55950\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% filter(A == Z) %$% table(A, Z)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Z\nA       0     1\n  0 30033     0\n  1     0 55950\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n  # only restrict analysis to those with perfect adherence / compliance\n  filter(A == Z) %$% \n  table(Y, A) %>% \n  print() %>% \n  sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   A\nY       0     1\n  0 29772 49716\n  1   261  6234\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 85983\n```\n:::\n:::\n\n\nSImilar to the previous section, the posterior probability of success / conversion and the risk difference for the PP analysis can be obtained by using the same beta-binomial model from the previous section and visually assessed\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPP_posterior <- sample_from_beta_posterior(\n  theta_0 = c(261 + 1, 29772 + 9), \n  theta_1 = c(6234 + 1, 49716 + 9)\n)\n\nplot_posteriors(PP_posterior)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nThe PP effect is exaggerated -- albeit by a relatively small margin -- compared to both the ITT effect and the true treatment effect of +10 percentage points.\n\n## As-treated (AT)\n\nThe *as-treated* effect (AT) estimates the effect of analyzing individuals by treatment received. This analysis includes individuals in the group corresponding to the treatment actually received. So the individuals randomized to the treatment group who did not see an ad are included in the control group.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %$% table(Y, A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   A\nY       0     1\n  0 43771 49716\n  1   279  6234\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nAT_posterior <- sample_from_beta_posterior(\n  theta_0 = c(279 + 1, 43771 + 9), \n  theta_1 = c(6234 + 1, 49716 + 9)\n)\n\nplot_posteriors(AT_posterior)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nLike the PP effect, this effect is a measure of the effectiveness of the actual treatment itself. By including the untreated from the treatment group -- who had a lower probability of conversion -- into the control group, the overall conversion probability in the control group is attenuated thereby leading to an exaggeration of the ATE. Thus, it overestimates the true effect and is likewise exaggerated compared to the ITT effect estimate. It is similarly prone to bias.\n\nWe've produced three estimates so far, and since we know the true effect of treatment here, we can see that they're all biased. Great! The analysis could stop here and all the three estimates could be presented with their respective caveats.\n\nBut can we do better (in this experiment)?\n\n## Instrumental variable analysis (IV)\n\nThe fourth and final method of analysis looks at the method of *instrumental variables*. Causal estimates for the treatment effect can be obtained by using just the three variables $\\{Z, A, Y\\}$ if the assumptions behind this method are plausible for our (fictional) ad campaign.\n\nFor an instrumental variable analysis to be valid, we need an *instrument* (say $Z$) that needs to satisfy the first three conditions and at least the fourth or the fifth condition\n\n-   *Relevance condition* -- Be correlated (hopefully strongly) with the treatment variable (in our case $A$)\n\n-   *Ignorability -* Be uncorrelated with all measured and unmeasured confounders ($U$) of the $A \\rightarrow Y$ relationship and share no common causes with $Y$\n\n-   *Exclusion restriction* -- have no direct impact on the outcome $Y$, only an indirect effect entirely mediated via the treatment $A$\n\n-   *(Weak) Homogeneity* -- for a binary $Z-A$ system, if $Z$ is not an *effect modifier* of the $A-Y$ relationship, i.e.,\\\n    $\\mathbb{E}[Y^{a = 1} - Y^{a = 0} | Z = 1, A = a] = \\mathbb{E}[Y^{a = 1} - Y^{a = 0} | Z = 0, A = a]$ then the IV estimate can be interpreted as the *average treatment effect among the treated (ATT)* in the population. If the ATT is identical to the *ATU(ntreated),* then the IV estimate can be interpreted as the *ATE in the population.* Other homogeneity conditions are described in section 16.3 of the what-if book. Appendix 2.1 of the Hernán, Robins 2006 paper has some stuff on this too.\n\n-   *Monotonicity* - The trial population can be partitioned into four-latent subgroups / *prinicipal strata* defined by a combination of $A, Z$ - *always takers, compliers, never-takers, and defiers*. Always (or never) takers are people who would always (or never) take the treatment independent of group assignment. Compliers would comply with the treatment corresponding to the group they are assigned to. Defiers would do the opposite to the group they're assigned to. In the absence of defiers and always takers, the IV estimand is the *complier average causal effect (CACE)* or the *local average treatment effect (LATE).* This can be written as\n\n    $$\n    \\mathbb{E}[Y^{a = 1} - Y^{a = 0} | A^{z = 1} \\gt A^{z = 0}] = \\frac{\\mathbb{E}[Y = 1 | Z = 1] - \\mathbb{E}[Y = 1 | Z = 0]}{\\mathbb{E}[A = 1 | Z = 1] - \\mathbb{E}[A = 1 | Z = 0]}\n    $$\n\nThe variable $Z$ from the DAG for this problem\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot(dag)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nseems to fulfill the first three conditions, since individuals are randomly assigned into treatment and control groups. This makes it a *causal instrument*, as an individual cannot choose to see an ad unless they're assigned to the treatment group.\n\nRegarding homogeneity, it can be argued that the assignment of an individual[^10] to the group cannot be an effect modifier, since this is done by a random number generator and so should not affect the potential outcomes.\n\n[^10]: In the presence of *interference*, e.g. individuals in the same household being assigned to different groups but affecting each others' potential outcomes, the unit of assignment and analysis would be at the household level.\n\nSince the ad is randomized at the individual level, there can never be any defiers (i.e., individuals assigned to the control group can never deliberately switch over to the treatment group). Always takers are also not an option, since people assigned to the control group cannot choose to take the treatment[^11]. This essentially leaves the population to be a mix of never-takers and compliers.\n\n[^11]: Not that anyone would *choose* to see an ad.\n\n### IV in action\n\nSince the instrument, treatment, and outcome are all binary, we can use the following *ratio / Wald estimator* for the IV estimand\n\n$$\n\\text{IV} = \\frac{\\text{Pr}[Y = 1 | Z = 1] - \\text{Pr}[Y = 1 | Z = 0]}{\\text{Pr}[A = 1 | Z = 1] - \\text{Pr}[A = 1 | Z = 0]}\n$$\n\nwhere the numerator is the ITT effect estimate defined a few sections above, and the denominator is a measure of compliance on the risk difference scale with regards to the assigned treatment, i.e., an association between treatment assignment $Z$ and treatment uptake $A$.\n\nDepending on whether we invoke one of the homogeneity assumptions or the monotonicity assumption, we end up with either the average causal effect in the treated, or in the population, or the average causal effect among the compliers. All the different assumptions for this are described in the Swanson et al 2018 article.\n\nTo fit this Beta-binomial model to the data, we can use the posterior distribution of the risk difference from the ITT effect for the numerator. For the denominator, we need to get a similar distribution of treatment compliance. Looking at the 2x2 $Z-A$ table,\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %$% table(A, Z)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Z\nA       0     1\n  0 30033 14017\n  1     0 55950\n```\n:::\n:::\n\n\nthe group of $\\text{Pr}[A = 1 | Z = 0]$ is (expected to be) 0, since those assigned to the control group should not be able to see the ad. We can pick a *degenerate* (Beta) prior distribution[^12] with $\\alpha = 0,\\ \\beta > 0$, e.g., $\\text{Beta}(0, 1)$, which results in a Beta posterior with shape $\\alpha = 0 + 0 = 0,\\ \\beta = 30033 + 1 = 30034$. Since $\\alpha = 0$, all draws from this posterior distribution are exactly zero.\n\n[^12]: Since I'm not using Stan to fit the model here, I can get away with this. Using Stan, I'd pick something like a $\\text{Beta}(1, 10000)$ distribution that puts a very small non-zero probability for $\\text{Pr}[A = 1 | Z = 0]$. I don't know for sure whether the degenerate prior would cause issues for Stan to be honest.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(rbeta(1e2, 0, 30034))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n```\n:::\n:::\n\n\nFor $\\text{Pr}[A = 1 | Z = 1]$, we can expect the compliance to be reasonably high, so the $\\text{Beta}(7, 3)$ with a mean compliance of 70%, and a range of 10%-100% might be a good choice. With the large sample sizes in the dataset, the prior is going to be dominated by the likelihood anyway.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(rbeta(1e4, 7, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.09285 0.60786 0.71188 0.69917 0.80307 0.98952 \n```\n:::\n:::\n\n\nUsing the counts from the $A-Z$ table above, we get the $\\text{Beta}(55957, 14020)$ posterior with $\\alpha = 7 + 55950$ and $\\beta = 3 + 14017$. Taking a difference of these two posteriors would leave this posterior unchanged, so we can produce the corresponding IV posterior distribution\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(23)\ncompliance_posterior <- rbeta(nrow(ITT_risk_difference), 55957, 14020)\n\nIV_labels <- c(\"Intention-to-treat effect\", \n               \"Proportion of compliers\", \n               \"Instrumental variable estimate\")\n\nIV_posterior <- tibble(\n  id = rep(1:nrow(ITT_risk_difference), times = 3),\n  cat = factor(rep(IV_labels, each = nrow(ITT_risk_difference)), \n               levels = IV_labels),\n  x = c(ITT_risk_difference$x, compliance_posterior, \n        ITT_risk_difference$x / compliance_posterior)\n)\n\ntrue_effect_IV <- true_effect %>% \n  mutate(cat = factor(IV_labels, levels = IV_labels))\n\nplot_posteriors(IV_posterior, true_effect_IV)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nIV_posterior %>% \n  filter(cat == IV_labels[3]) %>% \n  summarize(mean = mean(x), \n            lower = quantile(x, probs = 0.025), \n            upper = quantile(x, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n   mean  lower upper\n  <dbl>  <dbl> <dbl>\n1 0.101 0.0979 0.104\n```\n:::\n:::\n\n\nCompared to the ITT, PP, and AT estimates, the IV estimate is the closest to the true treatment effect of +10 percentage points, which makes sense as all the IV conditions are valid here due to the use of a randomized experiment with the causal instrument $Z$. However, if any of the conditions (monotonicity, relevance, etc.) were to weaken, this would lead to bias in the IV estimate.\n\nSome recommend producing bounds[^13] for the IV estimate since it's usually not *point identified.* For the case of binary $Z, A, Y$, we can use `ivtools::ivbounds()` to produce the *natural bounds* -- which are between 8%-28% for the ATE (`CRD` in the result)\n\n[^13]: Note to (future) self: look at the Balke, Pearl 1997 and Swanson et al 2018 papers, and the `causaloptim` R package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n  ivtools::ivbounds(data = ., \n                    Z = \"Z\", X = \"A\", Y = \"Y\", \n                    monotonicity = TRUE) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:  \nivtools::ivbounds(data = ., Z = \"Z\", X = \"A\", Y = \"Y\", monotonicity = TRUE)\n\nThe IV inequality is not violated\n\nSymbolic bounds:\n   lower upper  \np0 p10.0 1-p00.0\np1 p11.1 1-p01.1\n\nNumeric bounds:\n       lower    upper\np0   0.00869  0.00869\np1   0.08910  0.28944\nCRD  0.08041  0.28075\nCRR 10.25255 33.30515\nCOR 11.15758 46.46413\n```\n:::\n:::\n\n\nThe width of the bounds is 20 percentage points, which matches the formula for the width[^14] -- $\\text{Pr}[A = 1 | Z = 0] + \\text{Pr}[A = 0 | Z = 1]$ -- where the former is 0, since we have no defiers, and 20% for the second quantity, which is the proportion of never-takers.\n\n[^14]: given in some of the references below\n\nThe slight difference between the true value and the estimate is due to sampling variability, which can be assessed by simulating multiple datasets with different seeds to give a mean ATE *very* close to +10 percentage points\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmap_dbl(\n  .x = 1:100, \n  .f = ~ {\n    .x %>% \n      simulate_data(seed = .) %>% \n      mutate(diff = (Y1 - Y0) / 0.8) %>%\n      pull(diff) %>% \n      mean()\n    }) %>% mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0999695\n```\n:::\n:::\n\n\n## All the estimates in a single plot\n\nWe can look at all four estimates in a single plot\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlist(\n  \"Instrumental variable\" = IV_posterior, \n  \"As-treated\" = AT_posterior, \n  \"Per-protocol\" = PP_posterior, \n  \"Intention-to-treat\" = ITT_posterior\n) %>% \n  map2_dfr(.x = ., .y = names(.), .f = ~ {\n    .x %>% \n      filter(str_detect(cat, \"Risk|Instrument\")) %>% \n      mutate(cat = {{ .y }})\n    }) %>% \n  mutate(\n    cat = factor(cat, levels = c(\n      \"Instrumental variable\", \"As-treated\",\n      \"Per-protocol\", \"Intention-to-treat\"\n  ))) %>% \n  ggplot(aes(x = x, y = cat, group = cat, fill = rev(cat))) + \n  ggdist::stat_halfeye(\n    show.legend = FALSE\n  ) +\n  geom_vline(\n    xintercept = pY1 - pY0, \n    color = \"gray50\", \n    linetype = \"dashed\",\n    linewidth = 1.3\n  ) +\n  geom_vline(\n    xintercept = true_ITT_effect, \n    color = \"gray20\", \n    linetype = \"dotted\",\n    linewidth = 1.3\n  ) +\n  theme_classic() + \n  xlab(paste0(\n    \"Posterior distribution of risk difference / uplift\\n\", \n    \"(Dotted line: ATE in this population; dashed line: true ATE)\"\n  )) + \n  ylab(\"\") + \n  scale_x_continuous(labels = scales::label_percent(accuracy = 0.1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\nThe IV estimate is the closest to the true treatment effectiveness of +10 percentage points (in gray), with the AT and the PP effects both showing some bias.\n\nThe simulated data analyzed here are from a relatively clean setting where the treatment effectiveness, sample sizes, and strength of association between A-Z are all very high. If any of these conditions weaken, the analysis may become more challenging.\n\n## Profit distribution\n\nSo we have two valid estimates -- the ITT and the IV estimate. Can we use these estimates to calculate the total profit from this campaign, as well as other hypothetical campaigns with varying treatment / control splits and compliance?\n\nThis assumes that\n\n-   the effect of treatment among the compliers and the never-takers would be the same\n\n-   the conversion probabilities specified below would apply to these other populations\n\n-   compliance varies randomly between populations\n\nData from other experiments, if available, can be used to get more realistic estimates by capturing the heterogeneity in the estimated probabilities (e.g., by meta-analysis).\n\nTo estimate the total profit, we need the following probability of conversion\n\n$$\n\\text{Pr}[Y = 1] = \\sum_z \\text{Pr}[Y = 1 | Z = z] \\text{Pr}[Z = z] \n$$\n\nwhere $\\text{Pr}[Z = z]$ is the proportion of people assigned to treatment ($Z = 1$) and control ($Z = 0$), and the $\\text{Pr}[Y = 1 | Z = z]$ is the ITT estimate[^15] in each arm. The ITT estimate under the monotonicity assumption can be further decomposed -- using the [*law of total probability*](https://en.wikipedia.org/wiki/Law_of_total_probability) -- into a weighted sum of the conversion probabilities among the compliers and the never-takers[^16]\n\n[^15]: Technically an *estimand*, but feels odd to type out 'estimand' everywhere in this and following paragraphs instead of 'estimate'.\n\n[^16]: The $\\text{Pr}[Z = z]$ term in the numerator on the right side of the equation is constant with respect to the summation (over $c$), so can be pulled out and cancels out the same term in the denominator.\n\n$$\n\\text{Pr}[Y = 1 | Z = z] = \\sum_c \\text{Pr}[Y = 1 | Z = z, C = c] \\text{Pr}[C = c | Z = z] \n$$\n\nwhere $\\text{Pr}[C = c | Z = z] = \\text{Pr}[C = c]$ because $Z$ is randomly assigned, so $Z \\perp\\!\\!\\!\\perp C$, i.e., $Z$ and $C$ are independent.\n\nOur best estimate of compliance is given by\n\n$$\n\\text{Pr}[C = 1] = \\text{Pr}[A = 1 | Z = 1] - \\text{Pr}[A = 1 | Z = 0]\n$$\n\nand $\\text{Pr}[C = 0] = 1 - \\text{Pr}[C = 1]$ is the proportion of never-takers. To get the conditional probability in the first term in the sum, we can rewrite the previous equation -- shown only for $Z = 1$ but is the same for $Z = 0$ -- after diving each term by $\\text{Pr}[C = 1]$\n\n$$\n\\begin{split}\n\\frac{\\text{Pr}[Y = 1 | Z = 1]}{\\text{Pr}[C = 1]} & = \\text{Pr}[Y = 1 | Z = 1, C = 1] \\\\ & + \\text{Pr}[Y = 1 | Z = 1, C = 0] \\frac{\\text{Pr}[C = 0]}{\\text{Pr}[C = 1]} \n\\end{split}\n$$\n\nThe term on the left is the ITT estimate rescaled by the proportion of compliers. The first term on the right is the conversion probability among the compliers who received the treatment $\\text{Pr}[Y = 1 | Z = 1, C = 1]$, and the second term on the right is the probability of conversion among the never-takers who were assigned to the treatment, weighted by the ratio of never-takers to compliers.\n\nThis also shows why subtracting the two rescaled ITT estimates (i.e., the IV estimate) is the effect of treatment among the compliers -- the second term on the right cancels out (under the assumption of exclusion restriction) since $Z$ is randomly assigned, so the never-takers in each arm should have the same probability of conversion\n\n$$\n\\text{Pr}[Y = 1 | Z = 1, C = 0] = \\text{Pr}[Y = 1 | Z = 0, C = 0]\n$$\n\nand proportion of compliers $\\text{Pr}[C = 1 | Z = 1] = \\text{Pr}[C = 1 | Z = 0]$, and we're left with\n\n$$\n\\text{Pr}[Y = 1 | Z = 1, C = 1] - \\text{Pr}[Y = 1 | Z = 0, C = 1]\n$$\n\nwhich is the CACE from the IV analysis.\n\nWhile compliance ($C$) is unobserved, we can estimate all these probabilites using the combined information present in $Z, A, Y$ along with the assumptions in the previous paragraphs.\n\nFor the proportion of compliers ($\\text{Pr}[C = 1]$), we can use the posterior distribution we obtained for the IV analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop_compliers <- IV_posterior %>% \n  filter(cat == \"Proportion of compliers\") %>% \n  select(id, p_compliers = x)\n\nprop_compliers %>%\n  mutate(p_never_takers = 1 - p_compliers) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       id          p_compliers     p_never_takers  \n Min.   :     1   Min.   :0.7913   Min.   :0.1933  \n 1st Qu.: 25001   1st Qu.:0.7986   1st Qu.:0.1993  \n Median : 50000   Median :0.7997   Median :0.2003  \n Mean   : 50000   Mean   :0.7996   Mean   :0.2004  \n 3rd Qu.: 75000   3rd Qu.:0.8007   3rd Qu.:0.2014  \n Max.   :100000   Max.   :0.8067   Max.   :0.2087  \n```\n:::\n:::\n\n\nFor the probability of conversion among the never takers, we can calculate this from information among the untreated assigned to the treatment group. A $\\text{Beta}(1, 1)$ prior can be assumed here to get the following $\\text{Beta}(18 + 1, 13999 + 1) = \\text{Beta}(19, 14000)$ posterior\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% filter(A != Z) %$% table(Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nY\n    0     1 \n13999    18 \n```\n:::\n\n```{.r .cell-code}\npY_never_takers <- tibble(\n  id = 1:1e5, \n  pY_nt = rbeta(1e5, 19, 14000)\n)\n\npY_never_takers %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       id             pY_nt          \n Min.   :     1   Min.   :0.0004274  \n 1st Qu.: 25001   1st Qu.:0.0011353  \n Median : 50000   Median :0.0013325  \n Mean   : 50000   Mean   :0.0013561  \n 3rd Qu.: 75000   3rd Qu.:0.0015507  \n Max.   :100000   Max.   :0.0031606  \n```\n:::\n:::\n\n\nFor the ITT estimates, the posterior probabilities from the ITT analysis above are used\n\n\n::: {.cell}\n\n```{.r .cell-code}\nITT_posterior_subset <- ITT_posterior %>% \n  filter(cat != \"Risk difference\") %>% \n  pivot_wider(id_cols = id, names_from = cat, values_from = x) %>% \n  rename(\n        \"p_control\" = \"Conversion probability \\n(control group)\", \n        \"p_treated\" = \"Conversion probability \\n(treatment group)\"\n    ) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 100,000\nColumns: 3\n$ id        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ p_control <dbl> 0.008838875, 0.008459186, 0.009314263, 0.009458440, 0.008553…\n$ p_treated <dbl> 0.09029532, 0.08987783, 0.09138095, 0.08802061, 0.09105225, …\n```\n:::\n:::\n\n\nPutting these together gives\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_draws <- list(\n  prop_compliers, pY_never_takers, ITT_posterior_subset\n  ) %>% \n  reduce(.f = ~ full_join(.x, .y, by = \"id\")) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 100,000\nColumns: 5\n$ id          <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ p_compliers <dbl> 0.7993178, 0.8003960, 0.7980252, 0.7976448, 0.8001245, 0.7…\n$ pY_nt       <dbl> 0.0010972405, 0.0010973038, 0.0017398618, 0.0014166189, 0.…\n$ p_control   <dbl> 0.008838875, 0.008459186, 0.009314263, 0.009458440, 0.0085…\n$ p_treated   <dbl> 0.09029532, 0.08987783, 0.09138095, 0.08802061, 0.09105225…\n```\n:::\n:::\n\n\nThe conversion probabilities among the compliers are calculated, which results in the data frame with all the posterior draws for the probabilities we need.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed in case a subset of the posterior draws are used\n# set.seed(23)\nposterior_draws <- posterior_draws %>% \n  mutate(\n    across(\n      .cols = c(p_treated, p_control), \n      # rescale the ITT probabilities by p(compliance) and\n      # shave off the effect of never takers on this probability\n      # to recover the p(Y = 1 | Z = z, C = 1)\n      .fns = ~ ((.x / p_compliers) - \n                  (pY_nt * ((1 - p_compliers) / p_compliers))), \n      .names = \"{.col}_compliers\"\n    )\n  ) %>% \n  # slice_sample(n = 5000) %>% \n  # select(-id)  %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 100,000\nColumns: 7\n$ id                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ p_compliers         <dbl> 0.7993178, 0.8003960, 0.7980252, 0.7976448, 0.8001…\n$ pY_nt               <dbl> 0.0010972405, 0.0010973038, 0.0017398618, 0.001416…\n$ p_control           <dbl> 0.008838875, 0.008459186, 0.009314263, 0.009458440…\n$ p_treated           <dbl> 0.09029532, 0.08987783, 0.09138095, 0.08802061, 0.…\n$ p_treated_compliers <dbl> 0.1126900, 0.1120181, 0.1140685, 0.1099912, 0.1135…\n$ p_control_compliers <dbl> 0.010782542, 0.010295105, 0.011231293, 0.011498576…\n```\n:::\n:::\n\n\nIn principle, any of these probabilities can be replaced with other priors[^17] when attempting to *transport* these estimates to the other scenarios.\n\n[^17]: rather than using the observed posteriors as priors.\n\nFor the plot below, there are two dimensions\n\n-   compliance with levels set to 50%, 80%, 100%\n\n-   treatment / control split\n\n    -   70/30% as in this experiment\n\n    -   0/100% - don't treat the population\n\n    -   100/0% - treat the full population\n\nIt feels a bit odd to look at the combination of compliance with 0% of the population assigned to treatment, as the (non-)compliers are only identified -- and the respective conditional probabilities estimated -- when at least some individuals are assigned to treatment. In this case, it's more of a hypothetical exercise using data from an experiment where \\>0% of the population was assigned to the treatment arm. Page 41 of the Greenland et al 1999 paper mentions\n\n> \\[...\\] noncompliance represents movement into a third untreated state that does not correspond to any assigned treatment (Robins, 1998).\n\nI haven't managed to go through the Robins 1998 paper yet but it would be interesting to dig into this further.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparameter_grid <- expand_grid(\n  compliance = c(0.5, 0.8, 1.0),\n  split = c(0.7, 1.0, 0.0), \n  n = n\n)\n```\n:::\n\n\nTotal profit distributions for each of these hypothetical experiments is calculated using the formulas specified above. To get the variability in the profit distribution, total sales $S \\sim \\text{BetaBin}(n, \\alpha, \\beta)$ can be sampled from the [*(beta-binomial) posterior predictive distribution*](https://en.wikipedia.org/wiki/Beta-binomial_distribution#Generating_beta_binomial-distributed_random_variables)*,* where the probability of conversion $p \\sim \\text{Beta}(\\alpha, \\beta)$ and $S \\sim \\text{Binomial}(n, p)$ under the different interventions in different settings. The total profit can be obtained from multiplying $S$ with the profit per unit, which can be fixed at (say) +100 euros.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprofit_distributions <- expand_grid(parameter_grid, posterior_draws) %>% \n  mutate(\n    # get the ITT estimates for the hypothetical campaigns\n    # under the different parameters\n    # pY_nt is scaled by proportion of compliers\n    # as this wasn't done above\n    p_treated_hyp = ((compliance * p_treated_compliers) + \n      ((1 - compliance) * (pY_nt / p_compliers))),\n    p_control_hyp = ((compliance * p_control_compliers) + \n      ((1 - compliance) * (pY_nt / p_compliers))),\n    n_treated = round(split * n), \n    n_control = n - n_treated, \n    s_treated = rbinom(n(), n_treated, p_treated_hyp), \n    s_control = rbinom(n(), n_control, p_control_hyp), \n    total_profit = 100 * (s_treated + s_control)\n  ) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 900,000\nColumns: 17\n$ compliance          <dbl> 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, …\n$ split               <dbl> 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, …\n$ n                   <int> 100000, 100000, 100000, 100000, 100000, 100000, 10…\n$ id                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ p_compliers         <dbl> 0.7993178, 0.8003960, 0.7980252, 0.7976448, 0.8001…\n$ pY_nt               <dbl> 0.0010972405, 0.0010973038, 0.0017398618, 0.001416…\n$ p_control           <dbl> 0.008838875, 0.008459186, 0.009314263, 0.009458440…\n$ p_treated           <dbl> 0.09029532, 0.08987783, 0.09138095, 0.08802061, 0.…\n$ p_treated_compliers <dbl> 0.1126900, 0.1120181, 0.1140685, 0.1099912, 0.1135…\n$ p_control_compliers <dbl> 0.010782542, 0.010295105, 0.011231293, 0.011498576…\n$ p_treated_hyp       <dbl> 0.05703136, 0.05669450, 0.05812436, 0.05588362, 0.…\n$ p_control_hyp       <dbl> 0.006077632, 0.005833028, 0.006705751, 0.006637289…\n$ n_treated           <dbl> 70000, 70000, 70000, 70000, 70000, 70000, 70000, 7…\n$ n_control           <dbl> 30000, 30000, 30000, 30000, 30000, 30000, 30000, 3…\n$ s_treated           <int> 3981, 3988, 4160, 3843, 3968, 3943, 3813, 3905, 40…\n$ s_control           <int> 186, 172, 209, 214, 199, 189, 204, 209, 183, 159, …\n$ total_profit        <dbl> 416700, 416000, 436900, 405700, 416700, 413200, 40…\n```\n:::\n:::\n\n\nThe posterior predictive distribution for the total profit is visually compared with simulated draws from the data generating process\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsimulated_profit <- parameter_grid %>% \n  expand_grid(id = seq(from = 20, by = 1, length.out = 20)) %>% \n  pmap_dfr(\n    .f = function(compliance, split, id, n) {\n      simulate_data(n = n,\n                    p_compliers = compliance, \n                    p_treatment = split,\n                    seed = id) %>%\n        summarise(total_profit = 100 * sum(Y)) %>% \n        tibble(compliance, split, .)\n    }\n  )\n\nprofit_distributions_plot_data <- list(\n  \"extrapolated\" = profit_distributions %>% \n    select(compliance, split, total_profit),\n  \"simulated\" = simulated_profit\n  ) %>% \n  map(\n    .f = ~ {\n      .x %>% \n        mutate(\n          total_profit = total_profit / 1e5, \n          compliance = paste0(\"Compliers: \", compliance * 100, \"%\"), \n          compliance = factor(compliance, \n                        levels = c(\"Compliers: 50%\",\n                                   \"Compliers: 80%\",\n                                   \"Compliers: 100%\")),\n          split = paste0(\"Treatment: \", split * 100, \"%\"), \n          split = factor(split, \n                         levels = c(\"Treatment: 0%\", \n                                    \"Treatment: 70%\", \n                                    \"Treatment: 100%\"))\n        )\n    }\n  )\n\nprofit_distributions_plot_data %>% \n  pluck(\"extrapolated\") %>% \n  ggplot(aes(x = total_profit)) + \n  geom_density(trim = TRUE) + \n  geom_vline(\n    data = profit_distributions_plot_data %>% pluck(\"simulated\"), \n    aes(xintercept = total_profit), color = \"maroon\"\n  ) + \n  theme_bw() + \n  facet_wrap(vars(split, compliance), scales = \"free\") + \n  #facet_grid(compliance ~ split, scales = \"fixed\") +\n  xlab(\"Total profit (multiples of 100,000 EUR, per 100,000 individuals)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\nThe middle column shows the total profit distribution for this experiment with a 70-30% split and 80% compliance. In this fictional scenario, the ad campaign has a strong positive impact on the total profits (90,000 under 0% treated and 80% compliance vs 900,000 under 100% treated) so with the gift of hindsight, it could have been rolled out to the full population.\n\nIf you're wondering why the posterior predictive distribution (black line) in each panel is not centered at the mean of the vertical lines (in maroon), it's from using the estimated probabilities from the original dataset which differ from the true probabilities because of sampling variability[^18].\n\n[^18]: This can be checked by using the true probabilities, i.e., `p_compliers = 0.8, pY_nt = 0.001, p_treated_compliers = 0.1, p_control_compliers = 0.01` instead of the posterior distributions for these probabilities.\n\n## Ignored complexities\n\nIn this post, I ignored the complexity where the exposure among the treated -- i.e., number of impressions of the ad -- would be continuous, possibly something like this\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(23)\ntibble(impressions = pmax(round(rgamma(5e4, shape = 4, scale = 3)), 1)) %>% \n  ggplot(aes(x = impressions)) + \n  geom_bar() +\n  #stat_ecdf() + \n  theme_classic() + \n  xlab(\"Impressions\") + \n  scale_x_continuous(breaks = seq(0, 50, 5), labels = seq(0, 50, 5))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\nIgnoring the actual impressions effectively assumes *treatment variation irrelevance*, i.e, seeing 1 impression is as effective as seeing 20 impressions with regards to conversion. This might be fun to explore in a future post on binary instruments, with a continuous exposure[^19]. Baiocchi et al. (section 12) describes an analysis with continuous treatments.\n\n[^19]: although all references mention that this complicates the analysis\n\nIssue with handling partial exposures, i.e., people with one or more partial impressions are also not tackled here.\n\nFor building more complex / full Bayesian IV models using brms / Stan, see [this](https://bookdown.org/content/4857/adventures-in-covariance.html#instruments-and-causal-designs), [this](https://avehtari.github.io/ROS-Examples/Sesame/sesame.html), [this](https://modernstatisticalworkflow.blogspot.com/2017/11/bayesian-instrumental-variables-with.html), or [this](https://rpsychologist.com/adherence-analysis-IV-brms).\n\n## References\n\nThese are the main references I'm using for this post[^20].\n\n[^20]: although I can't claim to have read and mastered all these.\n\n-   Simulating data for instrumental variables - [Andrew Gelman blog post link](https://statmodeling.stat.columbia.edu/2019/06/20/how-to-simulate-an-instrumental-variables-problem/), or [this link](https://bookdown.org/content/4857/adventures-in-covariance.html#instruments-and-causal-designs) to *statistical rethinking* models implemented using brms\n-   [Hernán, Robins](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/) - Causal Inference - What if? - Chapters 16 (instrumental variables), and 22 (sections on intention-to-treat and per-protocol analyses)\n-   [Gelman, Hill, Vehtari](https://avehtari.github.io/ROS-Examples/) - Regression and other stories, Sections 21.1-21.2\n-   [Hernán, Robins 2006](https://pubmed.ncbi.nlm.nih.gov/16755261/) - Instruments for causal inference: an epidemiologist's dream?\n-   [Burgess, Small, Thompson 2017](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5642006/) - A review of instrumental variable estimators for Mendelian randomization\n-   [Baiocchi, Cheng, Small 2014](https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6128) - Instrumental variable methods for causal inference\n    -   Good review paper for different IV situations\n-   [Sussman, Hayward 2010](https://pubmed.ncbi.nlm.nih.gov/20442226/) - An IV for the RCT: using instrumental variables to adjust for treatment contamination in randomised controlled trials\n-   [Naimi, Whitcomb 2020](https://academic.oup.com/aje/article/189/6/508/5812650) - Calculating risk differences and risk ratios from regression models\n-   [Imbens, Rubin 1997](https://projecteuclid.org/journals/annals-of-statistics/volume-25/issue-1/Bayesian-inference-for-causal-effects-in-randomized-experiments-with-noncompliance/10.1214/aos/1034276631.full) - Bayesian inference for causal effects in randomized experiments with noncompliance\n-   [Balke, Pearl 1997](https://arxiv.org/abs/1302.6784) - Counterfactual Probabilities: Computational Methods, Bounds and Applications\n-   [Swanson et al 2018](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6752717/) - Partial Identification of the Average Treatment Effect Using Instrumental Variables: Review of Methods for Binary Instruments, Treatments, and Outcomes\n    -   Note to future self: good rabbit hole to go down to calculate bounds on the ATE under different assumption sets\n-   [Greenland, Pearl, Robins 1999](https://projecteuclid.org/journals/statistical-science/volume-14/issue-1/Confounding-and-Collapsibility-in-Causal-Inference/10.1214/ss/1009211805.full) - Confounding and Collapsibility in Causal Inference\n-   [Robins 1998](https://pubmed.ncbi.nlm.nih.gov/9493255/) - Correction for non-compliance in equivalence trials ([free link](https://www.hsph.harvard.edu/wp-content/uploads/sites/343/2013/03/corr-noncomp-98.pdf))\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}