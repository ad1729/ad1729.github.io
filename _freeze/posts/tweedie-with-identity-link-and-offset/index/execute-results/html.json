{
  "hash": "042b0086ea1e5233c79e45df1cbf0039",
  "result": {
    "markdown": "---\ntitle: \"Specifying an offset in a Tweedie model with identity link\"\ndate: \"2023-07-30\"\ncategories: [R, Tweedie, Offset]\ncode-fold: false\nreference-location: margin\nimage: \"tweedie-image.png\"\n---\n\n\nThis post explores several things:\n\n-   differences between two ways of calculating the mean\n-   specifying the response variable and offset in a Tweedie regression model depending on the quantity of interest\n-   log-likelihood and the corresponding score function for a Tweedie model with weights\n-   why one way of specifying offsets in poisson GLMs with the identity link does not extend to Tweedie GLMs\n\nIn this article, the words duration, offset, exposure, and weight are used interchangeably.\n\n## Introduction\n\nData coming from fields like ecology, insurance, epidemiology -- such as number of new cases of a disease in multiple cities with very different population sizes, number of claims and total claim amounts from insurance policies with different durations, etc. -- need to have the varying exposures accounted for as offsets while building models. These might look something like the following simulated dataset with an exposure variable (w) and a response variable (y):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nn <- 1e6\n\n# tweedie distribution parameters\np <- 1.3 # variance power\nmu <- 200 # mean\nphi <- 350 # dispersion, so variance = phi * mu ^ p\n\nset.seed(45)\nsim_data <- tibble(\n  w = runif(n, min = 0.5, max = 3),\n  y = tweedie::rtweedie(n = n, mu = mu, phi = phi / w, power = p)\n)\n\n# summary(sim_data)\n\nsim_data %>% slice_head(n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n      w     y\n  <dbl> <dbl>\n1  2.08     0\n2  1.29     0\n3  1.10     0\n```\n:::\n:::\n\n\nFor readability, let's pretend that this response variable is the individual claim amounts for an insurance company with a client base of 1 million customers. Additionally, for each individual the duration of how long they've been insured is also recorded. So the first customer has been insured for about 2 years and 1 month, and has not filed a claim at all. The third customer has been insured for a little over a year and has also not filed any claims.\n\n## Ratio of means or mean of ratios?\n\nThe quantity of interest from such datasets may be the mean per unit exposure -- expected claim amount per individual per year, expected number of new cases of a disease per year per 100,000 people, etc. There are two separate quantities that can be calculated as expected values -- the ratio of the means of amount and duration variables, or the mean of the individual ratios. The differences between these two quantities is explained pretty well in [this stackexchange thread](https://stats.stackexchange.com/a/105715).\n\nThe [ratio estimator](https://en.wikipedia.org/wiki/Ratio_estimator) estimates the first quantity $R = \\sum_i{y_i} / \\sum_i{w_i}$. This answers the following question: given a population (or a group of individuals) who were followed-up / observed for a given amount of time, and generated a total amount of claims, how much of the total claim amount can be attributed per person per unit exposure?\n\nThe [weighted sample mean](https://en.wikipedia.org/wiki/Weighted_arithmetic_mean) of the individual ratios estimates the second quantity $\\mathbb{E} = \\sum_i{w_i y_i} / \\sum_i{w_i}$. This answers the question: given each individual's claim amount and exposure, what was the average individual's claim amount per unit exposure?\n\nIn the latter case, the link between claim amount and duration at the individual level is preserved, whereas for the former, the numerator and denominators are totals at the population level.\n\nIf everyone had exactly the same weight $w_i$ of 1 year, the denominator would sum to the sample size of 1 million for the data above, and both the estimates would be the same. If the weights were different, then the two results would differ.\n\nIn epidemiology, the ratio estimator is used for *incidence rate* calculations. On the other hand, the insurance papers on Tweedie models mentioned at the bottom of this post all seem to use the weighted mean approach.\n\nFor the simulated data above, the two estimates are quite different:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(sim_data$y) / sum(sim_data$w)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 114.5185\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nweighted.mean(sim_data$y, w = sim_data$w)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 200.4661\n```\n:::\n:::\n\n\nIn this case, the second estimate matches the true mean value chosen for the simulation, which makes sense given that the data are simulated from a Tweedie model with varying exposures ($Y_i \\sim \\text{Tweedie}(\\mu_i, \\phi / w_i, p)$) but with the mean-variance relationship preserved under unit exposure. This is described in part A of the supplement to the Yang et al 2016 paper.\n\nHowever, both the numbers are right, as they answer different questions.\n\n## Building a regression model\n\nThis formula is easy enough to apply if there are no predictors, but a regression model is needed to estimate these quantities when there are predictors in the data.\n\nThere are two ways of specifying the model. The first method corresponds to passing the claim amounts unmodified, and passing the exposures as weights:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# weighted mean estimator\nsummary(glm(y ~ 1, weights = w, data = sim_data,\n            family = statmod::tweedie(var.power = 1.3, link.power = 1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = y ~ 1, family = statmod::tweedie(var.power = 1.3, \n    link.power = 1), data = sim_data, weights = w)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 200.4661     0.4431   452.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 349.4572)\n\n    Null deviance: 240499666  on 999999  degrees of freedom\nResidual deviance: 240499666  on 999999  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n```\n:::\n:::\n\n\nThe estimated intercept and dispersion correspond to the same parameters ($\\mu = 200$, $\\phi = 350$) picked for the simulation. This estimate also coincides with the weighted mean estimate. `link.power = 1` indicates that the identity link function is used, and `var.power = 1.3` or $p = 1.3$ is assumed to be known here. Usually the profile likelihood approach is used to pick the best value on the interval $p \\in (1, 2)$ for data with exact zeroes.\n\nOnce $p$ is chosen, $\\mu$ can be estimated independently of $\\phi$, and finally dispersion $\\phi$ can be estimated using an optimization algorithm, or one of the estimators described in section 6.8 of the Dunn and Smyth book. The glm function in R uses the *Pearson estimator*:\n\n$$\n\\phi = \\frac{1}{N-p'} \\sum_{i=1}^{N} \\frac{w_i (y_i - \\hat\\mu_i) ^ 2}{\\hat\\mu_i^p}\n$$\n\nwhere $p'$ is the number of parameters in the model and $\\text{Var}(\\mu_i) = \\mu_i^p$ is the variance function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# pearson estimate of dispersion\n(sum((sim_data$w * ((sim_data$y - 200.4661)^2)) / (200.4661^1.3))) / (n - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 349.4573\n```\n:::\n:::\n\n\nThe second method rescales the response variable with the weights, and passes exposures as weights to the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ratio estimator\nsummary(glm(y / w ~ 1, weights = w, data = sim_data,\n            family = statmod::tweedie(var.power = 1.3, link.power = 1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = y/w ~ 1, family = statmod::tweedie(var.power = 1.3, \n    link.power = 1), data = sim_data, weights = w)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 114.5185     0.3655   313.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 492.3754)\n\n    Null deviance: 177557174  on 999999  degrees of freedom\nResidual deviance: 177557174  on 999999  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n```\n:::\n:::\n\n\nThis estimate corresponds to the same output as the ratio estimator from the previous section. The estimate of dispersion $\\phi$ is different from the true value of 350, which makes sense as dispersion is a function of the estimated value of the mean $\\mu$.\n\n## What does the math look like behind these two models?\n\nTo fit a Tweedie model to the data with weights to calculate the two different values in the previous section, the following log-likelihood function -- taken from the Yang et al paper -- can be maximized.\n\n$$\nl(\\mu, \\phi, p | \\{y, x, w\\}_{i=1}^n) = \\sum_{i = 1}^{n}{ \\frac{w_i}{\\phi}} \\Bigg(y_i \\frac{\\mu_i^{1-p}}{1-p} - \\frac{\\mu_i^{2-p}}{2-p} \\Bigg) + log\\ a(y_i, \\phi / w_i, p)\n$$\n\nFor this simulated dataset, $w_i$ and $y_i$ vary for each individual, but there are no predictors so an identity link function can be used [^1], the overall mean is of interest so the $\\mu_i$ term can be collapsed into a single $\\mu$ parameter to be optimized, and the `log a(...)` term can be ignored as it's not a function of $\\mu$. Since the regression equation for an intercept only term is $\\mu_i = \\beta_0$, we can replace the $\\mu$ with $\\beta_0$.\n\n[^1]: which should be avoided if the model will be used for out-of-sample prediction to avoid predicting values below 0\n\nThis makes it easy to compute the closed form solution, which is calculated by differentiating this function to produce the score function, and setting it to 0.\n\n$$\n\\frac{\\partial l}{\\partial \\beta} = \\sum_{i = 1}^{n}{ \\frac{w_i}{\\phi}} (y_i \\beta_0^{-p} - \\beta_0^{1-p}) = 0\n$$\n\n$\\phi$ and $\\beta_0^{-p}$ are non-zero constants, so can be pulled out of the summation and absorbed into the zero on the right-hand side to give\n\n$$\n\\sum_{i = 1}^{n}{ w_i (y_i - \\beta_0) } = 0\n$$\n\nFor `glm(y ~ 1, weights = w, ...)`, this equals\n\n$$\n\\sum_{i = 1}^{n} w_i y_i - \\beta_0 \\sum_{i = 1}^{n} w_i = 0 \\\\\n\\Rightarrow \\beta_0 = \\frac{\\sum_{i = 1}^{n} {w_i y_i}}{\\sum_{i = 1}^{n} w_i}\n$$\n\nand for `glm(y / w ~ 1, weights = w, ...)`, this equals\n\n$$\n\\sum_{i = 1}^{n} w_i \\frac{y_i}{w_i} - \\beta_0 \\sum_{i = 1}^{n} w_i = 0 \\\\\n\\Rightarrow \\beta_0 = \\frac{\\sum_{i = 1}^{n} {y_i}}{\\sum_{i = 1}^{n} w_i}\n$$\n\n## A method that works for Poisson but not for Tweedie\n\nWhat initially led me down this rabbit hole was finding [this stackexchange post](https://stats.stackexchange.com/questions/275893/offset-in-poisson-regression-when-using-an-identity-link-function) for specifying the offset with identity link in a poisson model, and naively (and incorrectly) fitting the Tweedie glm like this\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm(y ~ w - 1, data = sim_data,\n            family = statmod::tweedie(var.power = 1.3, link.power = 1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = y ~ w - 1, family = statmod::tweedie(var.power = 1.3, \n    link.power = 1), data = sim_data)\n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \nw 121.7328     0.4064   299.6   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 465.8885)\n\n    Null deviance:       NaN  on 1000000  degrees of freedom\nResidual deviance: 158130565  on  999999  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n```\n:::\n:::\n\n\nand trying to figure out why this didn't coincide with the ratio estimate of 114.5185, which is the case for the poisson models\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ratio estimator\nsum(sim_data$y) / sum(sim_data$w)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 114.5185\n```\n:::\n\n```{.r .cell-code}\n# generates warnings because we're passing a non-discrete response\n# point estimates are the same though\nunname(coef(suppressWarnings(glm(y ~ w - 1, family = poisson(link = \"identity\"), data = sim_data))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 114.5185\n```\n:::\n\n```{.r .cell-code}\nunname(coef(suppressWarnings(glm(y / w ~ 1, weights = w, family = poisson(link = \"identity\"), data = sim_data))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 114.5185\n```\n:::\n:::\n\n\nThis happens because substituting $\\mu_i = w_i \\beta_0$ instead of $\\mu_i = \\beta_0$ in the log-likelihood function\n\n$$\nl(\\beta_0, \\phi, p | \\{y, x, w\\}_{i=1}^n) = \\sum_{i = 1}^{n}{ \\frac{w_i}{\\phi}} \\Bigg(y_i \\frac{(w_i \\beta_0)^{1-p}}{1-p} - \\frac{(w_i\\beta_0)^{2-p}}{2-p} \\Bigg) + log\\ a(y_i, \\phi / w_i, p)\n$$\n\nleads to the following score function\n\n$$\n\\frac{\\partial l}{\\partial \\beta} = \\sum_{i = 1}^{n} \\frac{w_i}{\\phi} (y_i w_i^{1-p} \\beta_0^{-p} - w_i^{2-p} \\beta_0^{1-p}) = 0\n$$\n\nThe constant $\\phi$ and $\\beta_0^{-p}$ terms can be dropped from the equation, and the $w_i$ outside the bracket are all 1 because no weights are passed to the glm call, so the following equation is solved\n\n$$\n\\sum_{i = 1}^{n} y_i w_i^{1-p} - w_i^{2-p} \\beta_0 = 0\n$$\n\nwhich can be simplified by pulling $w_i^{1-p}$ as a common term\n\n$$\n\\sum_{i = 1}^{n} w_i^{1-p} (y_i  - w_i \\beta_0) = 0\n$$\n\nThis shows where the logical error happens, as well as how the correct estimate can be obtained, i.e., by passing `weights = w ^ (p - 1)` to the glm call, so that the $w_i^{1-p}$ term cancels out\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm(y ~ w - 1, weights = I(w^(1.3 - 1)), data = sim_data,\n            family = statmod::tweedie(var.power = 1.3, link.power = 1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = y ~ w - 1, family = statmod::tweedie(var.power = 1.3, \n    link.power = 1), data = sim_data, weights = I(w^(1.3 - 1)))\n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \nw 114.5185     0.3655   313.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 492.3754)\n\n    Null deviance:       NaN  on 1000000  degrees of freedom\nResidual deviance: 177557174  on  999999  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n```\n:::\n:::\n\n\nNow the point estimates, standard errors, and dispersion parameters correspond to the model where the ratio estimator is correctly specified.\n\nHere's the code and the plot of the score equations for the different models:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nparameter_grid <- seq(50, 300, 5)\n\nestimating_equation_curves <- tibble(\n  parameter = parameter_grid,\n   `Mean of ratios` = map_dbl(.x = parameter_grid, .f = ~ {\n    sim_data %>%\n      mutate(s = w * (y - .x)) %>%\n      pull(s) %>%\n      sum()\n  }),\n  `Ratio of means` = map_dbl(.x = parameter_grid, .f = ~ {\n    sim_data %>%\n      summarize(across(.cols = c(w, y), .fns = sum)) %>%\n      mutate(s = y - (.x * w)) %>%\n      pull(s)\n  }),\n  Incorrect = map_dbl(.x = parameter_grid, .f = ~ {\n    sim_data %>%\n      mutate(s = (w ^ (1 - 1.3)) * (y - w * .x)) %>%\n      pull(s) %>%\n      sum()\n  })\n)\n\nestimating_equation_curves %>%\n  pivot_longer(-parameter) %>%\n  ggplot(aes(x = parameter, y = value, linetype = name)) +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype = \"dotdash\") +\n  # geom_vline(xintercept = 114.5185, linetype = 2) +\n  # geom_vline(xintercept = 121.7328, linetype = 1) +\n  # geom_vline(xintercept = 200, linetype = 11) +\n  theme_bw() +\n  ylab(\"Score function\") +\n  xlab(\"Parameter\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## References\n\n-   Yang et al 2016 paper and supplement ([link](https://www.tandfonline.com/doi/full/10.1080/07350015.2016.1200981))\n-   Delong et al 2021 paper ([link](https://link.springer.com/article/10.1007/s13385-021-00264-3))\n-   Chapter 12 on Tweedie models from the Dunn and Smyth book on GLMs\n\nOthers\n\n-   Zhang 2013 ([link](https://link.springer.com/article/10.1007/s11222-012-9343-7))\n-   Stan code for Tweedie\n    -   <https://discourse.mc-stan.org/t/tweedie-likelihood-compound-poisson-gamma-in-stan/14636>\n    -   <https://gist.github.com/MatsuuraKentaro/952b3301686c10adcb13>\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}