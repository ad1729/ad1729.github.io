{
  "hash": "ff7bbfc67318d12cba88faed4700b8e0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simulating data from the cause-specific hazard approach to competing risks\"\ndate: \"2025-08-09\"\ncategories: [R, Simulation, Time-to-event]\nimage: \"cif_plot.png\"\ncode-fold: true\neditor: visual\n---\n\n\nIn a [previous post](../simulating-survival-data-with-non-PH-and-cure/index.qmd), I used the *cumulative hazard inversion method* to simulate *time-to-event (TTE)* data from a log-logistic accelerated-failure time (AFT) model with statistical cure. The estimand of interest was the *cumulative incidence function (CIF)*, which is the probability of experiencing the event of interest before time $t$ (i.e., $\\text{Pr}[T \\leq t]$, also known as the cumulative distribution function (CDF) in the absence of censoring).\n\nI came across two things while reading up on material for that post, that I'd mentally filed away for looking into in the future. The first was on simulating TTE data using numerical methods. The second was on estimating the CIF in the *competing risk (CR)* setting where it wasn't correct to invert the survivor function ($S(t) = \\text{Pr}[T > t] = 1 - \\text{Pr}[T \\le t]$) to get to the CIF.\n\nSo this post is on simulating data from the *cause-specific hazards (CSH)* approach to CR based on [this paper](https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3516) (Beyersmann et al. 2009); other key papers I use material from are listed in the references below. To check whether my implementation (and understanding) is correct, I run a mini simulation to see if I can recover the true parameters used for generating the data. I also compare the estimated cause-specific CIFs with the true CIFs.\n\nI initially tried to use the `survsim` package to simulate CSH CR data, but I found it quite slow. Figuring out what I was doing suboptimally was only clear to me after digging deeply into it and reimplementing the code myself by studying the [implementation](https://github.com/cran/survsim/blob/6e1bf878cf3e35b37d6cb7d1ef4b0179ed99b1d3/R/crisk.ncens.sim.R) of `survsim::crisk.ncens.sim` and the Beyersmann paper. My implementation has the advantage that it\n\n-   is faster\n\n-   uses closed-form expressions for the cumulative hazards rather than numerically integrating the hazards\n\n-   uses the same parameterizations as the `flexsurv` package I'm using to fit the models so I don't have to mess around with transforming between parameterizations\n\n-   can use any distribution that from `flexsurv` that is not in `survsim` (so those beyond Weibull, log-normal, log-logistic, and those that are subsumed within these (e.g. exponential within Weibull))\n\n-   managed to increase my knowledge of this topic\n\nI mean I'd most likely still use `survsim`, unless I need to simulate a lot of not very small datasets, or want the additional flexibility around simulation-model specification.\n\nI think `adjustedCurves::sim_confounded_crisk()` can simulate CR data from the CSH approach as well, but I haven't tried it out.\n\nI'm using the following packages\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(survival)\nlibrary(flexsurv)\n# functions called via namespace::fun() to avoid conflicts\n# library(glue)  # string interpolation\n# library(mirai)  # for setting daemons() for parallelization\n# library(survsim)\n# library(ggh4x)  # facet_grid2 function\n# library(bench)  # for benchmarking\n\ntheme_set(theme_bw())\n```\n:::\n\n\n## Data generating process\n\nIn the usual TTE setting, interest is on a single event. An example from marketing is that of customer churn, where a customer cancels their subscription. The competing risk approach to TTE analysis extends this to cases where more than one event can occur.\n\nSticking with the churn example, this could be (say) three possible event types (using the event indicator $D$ with values $k = 1, 2, 3 (= K)$) — churn due to customer dissatisfaction with some aspect of the product / service which is the main event of interest that a company would like to reduce ($D = 1$), churn due to financial reasons ($D = 2$), and churn due to unavoidable reasons ($D = 3$). $D = 0$ indicates not having churned (yet). I'm using an example just to make things a bit more concrete; these hazard functions are unlikely to be representative of any actual churn process.\n\nThis single vs multiple TTE analysis is analogous to the distinction between binary (binomial) vs categorical (multinomial) outcomes, with the former taking on values 0 / 1, and the latter taking on more than 2 values e.g., 0 / 1 / 2 / 3.\n\n### Distributional parameters\n\nThe three churn event types are assumed to have the hazard functions corresponding to these distributions\n\n$$\n\\begin{align*}\nT^1 | x_1 = 0 &\\sim \\text{Weibull}(\\text{shape} = 1.6, \\text{scale} = 5) \\\\\nT^2 | x_1 = 0 &\\sim \\text{Log-logistic}(\\text{shape} = 5, \\text{scale} = 5) \\\\\nT^3 | x_1 = 0 &\\sim \\text{Exponential}(\\text{rate} = 5)\n\\end{align*}\n$$\n\nfor the control arm ($x_1 = 0$) assuming some experiment is carried out to assess the effectiveness of some action intended to reduce churn. Treatment arm can be coded as $x_1 = 1$ with treatment assumed to reduce the rate of churn by a time acceleration factor of 1.4 or 40%. So we have\n\n$$\n\\begin{align*}\nT^1 | x_1 = 0 &\\sim \\text{Weibull}(\\text{shape} = 1.6, \\text{scale} = 5) \\\\\nT^1 | x_1 = 1 &\\sim \\text{Weibull}(\\text{shape} = 1.6, \\text{scale} = 7) \n\\end{align*}\n$$\n\nand $T^2 |x_1$ and $T^3 | x_1$ are the same for $x_1 = 0$ and $x_1 = 1$.\n\nChoosing $x_1$ to have a non-zero effect only on $T^1$ means I'm avoiding some of the messiness that comes with interpreting treatment effects on the CIFs.\n\nI'm also assuming a 50-50 (%) split of $x_1$ into the two groups.\n\nThese values for the shape / scale / rate parameters were chosen to have decent balance across the different event types and to not have some categories with very low event counts.\n\n### Latent failure time approach\n\nThe use of $T^{\\bullet}$ makes it seem like I'm invoking the *latent failure time approach to CR*, but I'm only using it as a convenient way of indicating the chosen CSH functions. As pretty much all papers on CR warn, the latent variable approach — which retains $T_i = min(T^1_i, \\dots, T^K_i)$ for each individual $i$ — assumes unverifiable dependence structures between the latent event time variable $T^k$ which are generally not identifiable from observed data.\n\nThis is sad because the latent variable approach seems a lot more intuitive and is very simple to generate data from when the latent event times are assumed to be independent\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nn <- 7\nset.seed(25)\ntibble(\n  # generate group membership\n  x1 = rbinom(n, 1, prob = 0.5),\n  # generate counterfactual for x1 = 0\n  t1_0 = rweibull(n, shape = 1.6, scale = 5),\n  # and counterfactual for x1 = 1 (using scale = 7)\n  t1_1 = rweibull(n, shape = 1.6, scale = exp(log(5) + log(1.4))),\n  # use the consistency equation to realize the latent t1\n  t1 =  (x1 * t1_1) + ((1 - x1) * t1_0),\n  t2 = rllogis(n, shape = 5, scale = 5),\n  # exponential distribution, same as rexp(n, 1 / 5)\n  t3 = rweibull(n, shape = 1, scale = 5),\n  # administrative censoring\n  cens_time = 4,\n  # take the minimum of the latent event and censoring times\n  time = pmin(t1, t2, t3, cens_time), \n  # record which event / censor time is the smallest\n  event = case_when(\n    cens_time < t1 & cens_time < t2 & cens_time < t3 ~ 0,\n    t1 < t2 & t2 < t3 ~ 1,\n    t2 < t1 & t2 < t3 ~ 2,\n    t3 < t1 & t3 < t2 ~ 3\n  )\n) %>% \n  mutate(across(.cols = everything(), .fns = ~ round(.x, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 9\n     x1  t1_0  t1_1    t1    t2    t3 cens_time  time event\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>     <dbl> <dbl> <dbl>\n1     0  5.26  3.72  5.26  4.81  3.15         4  3.15     3\n2     1  9.32 10.5  10.5   3.17  7.18         4  3.17     2\n3     0  5.79  5.18  5.79  3.56  0.97         4  0.97     3\n4     1  5.35  3.42  3.42  4.19  5.26         4  3.42     1\n5     0  5.04  5.8   5.04  4.22  7.2          4  4        0\n6     1  0.69  3.4   3.4   3    11.4          4  3        2\n7     1  3.36 14.5  14.5   5.39  0.92         4  0.92     3\n```\n\n\n:::\n:::\n\n\nI'm quite sure that if I used this for my simulation I'd still end up with unbiased parameter estimates, but the assumption of independence may not be very realistic depending on the application.\n\nA future rabbit hole to go down would be to look into the literature on using copulas for modelling dependence between the latent event times.\n\nThe event time for each individual will be referred to using a single random variable $T$ since no latent variables are assumed.\n\n### Cause-specific hazards (CSH)\n\nWe can plot the CSH functions defined as\n\n$$\nh_k(t | x_1) = \\lim_{\\Delta t \\rightarrow 0} \\frac{\\text{Pr}[t \\le T < t + \\Delta t, D = k | T \\ge t, x_1]}{\\Delta t}\n$$\n\nfor the $k^\\text{th}$ event showing the instantaneous rate of occurrence of that event\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntime_seq <- seq(0.01, 50, 0.01)\nhazards <- bind_rows(\n  tibble(\n    Distribution = \"Event 1: Weibull(shape = 1.6, scale = 5)\",\n    time = time_seq,\n    Hazard = hweibull(time_seq, shape = 1.6, scale = 5)\n  ),\n  tibble(\n    Distribution = \"Event 1: Weibull(shape = 1.6, scale = 7)\",\n    time = time_seq,\n    Hazard = hweibull(time_seq, shape = 1.6, scale = 7)\n  ),\n  tibble(\n    Distribution = \"Event 2: Log-logistic(shape = 5, scale = 5)\",\n    time = time_seq,\n    Hazard = hllogis(time_seq, shape = 5, scale = 5)\n  ),\n  tibble(\n    Distribution = \"Event 3: Exponential(rate = 5)\",\n    time = time_seq,\n    Hazard = hweibull(time_seq, shape = 1, scale = 5)\n  )\n)\n\nhazards %>%\n  ggplot(aes(x = time, y = Hazard, group = Distribution, \n             linetype = Distribution)) +\n  geom_line() +\n  xlab(\"Time (t)\") +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  guides(linetype = guide_legend(nrow = 2))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nI'm using the hazard functions implemented within `flexsurv` (e.g., `flexsurv::hweibull()`). The exponential hazard is constant over time. The log-logistic hazard first rises and then falls with time. The Weibull hazards for the chosen parameters are monotonically increasing functions of time. The hazard for each event is independent of the hazard for all other events.\n\n### Relative contribution to the overall hazard\n\nAs mentioned in the Hinchliffe and Lambert (2013) paper, it can be useful to look at the relative contribution of the hazard of each event to the overall hazard within each group at each $t$\n\n$$\n\\text{Pr}[D = k | t \\le T < t + \\Delta t, T \\ge t, x_1] = \\frac{h_k(t | x_1)}{\\sum_{k = 1}^K h_k(t | x_1)}\n$$\n\nThis can be interpreted as the probability of an event of type $k$ among those who experience an event **at** time $t$ and haven't experienced any of the events before $t$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# prob(cause | event = 1)\ncause_specific_hazards_by_group <- bind_rows(\n  hazards %>%\n    # for x = 0, drop the cause 1 for x = 1\n    filter(Distribution != \"Event 1: Weibull(shape = 1.6, scale = 7)\") %>%\n    mutate(Group = \"x1 = 0\"),\n  hazards %>%\n    # for x = 1, drop the cause 1 for x = 0\n    filter(Distribution != \"Event 1: Weibull(shape = 1.6, scale = 5)\") %>%\n    mutate(Group = \"x1 = 1\")\n)\n\nrelative_contribution_to_overall_hazard <-cause_specific_hazards_by_group %>%\n  arrange(Group, time, Distribution) %>%\n  group_by(Group, time) %>%\n  mutate(overall_hazard = sum(Hazard), p = Hazard / overall_hazard) %>%\n  ungroup()\n\nrelative_contribution_to_overall_hazard %>%\n  ggplot(aes(x = time, y = p, \n             group = interaction(Group, Distribution), \n             linetype = Distribution)) +\n  geom_line() +\n  xlab(\"Time (t)\") +\n  ylab(\"Pr [event type k | any event]\") +\n  facet_wrap(~ Group) +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  guides(linetype = guide_legend(nrow = 2))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nFor individuals that churn very soon after $t = 0$, they're more likely to have had $D = 3$, followed by $D = 1$, followed by $D = 2$. Events that occur around $t = 5$ are most likely to be $D = 2$ as shown by the long-dash line peaking after that time.\n\n### Cause-specific cumulative incidence function (CIF)\n\nA key quantity is the cumulative incidence function (CIF) for each of the $K = 3$ events, which is the probability that an individual will experience event $k$ before time $t$ in the absence of the occurrence of any of the other events\n\n$$\n\\text{CIF}_k(t|x_1) = \\text{Pr}[T \\le t, D = k | x_1] = \\int_{u=0}^{u=t} h_k(u | x_1) S(u | x_1) du\n$$\n\n$h_k(t|x_1)$ is the CSH function, and $S(t|x_1)$ is the overall survivor function in each group. The latter is defined as the probability of being free of any event up to time $t$\n\n$$\nS(t | x_1) = \\text{Pr}[T > t | x_1] =  \\text{exp}\\Bigg(- \\sum_{k = 1}^K H_k(t | x_1)\\Bigg)\n$$\n\n$H_k(t | x_1)$ is the cumulative hazard function for the $k^{\\text{th}}$ event in each group\n\n$$\nH_k(t | x_1) = \\int_{u = 0}^{u = t} h_k(u | x_1) du \n$$\n\nThe cumulative hazard functions are used as implemented in `flexsurv` (e.g., `flexsurv::Hweibull()`). In discrete-time, the integral for the CIF is replaced by summation, and $S(u|\\cdot)$ in the integrand / summand is replaced with $S(-u | \\cdot)$, which is the survival probability at a time point right before $u$. The CIFs and $S(t)$ are probabilities of being in the different states by time $t$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate the overall survival from the overall hazard\noverall_survival <- time_seq %>% \n  expand_grid(\n    x1 = c(0, 1), time = .\n  ) %>%\n  mutate(\n    surv = exp(-(\n      Hweibull(x = time, shape = 1.6, scale = exp(log(5) + log(1.4) * x1), \n               log = FALSE) +\n      Hllogis(x = time, shape = 5, scale = 5, log = FALSE) +\n      Hweibull(x = time, shape = 1, scale = 5, log = FALSE)\n    )),\n    Group = paste0(\"x1 = \", x1)\n  ) %>%\n  select(-x1)\n\ntime_step <- time_seq[2] - time_seq[1]\n\n# calculate CIFs\ncumulative_incidence_curves <- cause_specific_hazards_by_group %>%\n  inner_join(y = overall_survival, by = c(\"Group\", \"time\")) %>%\n  group_by(Group, Distribution) %>%\n  mutate(CIF = order_by(time, cumsum(Hazard * surv * time_step))) %>%\n  ungroup()\n\n# plot the CIFs\n# P[T <= t, cause = k | Event]\ncumulative_incidence_curves_plot_data <- cumulative_incidence_curves %>%\n  select(-Hazard) %>%\n  pivot_wider(id_cols = c(time, Group, surv), \n              names_from = Distribution, values_from = CIF) %>%\n  rename(`Event-free` = surv) %>%\n  pivot_longer(cols = -c(time, Group), \n               names_to = \"Event\", values_to = \"CIF\") %>%\n  filter(complete.cases(.))\n\ncumulative_incidence_curves_plot <- cumulative_incidence_curves_plot_data %>%\n  ggplot(aes(x = time, y = CIF, linetype = Event)) +\n  geom_line() +\n  xlab(\"Time (t)\") +\n  ylab(\"Probability of being in state\") +\n  facet_wrap(~ Group) +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  guides(linetype = guide_legend(nrow = 3))\n\ncumulative_incidence_curves_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThe CIFs are called *subdistribution* functions because they never reach 1 (at $t = \\infty$) due to the presence of competing events so are not proper probability distributions. In the single event case, the CIF eventually reaches 1 given enough time.\n\nWithin each panel, the curves sum to 1 at every time point, since each individual is in one state at each time point\n\n$$\n\\sum_{k = 1}^K\\text{CIF}_k(t | x_1) + S(t | x_1) = 1 = \\text{Pr}[T \\le t, D = 1 | x_1] + \\text{Pr}[T \\le t, D = 2 | x_1] + \\text{Pr}[T \\le t, D = 3 | x_1] + \\text{Pr}[T > t | x_1]\n$$\n\nAll individuals start with being event free at $t = 0$, but as as time progresses, they move into any one of the $k$ states, so the probability of being event-free, i.e., staying in state $D = 0$, decreases with time and goes to 0 eventually (here around $t = 7$).\n\nOne difference between this plot and the unnormalized hazard plot is that here the hazard of each event contributes to the CIF of all other events via it's dependence on the overall survival. So $\\text{CIF}_2(t | x1 = 0) \\neq \\text{CIF}_2(t | x1 = 1)$ and $\\text{CIF}_3(t | x1 = 0) \\neq \\text{CIF}_3(t | x1 = 1)$ because $H_1(t | x_1 = 0) \\ne H_1(t | x_1 = 1)$, even though $H_2(t | x_1 = 0) = H_2(t | x_1 = 1)$ and $H_3(t | x_1 = 0) = H_3(t | x_1 = 1)$.\n\n### Relative contribution to overall churn\n\nSimilar to the relative contribution of the CSHs to overall hazard I came across in the Hinchliffe and Lambert paper, they also mention the rescaled cause-specific CIFs (disregarding $S(t | x_1)$)\n\n$$\n\\text{Pr}[D = k | T \\le t, x_1] = \\frac{\\text{CIF}_k(t | x_1)}{\\sum_{k = 1}^K \\text{CIF}_k(t | x_1)}\n$$\n\nwhich is the probability of having had event $k$ among those who have churned **by** time $t$ within levels of $x_1$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the rescaled CIFs conditional on event\n# so ignores the no even state\n# P[T <= t, cause = k | Event]\ncumulative_incidence_curves %>%\n  mutate(rescaled_CIF = CIF / sum(CIF), .by = c(Group, time)) %>%\n  # arrange(Group, time) %>%\n  # print()\n  ggplot(aes(x = time, y = rescaled_CIF, linetype = Distribution)) +\n  geom_line() +\n  xlab(\"Time (t)\") +\n  ylab(\"Pr [event type k by t | any event]\") +\n  facet_wrap(~ Group) +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  guides(linetype = guide_legend(nrow = 2))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## Implement the simulation steps\n\nThe key idea behind simulating CR data from the CSH approach is summarized in the following steps (here within each level of covariate $x_1$):\n\n-   specify the cause-specific hazards $h_k(t | x_1)$\n\n-   simulate survival time $T_i$ for the $i^\\text{th}$ individual with the all-cause cumulative hazard $\\sum_{k=1}^K H_k(t | x_1)$\n\n-   for a simulated $T_i$, simulate the event type $D_i$ from a multinomial distribution with $k$-dimensional probability vector from the scaled hazards $h_k(t | x_1) / \\sum_{k = 1}^K h_k(t | x_1)$. This gives the pair $(T_i, D_i)$ for the $i^\\text{th}$ individual\n\n-   simulate censoring time $C_i$ and set $T_i  = C_i$ if $C_i < T_i$ (and set $D_i = 0$).\n\nThe event time $T_i$ is generated by inverting the survivor function. The CDF $F(t)$ has a $\\text{Uniform}[0, 1]$ distribution where $F(t_i) = u_i \\sim U[0, 1]$. We can use $F(t) = 1 - S(t)$ to equate $S(t) = 1 - u$ which gives\n\n$$\n1 - u = \\text{exp}\\Bigg[- \\sum_{k = 1}^K H_k(t) \\Bigg]\n$$\n\nTaking logs on both sides and rearranging gives the objective function\n\n$$\n\\text{log}(1 - u) + \\sum_{k = 1}^K H_k(t) = 0\n$$\n\nFor a randomly drawn $u_i \\sim U[0,1]$ (and given $x_{i, 1}$), numerical root finding methods can be employed to estimate the value of $t_i$ which satisfies this equation. This process of using $u_i$ to estimate $t_i$ is known as [inverse transform sampling](https://en.wikipedia.org/wiki/Inverse_transform_sampling).\n\n### Overall hazard\n\nThis function implements the sum of the cumulative hazards $H_k(t | x_1)$ which is 0 at $t = 0$ but increases without bound beyond that\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nall_cause_cumulative_hazards <- function(t, x1) {\n  sum(\n    Hweibull(x = t, shape = 1.6, scale = exp(log(5) + log(1.4) * x1), log = FALSE),\n    Hllogis(x = t, shape = 5, scale = 5, log = FALSE),\n    Hweibull(x = t, shape = 1, scale = 5, log = FALSE)\n  )\n}\n\nall_cause_cumulative_hazards(t = 0, x1 = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nall_cause_cumulative_hazards(t = 10, x1 = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8.527941\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nall_cause_cumulative_hazards(t = 10, x1 = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.265977\n```\n\n\n:::\n:::\n\n\n### Event type probabilities\n\nThis next one calculates the event probabilities (conditional on an event at $t$) $\\text{Pr}[D = k | t \\le T < t + \\Delta t, T \\ge t, x_1]$ given $t$ and $x_1$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ncause_specific_probabilities <- function(t, x1) {\n  hazards <- c(\n    hweibull(x = t, shape = 1.6, \n             scale = exp(log(5) + log(1.4) * x1), log = FALSE),\n    hllogis(x = t, shape = 5, scale = 5, log = FALSE),\n    hweibull(x = t, shape = 1, scale = 5, log = FALSE)\n  )\n\n  hazards / sum(hazards)\n}\n\ncause_specific_probabilities(t = 0, x1 = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0 0 1\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\ncause_specific_probabilities(t = 10, x1 = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4145983 0.4144437 0.1709580\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\ncause_specific_probabilities(t = 10, x1 = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2924853 0.5008953 0.2066193\n```\n\n\n:::\n:::\n\n\nThe function returns a $K$-dimensional vector which sums to 1.\n\n### Objective function\n\nImplement the objective function\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nobj_fun <- function(t, u, x1) {\n  log(1 - u) + all_cause_cumulative_hazards(t, x1)\n}\n\nobj_fun(t = 3.771, u = 0.8, x1 = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.0001226652\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\ntibble(\n  time = time_seq[time_seq < 10.0], \n  obj_val = map_dbl(\n    .x = time, \n    .f = ~ obj_fun(.x, u = 0.8, x1 = 0)\n    )\n  ) %>% \n  ggplot(aes(x = time, y = obj_val)) + \n  geom_line() + \n  geom_hline(yintercept = 0, linetype = \"dotted\") + \n  geom_vline(xintercept = 3.771, linetype = \"dashed\") + \n  xlab(\"Time (t)\") + \n  ylab(\"Objective function\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nThis plot corresponding to a grid search on $t \\in (0, 10)$ shows that for $u_i = 0.8$, $t_i \\approx 3.771$ (dashed vertical line) is very close to the root since the objective function evaluates to approximately -0.00012.\n\nDoing grid search is kinda tedious, but we can use `stats::uniroot` to find the root $t_i$ for a given $u_i$ (and $x_{i, 1}$) and return the pair $(T_i, D_i)$.\n\n### Simulate single outcome\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nsimulate_single_outcome <- function(u, x1, admin_censor_time = 5, lower_bound_time = 1e-10) {\n\n  # check whether simulated time would lie outside the time interval we want to solve for\n  # in this case the function would have the same sign at the interval endpoints\n  # if the function has different signs at the endpoints then a root is within the interval\n  same_sign <- (obj_fun(u = u, t = 0.00001, x1 = x1) * obj_fun(u = u, t = admin_censor_time, x1 = x1)) > 0\n\n  if (same_sign) {\n    time <- admin_censor_time\n    cause <- 0\n    iter <- NA_real_\n    fn_value_at_root <- NA_real_\n  } else {\n    # for a handful of cases (out of 100,000s) we can end up with t = 0 from a distribution that doesn't support it\n    # https://stats.stackexchange.com/questions/176376/invalid-survival-times-for-this-distribution\n    # so we can set a lower bound on time > 0\n    uniroot_obj <- uniroot(\n      f = obj_fun, \n      interval = c(lower_bound_time, admin_censor_time), \n      # pass arguments to the objective function\n      u = u, x1 = x1\n    )\n    time <- uniroot_obj$root\n    cause <- which.max(\n      rmultinom(1, 1, cause_specific_probabilities(t = time, x1 = x1))\n    )\n    iter <- uniroot_obj$iter\n    fn_value_at_root <- uniroot_obj$f.root\n  }\n\n  tibble(u, x1, admin_censor_time, iter, fn_value_at_root, time, cause)\n}\n\nbind_rows(\n  # non-censored\n  simulate_single_outcome(u = 0.2, x1 = 1, admin_censor_time = 5),\n  # censored\n  simulate_single_outcome(u = 0.95, x1 = 1, admin_censor_time = 5),\n  # same u but larger admin censored time means not censored\n  simulate_single_outcome(u = 0.95, x1 = 1, admin_censor_time = 12)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 7\n      u    x1 admin_censor_time  iter fn_value_at_root  time cause\n  <dbl> <dbl>             <dbl> <dbl>            <dbl> <dbl> <dbl>\n1  0.2      1                 5     6     -0.00000118  0.920     3\n2  0.95     1                 5    NA     NA           5         0\n3  0.95     1                12     7     -0.000000103 5.76      2\n```\n\n\n:::\n:::\n\n\nA lower bound of $t$ close to 0 is applied to avoid ending up with estimates of exactly 0, which can cause problems with estimation for some parametric distributions. If the drawn $u_i$ is large enough that the objective function has the same sign at both endpoints of the interval, then we skip the root finding as that observation would be treated as censored anyway.\n\nAdministrative censoring at $t = 5$ is assumed, which should lead to about 7% censored individuals in $x_1 = 0$ and 10% in $x_1 = 1$ (and about 8-9% overall) as seen in this zoomed-in version of the CIF plot (from a few sections above)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncumulative_incidence_curves_plot + \n  geom_vline(xintercept = 5.0, color = \"forestgreen\") +\n  geom_hline(\n    data = tibble(Group = c(\"x1 = 0\", \"x1 = 1\"), y = c(0.068, 0.102)),\n    aes(yintercept = y), color = \"purple\"\n  ) +\n  coord_cartesian(xlim = c(4.5, 6), ylim = c(0.05, 0.12))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n### Simulate single dataset\n\nFinally, the next function stitches together the steps for simulating a full dataset with $n = 1,000$ individuals and their group indicator $x_{i, 1} \\in \\{0, 1\\}$, event time $T_i$, and event indicator $D_i \\in \\{0, 1, 2, 3\\}$.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nsimulate_single_dataset <- function(n_rows = 1000, prop_x1 = 0.5, seed = 12345) {\n  if(!is.null(seed)) set.seed(seed)\n  x1 <- rbinom(n = n_rows, size = 1, prob = prop_x1)\n  u <- runif(n_rows)\n  1:n_rows %>% \n    map(.f = ~ simulate_single_outcome(u = u[.x], x1 = x1[.x])) %>%\n    list_rbind()\n}\n\nsimulate_single_dataset(n_rows = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 7\n      u    x1 admin_censor_time  iter fn_value_at_root  time cause\n  <dbl> <int>             <dbl> <dbl>            <dbl> <dbl> <dbl>\n1 0.166     1                 5     6     -0.00000122  0.765     3\n2 0.325     1                 5     5     -0.000000279 1.52      1\n3 0.509     1                 5     6     -0.00000489  2.47      3\n4 0.728     1                 5     6     -0.000000505 3.70      2\n5 0.990     0                 5    NA     NA           5         0\n```\n\n\n:::\n:::\n\n\n## Test the implementation\n\n### Parameter estimates\n\nTo check that the simulation function works, a bunch of functions are defined in the next code chunk that take a single simulated dataset, convert it into cause-specific datasets (for each event type $k$, update the event indicator by censoring competing events), fit a (correctly specified) parametric model separately to each cause-specific dataset, extract the coefficients from each model, and return them as a data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ncreate_cause_specific_datasets <- function(data) {\n  map(\n    .x = 1:3,\n    .f = ~ {\n      data %>%\n        select(x1, time, cause) %>%\n        mutate(event = as.numeric(cause == .x))\n    }\n  )\n}\n\nfit_cause_specific_models <- function(cause_specific_datasets, distributions = c(\"weibull\", \"llogis\", \"weibull\")) {\n  map2(\n    .x = cause_specific_datasets,\n    .y = distributions,\n    .f = ~ flexsurvreg(Surv(time = time, event = event) ~ x1, data = .x, dist = .y)\n  )\n}\n\nextract_coefs <- function(list_models) {\n  map2_dfr(.x = list_models, .y = 1:length(list_models), .f = ~ {\n    .x %>%\n      # calls flexsurv:::tidy.flexsurvreg()\n      flexsurv::tidy() %>%\n      select(term, estimate, se = std.error) %>%\n      mutate(cause = .y, .before = term)\n  })\n}\n\nrun_pipeline <- function(seed) {\n  seed %>%\n    simulate_single_dataset(seed = .) %>%\n    create_cause_specific_datasets() %>%\n    fit_cause_specific_models() %>%\n    extract_coefs() %>%\n    mutate(rep = seed, .before = cause)\n}\n\n# wrap it up into purrr::safely() since it fails \n# for some seeds (e.g. 125)\nsafely_run_pipeline <- safely(run_pipeline)\n\nsafely_run_pipeline(seed = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$result\n# A tibble: 9 × 5\n    rep cause term  estimate     se\n  <dbl> <int> <chr>    <dbl>  <dbl>\n1     3     1 shape   1.66   0.0801\n2     3     1 scale   4.93   0.246 \n3     3     1 x1      0.297  0.0724\n4     3     2 shape   5.50   0.359 \n5     3     2 scale   4.88   0.153 \n6     3     2 x1      0.0281 0.0391\n7     3     3 shape   1.07   0.0418\n8     3     3 scale   4.99   0.333 \n9     3     3 x1     -0.0972 0.0851\n\n$error\nNULL\n```\n\n\n:::\n:::\n\n\nThe `safely_run_pipeline()` function returns the point estimates and standard errors for the shape, scale, and coefficient for $x_1$ from each cause-specific model.\n\nThis process is repeated 5,000 times. A single run takes about 1.6 seconds on my machine, so to save time, the runs are parallelized. Using the recently added `purrr::in_parallel` function cuts down the total execution time from a little over 2 hours to about 20 minutes (it still took 3x longer than I expected, so I'd like to look into that later — overhead is expected from parallelization but not 3x. Maybe it would have been better to use fewer cores.).\n\nUsing the version of `purrr` at the time of writing, `in_parallel` requires that all these functions defined above are nested into one big function which can then be shipped off to the executors. All the package functions should be explicitly namespaced via `package::fun()` as well. The full script for running this pipeline is [here](https://github.com/ad1729/ad1729.github.io/tree/master/posts/simulating-and-modelling-competing-risk-data/nested-simulation-functions-for-parallel-run.R).\n\nSaved results can be read back in. About 44 or 0.9% of the simulations (with the following seeds) failed at some step.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_reps <- 5000\nfailed_parallel_runs <- read_rds(\"saved_objects/failed_parallel_runs.rds\")\nfailed_parallel_runs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]    5   39  124  125  315  319  445  510  662  672  765 1058 1105 1238 1494\n[16] 1673 1832 1844 2152 2176 2277 2399 2521 2526 2888 2944 2995 3065 3574 3657\n[31] 3754 3818 4222 4234 4246 4259 4326 4414 4465 4481 4482 4742 4873 4978\n```\n\n\n:::\n\n```{.r .cell-code}\nlength(failed_parallel_runs) / n_reps\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0088\n```\n\n\n:::\n:::\n\n\nFor the seeds that ran without error, the estimates can be plotted with the true parameter values overlaid for comparison. No transformation is needed when using `flexsurv:::tidy.flexsurvreg()` (except for the $x_1$ coefficient which is reported on the log scale)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrue_params <- tribble(\n  ~ cause, ~ term, ~ estimate,\n  1L, \"shape\", 1.6,\n  1L, \"scale\", 5,\n  1L, \"x1\", log(1.4),\n  2L, \"shape\", 5,\n  2L, \"scale\", 5,\n  2L, \"x1\", 0,\n  3L, \"shape\", 1,\n  3L, \"scale\", 5,\n  3L, \"x1\", 0\n)\n\nparameter_estimates_parallel <- read_rds(\"saved_objects/parameter_estimates_parallel.rds\")\n\nparameter_estimates_parallel %>% \n  ggplot(aes(x = estimate, group = interaction(cause, term))) +\n  geom_density() +\n  geom_vline(data = true_params, aes(xintercept = estimate)) +\n  xlab(glue::glue(\"Sampling distribution of simulation parameters \", \n                  \"(5,000 datasets with n = 1,000)\")) +\n  ggh4x::facet_grid2(rows = vars(cause), cols = vars(term), \n                     scales = \"free\", independent = TRUE) +\n  theme_classic() + \n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nIt's reassuring but not surprising, given the ubiquity of this approach to CSH CR analysis, to see the density estimates centered at the true values indicating unbiasedness of the CSH approach.\n\n### CIF estimates\n\nSince the parameters are estimated correctly, any downstream quantities like the cause-specific CIFs and overall survival should be unbiased too. In the next figure, I'm visualizing these for the four states within $x_1 = 0$. The true curves and the average of 100 curves from randomly simulated datasets (restricted to $t \\in (0, 10]$) are pretty close\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsampling_distribution_CIFs <- read_rds(\"saved_objects/sampling_distribution_CIFs.rds\")\n\nsampling_distribution_CIFs_mean <- sampling_distribution_CIFs %>%\n  summarise(CIF = mean(CIF), .by = c(Event, time))\n\nsampling_distribution_CIFs %>%\n  ggplot(aes(x = time, y = CIF, group = rep)) +\n  geom_line(color = \"gray80\") +\n  geom_line(\n    data = cumulative_incidence_curves_plot_data %>%\n    filter(Group == \"x1 = 0\", time <= 10.0) %>% \n    mutate(Event = fct_relevel(factor(Event), \"Event-free\", after = 0)),\n    aes(x = time, y = CIF),\n    color = \"gray20\", linewidth = 1.5,\n    inherit.aes = FALSE\n  ) +\n  geom_line(\n    data = sampling_distribution_CIFs_mean,\n    aes(x = time, y = CIF),\n    color = \"gray20\", linewidth = 1.5, linetype = \"dotted\",\n    inherit.aes = FALSE\n  ) +\n  geom_vline(xintercept = 5, linetype = \"dashed\", color = \"gray20\") +\n  xlab(glue::glue(\"Time (t)\\nSampling distribution of curves \", \n                  \"estimated from 100 datasets with \", \n                  \"n = 1,000 (light gray); \\ntrue curve \", \n                  \"(gray, solid); averaged curve (gray, dotted);\",\n                  \"\\nmaximum observed time (gray, dashed)\")) +\n  ylab(\"Probability of being in state\") +\n  facet_wrap(~ Event, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nIntuitively, the variability (or lack of) boils down to the number of events of each type $k$. Events that are more likely to occur first (in this case $k=3$) will have more events and more precise estimates. The estimate for $S(t|x_1 = 0)$ is the most precise here (all the gray lines are very close to the black line) because it uses information on all the events.\n\nEach individual curve is produced using this function\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nestimate_CIF <- function(seed, time_seq, covariate_level = data.frame(x1 = 0)) {\n  list_models <- seed %>%\n    simulate_single_dataset(seed = .) %>%\n    create_cause_specific_datasets() %>%\n    fit_cause_specific_models()\n\n  transition_matrix <- matrix(data = c(c(NA, 1, 2, 3), rep(NA, 12)), \n                              nrow = 4, ncol = 4, byrow = TRUE)\n\n  multi_state_object <- fmsm(list_models[[1]], \n                             list_models[[2]], \n                             list_models[[3]], \n                             trans = transition_matrix)\n\n  multi_state_object %>% \n    pmatrix.fs(\n      trans = transition_matrix, t = time_seq, \n      newdata = covariate_level, tidy = TRUE\n    ) %>%\n    as_tibble() %>%\n    filter(start == 1) %>%\n    select(-start) %>%\n    pivot_longer(cols = -time, names_to = \"Distribution\", values_to = \"CIF\")\n}\n\nsafely_estimate_CIF <- safely(estimate_CIF)\n```\n:::\n\n\nand the next code runs the pipeline for producing these 100 CIF estimates\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntime_seq_subset <- time_seq %>% \n  keep(.p = ~ .x <= 10.0)\n\nsampling_distribution_CIFs <- 1:100 %>%\n  map(\n    .f = ~ {\n      print(glue::glue(\"rep: {.x}\"))\n      safely_estimate_CIF(\n        seed = .x, \n        time_seq = time_seq_subset, \n        covariate_level = data.frame(x1 = 0)\n      )\n    }\n  ) %>%\n  # remove the seeds where the estimation failed\n  imap(\n    .f = ~ {\n      res <- pluck(.x, \"result\")\n      if (!is.null(res)) {\n        res %>%\n          mutate(rep = .y, .before = time)\n      } else {\n        res\n      }\n    }\n  ) %>%\n  compact() %>%\n  list_rbind() %>%\n  mutate(\n    Event = case_when(\n      Distribution == \"V1\" ~ \"Event-free\",\n      Distribution == \"V2\" ~ \"Weibull(shape = 1.6, scale = 5)\",\n      Distribution == \"V3\" ~ \"Log-logistic(shape = 5, scale = 5)\",\n      Distribution == \"V4\" ~ \"Exponential(rate = 5)\"\n    )\n  )\n\n# write_rds(sampling_distribution_CIFs, \"saved_objects/sampling_distribution_CIFs.rds\")\n```\n:::\n\n\n## Comparison with `survsim::crisk.ncens.sim`\n\nI initially started on this project by attempting to use the function from `survsim`, but I was getting very different results from fitting the models using `flexsurv`. It was also taking quite long (\\~6x longer) to produce simulated datasets mostly because of me picking simulation parameters without giving it much thought[^1]. Thinking through the process in detail is what led to this post being much longer than anticipated. This section compares the parameterizations for the hazard functions and simulated draws from `survsim` and `flexsurv`.\n\n[^1]: I picked a large value for administrative censoring, which meant that `uniroot` would take much longer to run due to increased number of iterations to converge to the root, and increased time due to multiple `integrate()` calls.\n\n### Converting `flexsurv` parameterization to `survsim`\n\nThe hazard function code for Weibull and log-logistic is taken from `survsim:::crisk.ncens.sim()` [here](https://github.com/cran/survsim/blob/6e1bf878cf3e35b37d6cb7d1ef4b0179ed99b1d3/R/crisk.ncens.sim.R#L85-L104)\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# code from survsim::crisk.ncens.sim()\n#\nif (dist.ev[k] == \"llogistic\") {\n  a.ev[k] <- 1/exp(beta0.ev[k] + suma[k])\n  b.ev[k] <- anc.ev[k]\n  cshaz[[k]] <- function(t, r) {\n    par1 <- eval(parse(text=\"a.ev[r]\"))\n    par2 <- eval(parse(text=\"b.ev[r]\"))\n    z    <- eval(parse(text=\"az1[r]\"))\n    return(z*(par1*par2*(t^(par2-1)))/(1+par1*(t^par2)))}\n}\n\nif (dist.ev[k] == \"weibull\") {\n  a.ev[k] <- beta0.ev[k] + suma[k]\n  b.ev[k] <- anc.ev[k]\n  cshaz[[k]] <- function(t, r) {\n    par1 <- eval(parse(text=\"a.ev[r]\"))\n    par2 <- eval(parse(text=\"b.ev[r]\"))\n    z    <- eval(parse(text=\"az1[r]\"))\n    return(z * ((1/par2)/((exp(par1))^(1/par2)))*\n           t^((1/par2) - 1))}\n}\n```\n:::\n\n\nIt's a bit confusing since the user function `survsim::crisk.sim()` takes `anc.ev` and `beta0.ev` as inputs, but these get transformed to `a.ev` and `b.ev` and then to `par1` and `par2` (ok that last transformation is straightforward).\n\nHere's the Weibull AFT programmed in `flexsurv`\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Weibull AFT parameterization hazard\nflexsurv::hweibull\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (x, shape, scale = 1, log = FALSE)\n{\n    h <- dbase(\"weibull\", log=log, x=x, shape=shape, scale=scale)\n    for (i in seq_along(h)) assign(names(h)[i], h[[i]])\n    if (log)\n        ret[ind] <- log(shape) + (shape-1)*log(x/scale) - log(scale)\n    else\n        ret[ind] <- shape * (x/scale)^(shape - 1)/scale\n    ret\n}\n<bytecode: 0x62fd9b6e0820>\n<environment: namespace:flexsurv>\n```\n\n\n:::\n:::\n\n\nIf the shape parameter is $a$ and scale parameter is $\\mu$, the hazard function is\n\n$$\nh_{\\text{wei}}^{\\text{flexsurv}}(t; a, \\mu) = a\\Bigg(\\frac{t}{\\mu}\\Bigg)^{a - 1} \\frac{1}{\\mu} = a \\mu^{-a}t^{a - 1}\n$$\n\nand for Weibull AFT from `survsim`\n\n$$\nh_{\\text{wei}}^{\\text{simsurv}}(t; p_1, p_2) = \\frac{1}{p_2} \\Bigg(\\Big(e^{p_1}\\Big)^{1 / p_2}\\Bigg)^{-1} t^{\\Big(\\frac{1}{p_2} - 1\\Big)}\n$$\n\nwith `beta0.ev = par1` = $p_1$ and `anc.ev = par2` = $p_2$. This gives the corresponding transformations for Weibull in `simsurv`, `anc.ev` = $1 / a$ (so inverse of `flexsurv` shape), and `beta0.ev` = $\\text{log}(\\mu)$ (so log of `flexsurv` scale).\n\nLog-logistic implemented in `flexsurv`\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# log-logistic hazard\nflexsurv::hllogis\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction(x, shape=1, scale=1, log = FALSE) \n{\n    h <- dbase(\"llogis\", log=log, x=x, shape=shape, scale=scale)\n    for (i in seq_along(h)) assign(names(h)[i], h[[i]])\n    if (log) ret[ind] <- log(shape) - log(scale) + (shape-1)*(log(x) - log(scale)) - log(1 + (x/scale)^shape)\n    else ret[ind] <- (shape/scale) * (x/scale)^{shape-1} / (1 + (x/scale)^shape)\n    ret\n}\n<bytecode: 0x62fd99288f80>\n<environment: namespace:flexsurv>\n```\n\n\n:::\n:::\n\n\nif shape is $a$ and scale is $b$, the hazard function is\n\n$$\nh_{\\text{ll}}^{\\text{flexsurv}}(t; a, b) = \\frac{(a / b) (t / b)^{a - 1}}{1 + (t / b)^a}\n$$\n\ncompared with the one from `simsurv` (with `par1` = $p_1$ and `par2` = $p_2$)\n\n$$\nh_{\\text{ll}}^{\\text{simsurv}}(t; p_1, p_2) = \\frac{p_1 p_2 t^{p_2 - 1}}{1 + p_1 t^{p_2}}\n$$\n\n$p_2 = \\text{anc.ev} = a$ (shape from `flexsurv`) and $p_1 = 1 / \\text{exp}(\\text{beta0.ev}) = a \\text{log}(b)$ (shape times log(scale) from `flexsurv`).\n\n### `survsim` function\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nsurvsim_sim_fn <- function(n = 1000) {\n  survsim::crisk.sim(\n    n = n,\n    foltime = 5,\n    # TTE distr\n    nsit = 3,\n    dist.ev = c(\"weibull\", \"llogistic\", \"weibull\"),\n    # anc.ev is 1 / shape, exp(beta0) is scale in flexsurv weibull AFT\n    # anc.ev is shape, beta0.ev is shape * log(scale) in flexsurv llogis\n    anc.ev = c(1 / 1.6, 5, 1),\n    beta0.ev = c(log(5), 5 * log(5), log(5)),\n    # covariate process\n    x = list(c(\"bern\", 0.5)),\n    beta = list(c(1.4), c(0), c(0)),\n    # censoring process, assume constant here to only apply administrative censoring\n    dist.cens = \"unif\", beta0.cens = 500, anc.cens = 500\n  ) %>%\n    as_tibble()\n}\n\nsurvsim_sim_fn(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 8\n     nid cause   time status start stop      z     x\n   <int> <dbl>  <dbl>  <dbl> <lgl> <lgl> <dbl> <dbl>\n 1     1    NA 5           0 NA    NA        1     0\n 2     2     1 3.79        1 NA    NA        1     0\n 3     3     3 0.321       1 NA    NA        1     0\n 4     4     3 1.02        1 NA    NA        1     1\n 5     5     1 2.38        1 NA    NA        1     1\n 6     6     3 2.45        1 NA    NA        1     1\n 7     7     3 0.0557      1 NA    NA        1     0\n 8     8     3 0.0681      1 NA    NA        1     0\n 9     9     3 0.0741      1 NA    NA        1     1\n10    10     1 2.34        1 NA    NA        1     0\n```\n\n\n:::\n:::\n\n\nThe $z$ column is for individual frailties.\n\n### Compare run times\n\nThe next figure shows a dot plot of the execution time distribution by repeating the process 10 times for both functions (again previously run results read back in)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# survsim_bench <- bench::mark(\n#   survsim_sim_fn(), iterations = 10, \n#   check = FALSE, time_unit = \"s\", memory = FALSE\n# )\n# write_rds(survsim_bench, file = \"saved_objects/survsim_bench.rds\")\n\nsurvsim_bench <- read_rds(\"saved_objects/survsim_bench.rds\")\n\n# my_implementation_bench <- bench::mark(\n#   run_pipeline(sample(1:100, size = 1, replace = TRUE)),\n#   iterations = 10, check = FALSE, time_unit = \"s\", memory = FALSE\n# )\n# write_rds(my_implementation_bench, file = \"saved_objects/my_implementation_bench.rds\")\n\nmy_implementation_bench <- read_rds(\"saved_objects/my_implementation_bench.rds\")\n\n# dot plot of the run times\ntibble(\n  time = c(survsim_bench %>% pluck(\"time\", 1), \n           my_implementation_bench %>% pluck(\"time\", 1)),\n  group = rep(c(\"survsim\", \"handrolled\"), each = 10)\n) %>%\n  ggplot(aes(x = time, y = group, fill = group, color = group)) +\n  geom_dotplot(binwidth = 0.05,  binaxis = \"x\", \n               method = \"histodot\", binpositions = \"bygroup\", \n               stackdir = \"centerwhole\") +\n  #geom_point(size = 3, position = position_jitter(height = 0.1, seed = 4)) +\n  #theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  theme(legend.position = \"none\") +\n  scale_x_continuous(breaks = seq(1, 4, 1), \n                     labels = seq(1, 4, 1), \n                     limits = c(1, 4)) +\n  xlab(\"Time (in seconds)\") +\n  ylab(\"\") + \n  scale_color_manual(values = c(\"orange\", \"gray30\")) + \n  scale_fill_manual(values = c(\"orange\", \"gray30\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nshows about 2x speedup; I'm assuming most of this comes from not having to use numerical integration but using the cumulative hazards programmed within `flexsurv`.\n\n### Compare samples\n\nThe next plot shows the empirical distribution functions for 100 samples of size 100 from the two implementations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# eCDFs for a large draw overlaid to compare the draws - they should be overlapping\n# set.seed(23)\n# survsim_sample <- survsim_sim_fn(n = 10000)\n# set.seed(23)\n# handrolled_sample <- simulate_single_dataset(n_rows = 10000, seed = NULL)\n#\n# write_rds(survsim_sample, file = \"saved_objects/survsim_sample.rds\")\n# write_rds(handrolled_sample, file = \"saved_objects/handrolled_sample.rds\")\n\nsurvsim_sample <- read_rds(\"saved_objects/survsim_sample.rds\")\nhandrolled_sample <- read_rds(\"saved_objects/handrolled_sample.rds\")\n\nbind_rows(\n  survsim_sample %>%\n    select(time) %>%\n    mutate(group = \"survsim\"),\n  handrolled_sample %>%\n    select(time) %>%\n    mutate(group = \"handrolled\")\n) %>%\n  mutate(rep = sample(1:100, size = 20000, replace = TRUE)) %>%\n  ggplot(aes(x = time, group = interaction(group, rep), color = group)) +\n  stat_ecdf(pad = FALSE, alpha = 0.8) +\n  xlab(\"Simulated Times\") +\n  ylab(\"Empirical CDF\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  scale_color_manual(values = c(\"orange\", \"gray30\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nIt seems that the gray curves (`survsim`) are a bit higher than the ones in orange (my implementation). I haven't looked into why, but it seems likely to be due to sampling variation.\n\n## References\n\nThe main refs for the material in this post\n\n-   Beyersmann, J., Latouche, A., Buchholz, A., & Schumacher, M. (2009). Simulating competing risks data in survival analysis. *Statistics in medicine*, *28*(6), 956-971.\n\n-   Allignol, A., Schumacher, M., Wanner, C. *et al.* Understanding competing risks: a simulation point of view. *BMC Med Res Methodol* **11**, 86 (2011). <https://doi.org/10.1186/1471-2288-11-86> - what I found cool is that this paper uses the non-parametric estimate of the baseline hazard function to simulate proportional hazard data from the cox model\n\n-   Crowther, M. J., & Lambert, P. C. (2012). Simulating Complex Survival Data. *The Stata Journal*, *12*(4), 674-687. <https://doi.org/10.1177/1536867X1201200407> (Original work published 2012)\n\n-   Putter, H., Fiocco, M. and Geskus, R.B. (2007), Tutorial in biostatistics: competing risks and multi-state models. Statist. Med., 26: 2389-2430. <https://doi.org/10.1002/sim.2712> - key paper on competing risk analyses\n\n-   Hinchliffe, S.R., Lambert, P.C. Flexible parametric modelling of cause-specific hazards to estimate cumulative incidence functions. *BMC Med Res Methodol* **13**, 13 (2013). <https://doi.org/10.1186/1471-2288-13-13>\n\n-   `flexsurv` package [vignette](https://cran.r-project.org/web/packages/flexsurv/vignettes/multistate.pdf) on multi-state modelling\n\n-   `flexsurv` package [introduction vignette](https://cran.r-project.org/web/packages/flexsurv/vignettes/flexsurv.pdf)\n\n-   `flexsurv` vignette on [distributions](https://cran.r-project.org/web/packages/flexsurv/vignettes/distributions.pdf)\n\n-   `survsim` package [docs](https://cran.r-project.org/web/packages/survsim/survsim.pdf)\n\nSome other papers / package vignettes I came across and read (parts of if not entirely)\n\n-   `survival` package [multi-state vignette](https://cran.r-project.org/web/packages/survival/vignettes/compete.pdf)\n\n-   `survival` package [vignette](https://cran.r-project.org/web/packages/survivalVignettes/vignettes/tutorial.html) reproducing Putter et al analysis\n\n-   Andersen PK, Geskus RB, de Witte T, Putter H. Competing risks in epidemiology: possibilities and pitfalls. Int J Epidemiol. 2012 Jun;41(3):861-70. doi: 10.1093/ije/dyr213. Epub 2012 Jan 9. PMID: 22253319; PMCID: PMC3396320.\n\n-   Austin PC, Lee DS, Fine JP. Introduction to the Analysis of Survival Data in the Presence of Competing Risks. Circulation. 2016 Feb 9;133(6):601-9. doi: 10.1161/CIRCULATIONAHA.115.017719. PMID: 26858290; PMCID: PMC4741409.\n\n-   Fine, J. P., & Gray, R. J. (1999). A Proportional Hazards Model for the Subdistribution of a Competing Risk. *Journal of the American Statistical Association*, *94*(446), 496–509. https://doi.org/10.2307/2670170\n\n-   Latouche A, Allignol A, Beyersmann J, Labopin M, Fine JP. A competing risks analysis should report results on all cause-specific hazards and cumulative incidence functions. J Clin Epidemiol. 2013 Jun;66(6):648-53. doi: 10.1016/j.jclinepi.2012.09.017. Epub 2013 Feb 14. PMID: 23415868.\n\n-   Scheike, T. H., & Zhang, M.-J. (2011). Analyzing Competing Risk Data Using the R timereg Package. *Journal of Statistical Software*, *38*(2), 1–15. https://doi.org/10.18637/jss.v038.i02\n\n-   Thomas H. Scheike, Mei-Jie Zhang, Thomas A. Gerds, Predicting cumulative incidence probability by direct binomial regression, *Biometrika*, Volume 95, Issue 1, March 2008, Pages 205–220, <https://doi.org/10.1093/biomet/asm096>\n\n-   Edouard F Bonneville, Liesbeth C de Wreede, Hein Putter, Why you should avoid using multiple Fine–Gray models: insights from (attempts at) simulating proportional subdistribution hazards data, *Journal of the Royal Statistical Society Series A: Statistics in Society*, Volume 187, Issue 3, August 2024, Pages 580–593, <https://doi.org/10.1093/jrsssa/qnae056>\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}