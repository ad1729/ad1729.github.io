{
  "hash": "0f8c49890ac1de99d0470959fccc91e1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simulating data from a time-to-event experiment with non-proportional hazards, random censoring, and cure\"\ndate: \"2024-09-22\"\nlast-modified-date: \"2024-09-22\"\ncategories: [Experimentation, R, Simulation, Time-to-event]\nimage: \"post-image.png\"\nfootnotes-hover: true\ncode-fold: false\n---\n\n\nThis post is about simulating data from a log-logistic accelerated failure time mixture cure model with censoring. It also visually compares the estimated cumulative incidence curves obtained from several time-to-event estimators on a large simulated dataset.\n\nIt's been quite a while since I've worked with time-to-event data and models. Also known as *survival analysis*, it is ideally suited for situations where the interest is not only in predicting *whether* an event will occur, but *how long it will take for it to occur.*\n\nSo I'm writing this post to reacquaint myself with some of the key concepts — and to learn some new things along the way — by simulating and analyzing data from a fictitious experiment motivated by a real experiment I worked on many moons ago.\n\nBefore writing any code, the following packages are attached:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(flexsurvcure)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: survival\nLoading required package: flexsurv\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggsurvfit)\n\ntheme_set(theme_bw())\n```\n:::\n\n\nSuppose a marketing manager is interested in the time it takes for an upgraded product to be adopted after launch among users of the previous version of the product. This rate-of-upgrade could be potentially hastened via a marketing campaign, which can be assumed to have a *transient response*[^1] — that is, the impact of this campaign decreases with time once the campaign has concluded.\n\n[^1]: Not sure what this is called in marketing jargon but this is how it's called in [engineering](https://en.wikipedia.org/wiki/Transient_response)\n\n## Key concepts\n\nOne of the key objects in survival analysis is the [*hazard function*](https://en.wikipedia.org/wiki/Failure_rate) ($h(t)$), i.e., the rate at which the event of interest (e.g., product purchase) occurs as a function of time. Mathematically, it is expressed for *continuous-time* models as\n\n$$\nh(t) = \n\\lim_{\\Delta t \\to 0} \\frac{P(t \\le T \\lt t + \\Delta t | T \\ge t)}{\\Delta t}\n$$\n\nwhere $T$ indicates the event time of interest which varies from individual to individual, $t$ is a specific value of time, and $\\Delta t$ is some very small increment in time. This event rate function cannot be interpreted as a probability but only as a rate, since it can have values larger than 1.\n\nIn case of *discrete-time* models, the hazard rate can be interpreted as the (conditional) probability $P(T = t | T \\ge t)$.\n\nFor our experiment, suppose the hazards in the treatment and control groups look like this:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nhazard_function_loglogistic <- function(alpha, beta, \n                               min_t = 0, \n                               max_t = 24, \n                               step_t = 0.1) {\n  \n  time <- seq(min_t, max_t, step_t)\n  numerator <- (beta / alpha) * ((time / alpha) ^ (beta - 1))\n  denominator <- 1 + ((time / alpha) ^ beta)\n\n  tibble(\n    alpha = alpha,\n    beta = beta,\n    parameters = paste0(\"LL(\", alpha, \n                        \", \", beta, \")\"),\n    time = time,\n    hazard = numerator / denominator\n  )\n}\n\nloglogistic_attributes <- bind_rows(\n  hazard_function_loglogistic(alpha = 4, beta = 7),\n  hazard_function_loglogistic(alpha = 5, beta = 7)\n) %>% \n  mutate(\n    parameters = case_when(\n      alpha == 4 ~ \"Treatment group (T ~ LL(4, 7))\",\n      alpha == 5 ~ \"Control group (T ~ LL(5, 7))\"\n    )\n  ) \n\nloglogistic_attributes %>% \n  ggplot(aes(x = time, y = hazard, group = parameters, color = parameters)) +\n  geom_line(linewidth = 1.1) +\n  # campaign end date\n  geom_vline(xintercept = 6, linetype = \"dotted\", color = \"gray40\") + \n  annotate(geom = \"text\", x = 6.2, y = 0.6, color = \"gray40\",\n           label = \"Campaign\\nends after\\n6 weeks\", hjust = \"left\") + \n  # impact of treatment disappears by 2.5 months\n  geom_vline(xintercept = 10, linetype = \"dotted\", color = \"gray40\") + \n  annotate(geom = \"text\", x = 9.8, y = 0.1, color = \"gray40\",\n           label = \"Impact of\\ntreatment\\ndies down\", hjust = \"right\") + \n  # analysis date (right-censoring at 3 months)\n  geom_vline(xintercept = 12, linetype = \"dotted\", color = \"gray40\") + \n  annotate(geom = \"text\", x = 12.2, y = 0.1, color = \"gray40\",\n           label = glue::glue(\"Cut-off date for analysis\\n\", \n                              \"(administrative right-censoring \", \n                              \"at 3 months)\"), \n           hjust = \"left\") + \n  scale_color_manual(values = c(\"gray20\", \"forestgreen\")) +\n  scale_x_continuous(breaks = seq(0, 24, 4)) +\n  scale_y_continuous(breaks = seq(0, 1.2, 0.2)) + \n  labs(x = \"Time since product launch (in weeks)\", y = \"Hazard rate\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThese hazard functions can be expressed mathematically as\\\n$$\nh(t; \\alpha, \\beta) = \\frac{(\\beta/\\alpha) (t / \\alpha)^{\\beta - 1}}{1 + (t / \\alpha)^\\beta}\n$$\n\nand correspond to a [*log-logistic distribution*](https://en.wikipedia.org/wiki/Log-logistic_distribution) (abbreviated as $\\text{LL}(\\alpha, \\beta)$) for the event times with *scale* parameter $\\alpha = 4$ for the treatment group, $\\alpha = 5$ for the control group, and shape parameter $\\beta = 7$ identical in both groups.\n\nThe motivation for picking the $\\text{LL}(\\alpha, \\beta)$ model for this experiment is that it's a well known example of an [*accelerated failure time (AFT) model*](https://en.wikipedia.org/wiki/Accelerated_failure_time_model). The impact of treatment is assumed to be positive and transient, where the treatment \"accelerates\" the event process compared to the control group, with this \"accelerating\" treatment group peak occurring before the peak in the control group. The hazard function in the treatment group tapers off after some time (8 weeks in this case). The non-monotonic nature — i.e., of the function first increasing and then decreasing with time — can refer to the time it takes for the information about the new product to spread through the population.\n\nThe AFT model can be contrasted with the [*proportional hazards (PH) model*](https://en.wikipedia.org/wiki/Proportional_hazards_model), where the hazard rates for the two groups are assumed to be proportional over time. The [*Weibull distribution*](https://en.wikipedia.org/wiki/Weibull_distribution) is a prominent example, although it can be parameterized as an AFT model as well. Compared to the log-logistic distribution, it has a monotonic hazard function\n\n$$\nh(t; \\alpha, \\beta) = \\frac{\\beta}{\\alpha} \\Bigg(\\frac{x}{\\alpha}\\Bigg) ^ {\\beta - 1}\n$$\n\nwith shape parameter $\\beta$, and scale parameter $\\alpha$, so is not suited for settings where the hazard is expected to go up and then down (or other non-monotonic shapes).\n\nThe hazard rate is a bit hard to interpret due to the fact that 1) it's a conditional measure, 2) is a rate rather than a probability (so can have values \\> 1), and 3) depends on the chosen unit of time.\n\nA related quantity known as the *survivor function* (aka survival curve) is the marginal probability of being event-free (\"surviving\") beyond time $t$. Mathematically,\n\n$$\nS(t) = P[T > t] = \\text{exp} \\Bigg[ - \\int_0^t h(u) du \\Bigg]\n$$\n\nwhere $h(u)$ is the hazard function (defined above).\n\nWhen the outcome is death due to disease (in healthcare), or machine failure (in engineering), it makes sense to look at factors (or treatments) that prolong survival. In the scenario where the event occurring faster is seen as positive, the survivor function can be flipped to get the *cumulative incidence curve (CIC)*\n\n$$\nP[T \\le t] = F(t) = 1 - S(t)\n$$\n\nwhere the curve traces the probability of experiencing the event before some time $t$, and $F(t)$ is the [CDF](https://en.wikipedia.org/wiki/Cumulative_distribution_function) of the event times. The cumulative incidence curve corresponding to $\\text{LL}(\\alpha, \\beta)$ is given by $F(t) = [ 1 + (t / \\alpha)^ {-\\beta}] ^ {-1}$ (with the survivor function $S(t) = [1 + (t / \\alpha)^ {\\beta}] ^ {-1}$ — notice the sign difference for $\\beta$)\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsurvivor_loglogistic <- function(alpha, beta, time) {\n  1 / (1 + ((time / alpha) ^ beta))\n}\n\nloglogistic_attributes <- loglogistic_attributes %>% \n  mutate(\n    survival = survivor_loglogistic(alpha, beta, time),\n    cdf = 1 - survival\n  ) \n\ncumulative_incidence_plot <- loglogistic_attributes %>% \n  ggplot(aes(x = time, y = cdf, group = parameters, color = parameters)) + \n  geom_line(linewidth = 1.1) + \n  # campaign end date\n  geom_vline(xintercept = 6, linetype = \"dotted\", color = \"gray40\") + \n  annotate(geom = \"text\", x = 6.2, y = 0.6, color = \"gray40\",\n           label = \"Campaign\\nends after\\n6 weeks\", hjust = \"left\") + \n  # impact of treatment disappears by 2.5 months\n  geom_vline(xintercept = 10, linetype = \"dotted\", color = \"gray40\") + \n  annotate(geom = \"text\", x = 9.8, y = 0.1, color = \"gray40\",\n           label = \"Impact of\\ntreatment\\ndies down\", hjust = \"right\") + \n  # analysis date (right-censoring at 3 months)\n  geom_vline(xintercept = 12, linetype = \"dotted\", color = \"gray40\") + \n  annotate(geom = \"text\", x = 12.2, y = 0.1, color = \"gray40\",\n           label = glue::glue(\"Cut-off date for analysis\\n\", \n                              \"(administrative right-censoring \", \n                              \"at 3 months)\"), \n           hjust = \"left\") + \n  scale_color_manual(values = c(\"gray20\", \"forestgreen\")) +\n  scale_y_continuous(labels = scales::label_percent()) +\n  labs(x = \"Time since product launch (in weeks)\", \n       y = \"Cumulative incidence\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\ncumulative_incidence_plot + \n  scale_x_continuous(breaks = seq(0, 24, 4))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nAs expected of CDFs, the CIC starts at 0% for both groups at $t = 0$ and keeps increasing over time until it reaches 100%.\n\n## Simulate true event times\n\nFinally, we can get around to simulating some data from our experiment. Normally I'd use either the [`coxed::sim.survdata()`](https://cran.r-project.org/web/packages/coxed/) function or the [`simsurv::simsurv()`](https://cran.r-project.org/web/packages/simsurv/) function, but in the interest of learning[^2] I'm going to do this manually using the *cumulative hazard inversion method* (usually attributed to [this paper](https://onlinelibrary.wiley.com/doi/10.1002/sim.2059) and concisely described in the `simsurv` package vignette [here](https://cran.r-project.org/web/packages/simsurv/vignettes/simsurv_technical.html)).\n\n[^2]: These packages don't support log-logistic models out-of-the-box anyway.\n\nFor simulating data from a Weibull PH model as opposed to the log-logistic AFT model, see [this](https://stats.stackexchange.com/questions/135124/how-to-create-a-toy-survival-time-to-event-data-with-right-censoring), [this](https://stats.stackexchange.com/questions/603395/simulating-survival-times), or the papers in the previous paragraph. [This paper](https://onlinelibrary.wiley.com/doi/10.1002/sim.5823) is also pretty useful for simulating data from more complex settings.\n\nPlugging in the value of $F(T_i)$ for the log-logistic distribution, and doing some algebra gives the function for simulating the true (or latent) event time $T_i$ for the $i$-th individual from a log-logistic distribution (matches equation 8 from [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5061707/)).\n\n$$\n\\begin{align*}\nS(T_i) = 1 - F(T_i) &= U_i \\sim \\text{Uniform}(0, 1) \\\\\n1 - \\frac{1}{1 + (T_i / \\alpha)^{-\\beta}} &= U_i \\\\\n1 + \\Bigg(\\frac{T_i}{\\alpha}\\Bigg)^{-\\beta} &= \\frac{1}{1 - U_i} \\\\\n\\Bigg(\\frac{T_i}{\\alpha}\\Bigg)^{-\\beta} &= \\frac{U_i}{1 - U_i} \\\\\n\\Bigg(\\frac{\\alpha}{T_i}\\Bigg)^{\\beta} &= \\frac{1 - U_i}{U_i} \\\\\n\\frac{\\alpha}{T_i} &= \\Bigg(\\frac{1 - U_i}{U_i}\\Bigg)^{1 / \\beta} \\\\\nT_i &= \\alpha (U_i^{-1} - 1)^{-1 / \\beta} \\\\\n\\end{align*}\n$$\n\nThe Wikipedia page for log-logistic distribution mentions that $\\text{log}(\\alpha_i) = X_i^T \\gamma$ can be used to specify covariates that influence the scale parameter $\\alpha_i$ — with $\\gamma$ the ($p$-dimensional row) vector of coefficients, and $X_i$ the ($p$-dimensional row) vector of covariates for individual $i$. The shape parameter $\\beta$ is assumed to be constant across covariate levels, but it's possible to model $\\beta$ as a function of covariates as well with $\\text{log}(\\beta_i) = Z_i^T \\delta$ where $Z$ and $X$ can be identical, partly overlapping, or completely disjoint covariate sets, and $\\delta$ the associated coefficients. Using logs ensures that both $\\alpha_i$ and $\\beta_i$ will have values greater than zero. This gives\n\n$$\nT_i = \\text{exp}(X_i^T \\gamma)(U_i^{-1} - 1)^{-1 / \\text{exp}(Z_i^T \\delta)}\n$$\n\nWe can draw simulated event times across 30 experiments each with about 1000 individuals split evenly between the treatment and control groups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_sim <- 30\nn_sample <- 1000\nn_total <- n_sim * n_sample\n\nset.seed(43)\nloglogistic_samples <- tibble(\n  sim_id = rep(1:n_sim, each = n_sample),\n  id = rep(1:n_sample, times = n_sim),\n  treatment = sample(c(0, 1), size = n_total,\n                     replace = TRUE, prob = c(0.5, 0.5)),\n  parameters = case_when(\n    treatment == 1 ~ \"Treatment group (T ~ LL(4, 7))\",\n    treatment == 0 ~ \"Control group (T ~ LL(5, 7))\"\n  ),\n  # -log(1.25) corresponds to alpha = 5 in control\n  # and alpha = 4 in treatment\n  linear_predictor_alpha = log(5) - log(1.25) * treatment,\n  linear_predictor_beta =log(7),\n  u = runif(n_total, min = 0, max = 1),\n  time = (exp(linear_predictor_alpha) *\n            (((1 / u) - 1) ^ (-1 / exp(linear_predictor_beta))))\n)\n\nloglogistic_samples %>% glimpse(width = 80)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 30,000\nColumns: 8\n$ sim_id                 <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ id                     <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …\n$ treatment              <dbl> 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,…\n$ parameters             <chr> \"Treatment group (T ~ LL(4, 7))\", \"Control grou…\n$ linear_predictor_alpha <dbl> 1.386294, 1.609438, 1.386294, 1.609438, 1.38629…\n$ linear_predictor_beta  <dbl> 1.94591, 1.94591, 1.94591, 1.94591, 1.94591, 1.…\n$ u                      <dbl> 0.74638903, 0.60531731, 0.35898984, 0.93116119,…\n$ time                   <dbl> 4.666927, 5.315004, 3.682061, 7.253854, 4.86455…\n```\n\n\n:::\n:::\n\n\nThe estimated CDFs for each sample are added to a zoomed-in version of cumulative incidence figure above.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncumulative_incidence_plot_samples <- cumulative_incidence_plot \n\n# for this hacky solution to overlay the true curves over\n# the sampled curves, see \n# https://stackoverflow.com/questions/20249653/insert-layer-underneath-existing-layers-in-ggplot2-object\ncumulative_incidence_plot_samples$layers <- c(\n  stat_ecdf(\n    data = loglogistic_samples, \n    aes(x = time, linetype = parameters, \n        group = interaction(sim_id, parameters)), \n    inherit.aes = FALSE, \n    color = \"gray70\"\n  ), \n  cumulative_incidence_plot$layers\n)\n\ncumulative_incidence_plot_samples + \n  coord_cartesian(xlim = c(2, 10)) + \n  scale_x_continuous(breaks = seq(2, 10, 1)) + \n  guides(\n    color = guide_legend(nrow = 2), \n    linetype = guide_legend(nrow = 2)\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThe empirical CDFs of survival times for each sample fluctuate around the true curve in each group, which is reassuring. These simulated event times are the *true* individual event times for that specific sample of individuals. These are not always observed, which brings us to our next point.\n\n## Add censoring\n\nOne complication I've ignored so far is *(right-)censoring*. Individuals are said to be right-censored if they haven't had the event of interest yet at the time of analysis and therefore only a lower bound on their true event time is observed.\n\nThe most common data structure for survival analysis is the tuple $(Y_i, \\delta_i)$ where $Y_i = \\text{min}(T_i, C_i)$ is the smaller of the event time $T_i$ or the censoring time $C_i$, and $\\delta_i = I(T_i \\le C_i)$ is the event indicator with value 1 indicating the true event time is observed, and 0 indicating that the observation is censored. The censoring indicator is $1 - \\delta_i$.\n\nNon-informative administrative (or Type-1) right censoring is the simplest type of censoring where the censoring time is the same for all individuals. If two analyses are carried out — one after three weeks, and another after twelve weeks — then the same individual with true $T_i = 4.5$ is recorded as $(Y_i = 3, \\delta_i = 0)$ in the first analysis and $(Y_i = 4.5, \\delta_i = 1)$ in the second analysis.\n\nThis can be applied to the simulated event times easily enough using simple conditionals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloglogistic_samples %>% \n  select(time) %>% \n  mutate(\n    latent = time,\n    time3 = pmin(time, 3.0),\n    cens3 = as.numeric(time > 3.0),\n    time6 = pmin(time, 6.0),\n    cens6 = as.numeric(time > 6.0),\n    time12 = pmin(time, 12.0),\n    cens12 = as.numeric(time > 12.0)\n  ) %>% \n  select(-time) %>% \n  summary(digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     latent          time3          cens3          time6          cens6     \n Min.   : 0.84   Min.   :0.84   Min.   :0.00   Min.   :0.84   Min.   :0.00  \n 1st Qu.: 3.73   1st Qu.:3.00   1st Qu.:1.00   1st Qu.:3.73   1st Qu.:0.00  \n Median : 4.45   Median :3.00   Median :1.00   Median :4.45   Median :0.00  \n Mean   : 4.64   Mean   :2.97   Mean   :0.93   Mean   :4.49   Mean   :0.13  \n 3rd Qu.: 5.33   3rd Qu.:3.00   3rd Qu.:1.00   3rd Qu.:5.33   3rd Qu.:0.00  \n Max.   :20.05   Max.   :3.00   Max.   :1.00   Max.   :6.00   Max.   :1.00  \n     time12          cens12      \n Min.   : 0.84   Min.   :0.0000  \n 1st Qu.: 3.73   1st Qu.:0.0000  \n Median : 4.45   Median :0.0000  \n Mean   : 4.64   Mean   :0.0015  \n 3rd Qu.: 5.33   3rd Qu.:0.0000  \n Max.   :12.00   Max.   :1.0000  \n```\n\n\n:::\n:::\n\n\nThe overall true event time distribution — in the latent column — goes up to 20 weeks, but due to censoring, 93% of observations are censored at 3 weeks, 13% at 6 weeks, and fewer than 1% at twelve weeks, with the maximum time equal to the censoring time. The larger the proportion of censoring, the larger the difference between the true mean and the mean of the censored event time distribution.\n\nIn more complex settings, censoring times can be simulated from (say) an exponential or gamma distribution that is assumed to be independent of the event times, or the censoring distribution can vary as a function of covariates. The next plot overlays the true event time densities in each treatment group on top of the censoring distribution $C_i \\sim \\text{Gamma}(6, 1)$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(43)\nloglogistic_samples %>%\n  select(parameters, time) %>%\n  bind_rows(\n    tibble(\n      parameters = \"Censoring times ~ Gamma(6, 1)\",\n      time = rgamma(floor(n_total / 2), shape = 6, scale = 1)\n    )\n  ) %>%\n  ggplot(aes(x = time, fill = parameters, color = parameters)) +\n  geom_density(alpha = 0.6) +\n  xlab(\"Time since product launch (in weeks)\") +\n  theme(legend.title = element_blank(), legend.position = \"bottom\") + \n  scale_color_manual(values = c(\"purple3\", \"gray20\", \"forestgreen\")) +\n  scale_fill_manual(values = c(\"purple3\", \"gray20\", \"forestgreen\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nIn this case, the code is nearly the same.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(43)\nloglogistic_samples %>%\n  select(parameters, latent = time) %>%\n  mutate(\n    censoring_time = rgamma(n_total, shape = 6, scale = 1),\n    observed_time = pmin(latent, censoring_time),\n    cens = as.numeric(latent > censoring_time)\n  ) %>%\n  summarize(\n    across(\n      .cols = everything(), \n      .fns = ~ round(mean(.x), 2), \n      .names = \"mean_{.col}\"\n    ), \n    .by = parameters\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  parameters        mean_latent mean_censoring_time mean_observed_time mean_cens\n  <chr>                   <dbl>               <dbl>              <dbl>     <dbl>\n1 Treatment group …        4.12                5.99               3.81      0.25\n2 Control group (T…        5.16                6                  4.46      0.4 \n```\n\n\n:::\n:::\n\n\nSummary of the simulated data shows that 25% of the individuals in the treatment group are censored compared to 40% of the individuals in the control group over the full range of time values (about 20 weeks).\n\n## Add cure\n\nAnother complication is the presence of *(statistical) cure*. Some individuals will never purchase the product irrespective of whichever group they fall in. Which means that if the follow-up period is extended from 12 weeks to 150 or 500 weeks, these so-called \"cured\" individuals will still show up as censored because their true survival time is effectively infinite. Ignoring cure and using a cox model can lead to biased estimates of the quantities we're interested in, as figure 2 of [this paper](https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-031017-100101) visually demonstrates. It also gives an overview of the different types of (major) cure models. Another reference I found useful was [this one](https://dial.uclouvain.be/pr/boreal/object/boreal%3A220045/datastream/PDF_02/view)[^3].\n\n[^3]: This is the link to the Bertand and Legrand book chapter mentioned in the references below.\n\nThe family of *mixture cure model* has the following survivor function form\n\n$$\nS_{\\text{pop}}(t|I, L) = (1 - \\pi(I)) + \\pi(I) S_u(t|L)\n$$\n\nwith two parts: the first part is based on the *incidence* which is whether an individual is cured or not ($\\pi(I) = P[B = 1 | I]$ is the probability of being cured as a function of covariate $I$, and $B$ is the latent cure status); and the second part is concerned with *latency*, which is about how long it takes for the event to occur among the uncured. $L$ is the set of factors that affect latency. Overlap between $I$ and $L$ is possible.\n\nThe other major family of cure models is the *promotion time cure model* with the survivor function\n\n$$\nS_{\\text{pop}}(t|x) = \\text{exp}[-\\theta(x)F(t)]\n$$\n\nwhich has a PH interpretation. In this model, parameters affecting cure and survival are not separated like they are in the mixture cure model.\n\nFinally, [this paper](https://journals.sagepub.com/doi/10.1177/1536867X0700700304) mentions relative survival / excess mortality models as well, which considers individuals in the treatment or control group to be cured when the hazard rate decreases to background levels.\n\nThe simplest cure model is the constant one, where the cure fraction is the same across all groups. This can be simulated by generating a latent \"cure\" variable $B_i \\sim \\text{Bernoulli}(\\pi_i)$ (with $\\pi_i = \\pi = 0.3$ denoting the proportion of cured individuals in the next code chunk)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(43)\nloglogistic_samples %>%\n  mutate(\n    cured = rbinom(n_total, 1, 0.3),\n    time = ifelse(cured == 1, 10000, time), \n    time6 = pmin(time, 6.0),\n    cens6 = as.numeric(time > 6.0)\n  ) %>% \n  count(cured, cens6) %>% \n  mutate(p = round(100 * (n / sum(n)), 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  cured cens6     n     p\n  <int> <dbl> <int> <dbl>\n1     0     0 18898  63  \n2     0     1  2117   7.1\n3     1     1  8985  30  \n```\n\n\n:::\n:::\n\n\nIf six weeks is used as the cutoff time for analysis, 63% of the individuals overall have experienced the event of interest, and 37% are censored of which 7% are the censored uncured observations, and 30% are the censored cured individuals.\n\nThe logistic model for incidence can be more elaborate — the following code generates data where the cure fraction varies as a function of (say) age\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloglogistic_samples %>%\n  mutate(\n    age = {\n      rgamma(n = n_total, shape = 40, rate = 0.7) %>% \n        pmin(., 90) %>% \n        pmax(20, .) %>% \n        round(., 1)\n    },\n    cured = rbinom(n_total, 1, plogis(4 - 0.07 * age)),\n    time = ifelse(cured == 1, 10000, time),\n    time6 = pmin(time, 6.0),\n    cens6 = as.numeric(time > 6.0),\n    event6 = 1 - cens6,\n  )\n```\n:::\n\n\n## Putting it all together\n\nAll of the respective code chunks can be combined into a single call with a total sample size of $n = 50,000$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_sim <- 50\nn_sample <- 1000\nn_total <- n_sim * n_sample\n\nset.seed(43)\n\nsimulated_data <- tibble(\n  #\n  # simulate id variables and covariates\n  #\n  sim_id = rep(1:n_sim, each = n_sample),\n  id = rep(1:n_sample, times = n_sim),\n  treatment = sample(c(0, 1), size = n_total,\n                     replace = TRUE, prob = c(0.5, 0.5)),\n  parameters = case_when(\n    treatment == 1 ~ \"Treatment group (T ~ LL(4, 7))\",\n    treatment == 0 ~ \"Control group (T ~ LL(5, 7))\"\n  ),\n  age = {\n      rgamma(n = n_total, shape = 40, rate = 0.7) %>% \n        pmin(., 90) %>% \n        pmax(20, .) %>% \n        round(., 1)\n    },\n  #\n  # simulate latent event times as a function of covariates\n  #\n  linear_predictor_alpha = log(5) - log(1.25) * treatment,\n  linear_predictor_beta =log(7),\n  u = runif(n_total, min = 0, max = 1),\n  uncured_time = (exp(linear_predictor_alpha) *\n            (((1 / u) - 1) ^ (-1 / exp(linear_predictor_beta)))), \n  # \n  # simulate the cure / incidence part from a logistic model\n  #\n  cured = rbinom(n_total, 1, plogis(4 - 0.07 * age)),\n  latent_time = ifelse(cured == 1, 10000, uncured_time),\n  #\n  # simulate censoring times and censor some individuals\n  # \n  random_censoring = rgamma(n_total, shape = 6, scale = 1),\n  # keep the smallest of the random censoring \n  # and administrative censoring (at t = 6) times\n  censoring_time = pmin(random_censoring, 6),\n  time = pmin(latent_time, censoring_time),\n  cens = as.numeric(latent_time > censoring_time), \n  event = 1 - cens\n)\n\nsimulated_data %>% glimpse(width = 80)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 50,000\nColumns: 16\n$ sim_id                 <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ id                     <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …\n$ treatment              <dbl> 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,…\n$ parameters             <chr> \"Treatment group (T ~ LL(4, 7))\", \"Control grou…\n$ age                    <dbl> 63.7, 44.8, 62.5, 58.0, 66.1, 39.5, 66.5, 54.3,…\n$ linear_predictor_alpha <dbl> 1.386294, 1.609438, 1.386294, 1.609438, 1.38629…\n$ linear_predictor_beta  <dbl> 1.94591, 1.94591, 1.94591, 1.94591, 1.94591, 1.…\n$ u                      <dbl> 0.83141130, 0.70736178, 0.03092926, 0.12105442,…\n$ uncured_time           <dbl> 5.024099, 5.671901, 2.445388, 3.766801, 2.51775…\n$ cured                  <int> 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,…\n$ latent_time            <dbl> 5.024099, 5.671901, 2.445388, 3.766801, 2.51775…\n$ random_censoring       <dbl> 6.513541, 9.204403, 3.982804, 12.196175, 8.3553…\n$ censoring_time         <dbl> 6.000000, 6.000000, 3.982804, 6.000000, 6.00000…\n$ time                   <dbl> 5.024099, 5.671901, 2.445388, 3.766801, 2.51775…\n$ cens                   <dbl> 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,…\n$ event                  <dbl> 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,…\n```\n\n\n:::\n:::\n\n\n## Sanity check\n\nAs a sanity check, the log-logistic mixture cure model can be fit to the simulated data using the `flexsurvcure` package to see whether we can recover the parameters used to generate data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated_data %>% \n  # rename the treatment variable to make the printed summary nicer looking\n  rename(trt = treatment) %>% \n  flexsurvcure(\n    formula = Surv(time, event, type = \"right\") ~ age,\n    data = .,\n    dist = \"llogis\",\n    link = \"logistic\",\n    anc = list(\n      scale = ~ trt,\n      shape = ~ trt\n    ),\n    mixture = TRUE\n  )\n```\n:::\n\n\nThe code takes a minute or two to run, so the saved model can be read back in.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nread_rds(\"sanity-check-model.rds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nflexsurvcure(formula = Surv(time, event, type = \"right\") ~ age, \n    data = ., dist = \"llogis\", link = \"logistic\", mixture = TRUE, \n    anc = list(scale = ~trt, shape = ~trt))\n\nEstimates: \n            data mean  est       L95%      U95%      se        exp(est)\ntheta             NA    0.98025   0.97650   0.98341        NA        NA\nshape             NA    7.14335   6.97237   7.31851   0.08829        NA\nscale             NA    4.98067   4.94343   5.01819   0.01907        NA\nage         57.15674   -0.06809  -0.07126  -0.06492   0.00162   0.93418\nshape(trt)   0.50436   -0.01890  -0.04914   0.01134   0.01543   0.98127\nscale(trt)   0.50436   -0.21891  -0.22755  -0.21027   0.00441   0.80340\n            L95%      U95%    \ntheta             NA        NA\nshape             NA        NA\nscale             NA        NA\nage          0.93122   0.93714\nshape(trt)   0.95204   1.01140\nscale(trt)   0.79648   0.81037\n\nN = 50000,  Events: 15676,  Censored: 34324\nTotal time at risk: 228001.9\nLog-likelihood = -42997.34, df = 6\nAIC = 86006.67\n```\n\n\n:::\n:::\n\n\nThe coefficients for the incidence submodel are usually on the logit scale, and the coefficients for the latency submodel are usually on the log scale. The printed output is a little confusing at first glance since the coefficients in the `est` column are either untransformed (age, shape and scale parameters in the treatment group), exponentiated (scale and shape parameters in the control group), or inverse-logit transformed (theta). It may have been better to split the printed output into two sections — one for the incidence model, and the other for latency. Similar to how `mgcv` gives separate output for the parametric and the spline effects.\n\nThe theta parameter is the intercept in the incidence submodel, with a true value of 4 on the logit scale, and therefore $P[B = 1 | \\text{age} = 0] = 0.982$ (calling `plogis(4)` in R or calculating $1 / (1 + \\text{exp}(-4))$). The estimated value is 0.9802, or `qlogis(0.98025) = 3.904` on the logit scale. The age coefficient is estimated to be -0.068 on the logit scale with the true value set to -0.07. This corresponds to an odds ratio of $\\text{exp}(-0.06809) = 0.934$.\n\nThe shape parameter $\\beta$ of the log-logistic distribution in the latency submodel is very close to the true value of 7. The `shape(trt)` value is very close to the true value of 0, since $\\beta$ is assumed to be the same in each arm.\n\nThe scale parameter $\\alpha$ in the control group is very close (4.98) to the true value of 5. The coefficient for $\\alpha$ in the treatment group — with the true value of 4 — can be derived by either 1) multiplying the alpha in the control group by `exp(scale(trt))` (so $4.98067 \\times 0.80340 \\approx 4$), or 2) by adding these up on the log scale and exponentiating the result ($\\text{exp}(\\text{log}(4.98067) - 0.21891)$).\n\n## Comparing arm-specific cumulative incidence estimates\n\nThis post could very well end now, but I already wrote the code for this section and I don't feel like making another post, so I've included just the plots here with minimal code. The full code for this section can be found [here](https://github.com/ad1729/ad1729.github.io/blob/master/posts/simulating-survival-data-with-non-PH-and-cure/analyze-data.R).\n\nEssentially, I'm simulating data with administrative censoring at three and twelve weeks, with a constant cure fraction of 30%, and visually comparing the derived cumulative incidence curves in each of the treatment and control groups from the survivor functions estimated via 1) a *Kaplan-Meier* fit, 2) a *stratified cox model*, 3) the *casebase* sampling approach described in [this](https://doi.org/10.2202/1557-4679.1125) paper and implemented in the [`casebase` package](https://cran.r-project.org/web/packages/casebase/index.html)[^4], and 4) a log-logistic mixture cure model with the treatment status as a variable in both the incidence and latency submodels.\n\n[^4]: which I find extra attractive because it supports fitting penalized splines via the `mgcv` package\n\nThe following plot shows $\\text{log}(-\\text{log}({\\hat{S}(t)}))$ vs $\\text{log}(t)$ in the left panel, and $\\text{log}(\\hat{S}(t) / (1 - \\hat{S}(t)))$ vs $\\text{log}(t)$ in the right panel. The former is the log-minus-log-survival plot for assessing the proportional hazards assumption (parallel straight lines indicate a Weibull model may be a good fit), and the latter is the log-survival-odds plot for assessing the proportional odds assumption (straight parallel lines together imply a log-logistic AFT model). Chapter 7 from the third edition of the Kleinbaum and Klein book is a good reference on how to pick parametric models based on the information in these plots.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# very weird behaviour when seed is set to 43 here\n# cure fraction ends up being 0 in the treatment group\n# and 60% in the control group. Overall it's correct with value of 0.3\n# this goes away by removing the set.seed(43) call before generating\n# or using any other seed\nset.seed(49)\ncensored_and_cured_samples <- loglogistic_samples %>%\n  mutate(\n    # add cure status and modify time distribution accordingly\n    latent = time,\n    cured = rbinom(n(), 1, 0.3),\n    time = ifelse(cured == 1, 10000, time),\n    # add censoring indicator\n    time3 = pmin(time, 3.0),\n    cens3 = as.numeric(time > 3.0),\n    event3 = 1 - cens3,\n    time6 = pmin(time, 6.0),\n    cens6 = as.numeric(time > 6.0),\n    event6 = 1 - cens6,\n    time12 = pmin(time, 12.0),\n    cens12 = as.numeric(time > 12.0),\n    event12 = 1 - cens12,\n    across(starts_with(\"event\"), as.integer)\n  )\n\ntwelve_weeks_data <- censored_and_cured_samples %>%\n  select(group = parameters, time = time12, event = event12)\n\nthree_weeks_data <- censored_and_cured_samples %>%\n  select(group = parameters, time = time3, event = event3)\n\n# plots for assessing the proportional hazards and proportional odds assumptions\ntwelve_weeks_data %>%\n  survfit2(Surv(time, event, type = \"right\") ~ group, data = .) %>%\n  tidy_survfit(time = seq(0, 12, 0.05), type = \"survival\") %>%\n  select(time, strata, estimate) %>%\n  mutate(\n    lntime = log(time),\n    `Log Minus Log Survival` = log(-log(estimate)),\n    `Log Survival Odds` = log(estimate / (1 - estimate))\n  ) %>%\n  pivot_longer(cols = starts_with(\"Log \")) %>%\n  ggplot(aes(x = lntime, y = value, color = strata, group = interaction(name, strata))) +\n  geom_step() +\n  labs(x = \"Log time\", y = \"\") +\n  scale_color_manual(values = c(\"gray20\", \"forestgreen\")) +\n  coord_cartesian(xlim = c(-0.5, 2.5)) +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  facet_wrap(~name, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nThe next figure shows the estimated curves from the different model overlaid on top of the true curve when analyzing data using the information available at twelve weeks. All the methods produce the same estimates and are nearly indistinguishable from the true curves.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nread_rds(file = \"plot-data-12-weeks.rds\") %>% \n  ggplot(aes(x = time, y = CI_est, color = group)) +\n  geom_line(aes(linetype = type, linewidth = type)) +\n  # add cure fraction line and text\n  geom_hline(yintercept = 0.7, color = \"gray40\", linetype = \"solid\") +\n  annotate(geom = \"text\", x = 0.5, y = 0.76, color = \"gray40\",\n           label = \"Cure fraction of 70% in both groups\", hjust = \"left\") +\n  labs(x = \"Time since product launch (in weeks)\",\n       y = \"Cumulative incidence\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  scale_color_manual(values = c(\"gray20\", \"forestgreen\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dotdash\", \"dashed\", \"dotted\", \"longdash\")) +\n  scale_linewidth_manual(values = c(0.4, 1.1, 1.1, 1.1, 1.1)) +\n  scale_x_continuous(breaks = seq(0, 12, 1)) +\n  scale_y_continuous(labels = scales::label_percent(), breaks = seq(0, 1, 0.2)) +\n  coord_cartesian(xlim = c(0, 12), ylim = c(0, 1)) +\n  guides(\n    color = guide_legend(nrow = 2),\n    linetype = guide_legend(nrow = 3)\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nThe true cumulative incidence curves are scaled from $[0, 1]$ to the interval $[0, 0.7]$ to correspond to a cure fraction of 0.3 using the [usual rescaling formula](https://stats.stackexchange.com/questions/281162/scale-a-number-between-a-range) for mapping $[x_{\\text{min}}, x_{\\text{}max}] \\rightarrow [a,b]$ (with $a = 0$, $b = 0.7$, $x_{\\text{max}} = 1$, $x_{\\text{min}} = 0$, and $x$ the probability $P[T \\le t]$ among the uncured)\n\n$$\nx_{\\text{normalized}} = a + (b - a) \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}\n$$\n\nwhich just leads to multiplying the CDF in each group by 0.7.\n\nDoing the analysis after three weeks shows some interesting results\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_data_three_weeks <- read_rds(file = \"plot-data-3-weeks.rds\")\n\nplot_data_three_weeks %>%\n  filter(time <= 3) %>%\n  ggplot(aes(x = time, y = CI_est, color = group)) +\n  geom_line(aes(linetype = type, linewidth = type)) +\n  labs(x = \"Time since product launch (in weeks)\",\n       y = \"Cumulative incidence\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  scale_color_manual(values = c(\"gray20\", \"forestgreen\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dotdash\", \"dashed\", \"dotted\", \"longdash\")) +\n  scale_linewidth_manual(values = c(0.4, 1.1, 1.1, 1.1, 1.1)) +\n  scale_x_continuous(breaks = seq(0, 3, 1)) +\n  scale_y_continuous(labels = scales::label_percent(), breaks = seq(0, 0.1, 0.02)) +\n  guides(\n    color = guide_legend(nrow = 2),\n    linetype = guide_legend(nrow = 3)\n  ) + \n  coord_cartesian(xlim = c(0, 3), ylim = c(0, 0.1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nAll the curves are close. Facetting by estimator and plotting the pointwise confidence bands for week 2 shows that the mixture cure model runs into some difficulties\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_data_three_weeks %>%\n  filter(time <= 3, type != \"Truth\") %>%\n  ggplot(aes(x = time, y = CI_est)) +\n  # plot the true line on each panel\n  geom_line(\n    data = {\n      plot_data_three_weeks %>%\n        filter(type == \"Truth\", time <= 3) %>%\n        select(-type)\n    },\n    inherit.aes = FALSE,\n    aes(x = time, y = CI_est, group = group, color = group),\n    linetype = \"solid\", linewidth = 1\n  ) +\n  # plot estimated curves and their confidence intervals\n  geom_line(aes(color = group), linetype = \"dashed\", linewidth = 1.1) +\n  geom_ribbon(aes(ymin = CI_lcl, ymax = CI_ucl, \n                  color = group, fill = group), alpha = 0.3) +\n  labs(x = \"Time since product launch (in weeks)\",\n       y = \"Cumulative incidence\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  scale_color_manual(values = c(\"gray20\", \"forestgreen\")) +\n  scale_fill_manual(values = c(\"gray20\", \"forestgreen\")) +\n  scale_x_continuous(breaks = seq(2, 3, 0.2)) +\n  scale_y_continuous(labels = scales::label_percent(), \n                     breaks = seq(0, 0.1, 0.02)) +\n  facet_wrap(~type) +\n  coord_cartesian(xlim = c(2, 3), ylim = c(0, 0.1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nIn this case, based on the lack of plateauing of the KM fit at three weeks, assuming a cure model would not be justifiable, so one of the other three methods used here are probably better for estimating the CIC. At twelve weeks, the KM (or the CIC) curves have plateaued so a cure model might be a decent option, and the parameters of the mixture cure model can be identified.\n\nFinally, I'm going to end this post with an interesting quote I came across in the Hanley and Miettinen paper from the inventor of the highly popular Cox model preferring to use a parametric model over the semi-parametric Cox model\n\n> Particularly notable are Cox’s own reflections (Reid, 1994) on the uses of his model:\\\n> \\\n> **Reid:** How do you feel about the cottage industry that’s grown up around it \\[the Cox model\\]?\n>\n> **Cox:** Don’t know, really. In the light of some of the further results one knows since, I think I would normally want to tackle problems parametrically, so I would take the underlying hazard to be a Weibull or something. I’m not keen on nonparametric formulations usually.\n>\n> **Reid:** So if you had a set of censored survival data today, you might rather fit a parametric model, even though there was a feeling among the medical statisticians that that wasn’t quite right.\n>\n> **Cox:** That’s right, but since then various people have shown that the answers are very insensitive to the parametric formulation of the underlying distribution \\[see, e.g., Cox and Oakes, Analysis of Survival Data, Chapter 8.5\\]. And if you want to do things like predict the outcome for a particular patient, it’s much more convenient to do that parametrically.\n\n## References\n\n-   Bender, R., Augustin, T. and Blettner, M. (2005), Generating survival times to simulate Cox proportional hazards models. Statist. Med., 24: 1713-1723. <https://doi.org/10.1002/sim.2059>\n\n-   Crowther, M.J. and Lambert, P.C. (2013), Simulating biologically plausible complex survival data. Statist. Med., 32: 4118-4134. <https://doi.org/10.1002/sim.5823>\n\n-   Al-Shomrani, A.A., Shawky, A.I., Arif, O.H. *et al.* Log-logistic distribution for survival data analysis using MCMC. *SpringerPlus* **5**, 1774 (2016). <https://doi.org/10.1186/s40064-016-3476-7>\n\n-   Lambert, P. C. (2007). Modeling of the Cure Fraction in Survival Studies. *The Stata Journal*, *7*(3), 351-375. <https://doi.org/10.1177/1536867X0700700304>\n\n-   Amico, M. and Van Keilegom, I. (2018). Cure Models in Survival Analysis. *Annual Review of Statistics and Its Application,* Vol. 5:311-342.\\\n    <https://doi.org/10.1146/annurev-statistics-031017-100101>\n\n-   Legrand, C. and Bertrand, A. (2019). Cure Models in Oncology Clinical Trials. (Seems to be a book chapter from [this book](https://www.taylorfrancis.com/chapters/edit/10.1201/9781315112084-22/cure-models-cancer-clinical-trials-catherine-legrand-aur%C3%A9lie-bertrand). Non-paywalled link [here](https://dial.uclouvain.be/pr/boreal/object/boreal%3A220045/datastream/PDF_02/view).)\n\n-   Kleinbaum, D.G. and Klein, M. (2012) Survival Analysis: A Self-Learning Text. 3rd Edition, Springer, New York.[https://doi.org/10.1007/978-1-4419-6646-9](#0)\n\n-   Hanley, J. & Miettinen, O. (2009). Fitting Smooth-in-Time Prognostic Risk Functions via Logistic Regression. *The International Journal of Biostatistics*, *5*(1). <https://doi.org/10.2202/1557-4679.1125>\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}