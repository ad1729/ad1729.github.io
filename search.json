[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My professional interests revolve around statistics / machine learning, programming, distributed computing, writing clean code, and using models to make real-world decisions.\nIn my free time, I like to read, cook, cycle, play tennis, and learn languages.\nThe primary reason for starting this (mostly) technical blog is to work through some (technical) concepts or challenges I encounter. Additional reasons include having my R / Python code in a clean and easy-to-navigate format, and to share recipes once in a while. I’ve been talking about wanting to have a blog since 2012, so I’m glad that I finally got around to having one in 2022.\nYou can reach out to me via LinkedIn or via email (first name [dot] last name [at] gmail)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Akshat Dwivedi",
    "section": "",
    "text": "R\n\n\nGLM\n\n\nMulticollinearity\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nGLM\n\n\nGEE\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMiscellaneous\n\n\nMeasurement\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\nSimulation\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMiscellaneous\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html",
    "href": "posts/gee-on-aggregated-data/index.html",
    "title": "GEE on aggregated data?",
    "section": "",
    "text": "TL;DR: The geepack R package doesn’t do what it doesn’t claim to do"
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html#model",
    "href": "posts/gee-on-aggregated-data/index.html#model",
    "title": "GEE on aggregated data?",
    "section": "Model",
    "text": "Model\nThe code below simulates data from the following mixed-effects model2 with random-intercept terms where person \\(j\\) is nested within cluster \\(i\\). Each cluster has a different proportion of treatment uptake \\(p_{treated,j}\\) varying between 40-60% where \\(X_{ij}\\) indicates whether person \\(j\\) in cluster \\(i\\) received the treatment or not.2 The model syntax is inspired by the statistical rethinking book\n\\[\n\\begin{gather}\nY_{ij} \\sim \\text{Bernoulli}(p_{ij}) \\\\\n\\text{logit}(p_{ij}) = -1.4 + 0.3 \\times X_{ij} + \\alpha_i \\\\\n\\alpha_i \\sim \\text{Normal}(0, 1) \\\\\nX_{ij} \\sim \\text{Bernoulli}(p_{treated,i}) \\\\\np_{treated,i} \\sim \\text{Uniform(0.4, 0.6)}\n\\end{gather}\n\\]\nThe time taken to train the model on the raw as well as the aggregate data, the point estimates, and standard errors for the coefficients are stored for analysis."
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html#code",
    "href": "posts/gee-on-aggregated-data/index.html#code",
    "title": "GEE on aggregated data?",
    "section": "Code",
    "text": "Code\n\nsimulate_data_and_get_estimates <- function(n, n_clus, seed) {\n  set.seed(seed)\n  # cluster random intercept\n  rand_intercept <- rnorm(n_clus, mean = 0, sd = 1)\n\n  set.seed(seed)\n  sim_data <- map_dfr(.x = 1:n_clus, .f = ~ {\n    tibble(\n      id = .x,\n      x = rbinom(round(n / n_clus), size = 1, prob = runif(1, 0.4, 0.6)),\n      y = rbinom(round(n / n_clus), size = 1, prob = plogis(-1.4 + 0.3 * x + rand_intercept[[.x]]))\n    )\n  })\n\n  # fit model to raw data\n  t0 <- Sys.time()\n  mod1_sim <- geeglm(\n    y ~ x,\n    data = sim_data,\n    family = \"binomial\", id = id, corstr = \"exchangeable\"\n  )\n  t1 <- Sys.time()\n  t1 <- as.numeric(difftime(t1, t0, units = \"secs\"))\n\n  # fit GLM model to raw data\n  mod1_sim_glm <- glm(y ~ factor(id) + x - 1,\n                      data = sim_data, family = \"binomial\")\n\n  # fit model to aggregated data\n  agg_data <- sim_data %>%\n    group_by(id, x) %>%\n    summarize(n = n(), y = sum(y), .groups = \"drop\") %>%\n    mutate(y_prob = y / n)\n\n  t2 <- Sys.time()\n  mod2_sim <- geeglm(\n    y_prob ~ x,\n    data = agg_data, weights = n,\n    family = \"binomial\", id = id, corstr = \"exchangeable\"\n  )\n  t3 <- Sys.time()\n  t3 <- as.numeric(difftime(t3, t2, units = \"secs\"))\n\n  # fit GLM model to aggregated data\n  mod2_sim_glm <- glm(y_prob ~ factor(id) + x - 1, data = agg_data,\n                      weights = n, family = \"binomial\")\n\n  results <- bind_rows(\n    # GEE results\n    broom::tidy(mod1_sim) %>%\n      mutate(data = 'raw data', time_secs = t1, estimator = \"GEE\"),\n    broom::tidy(mod2_sim) %>%\n      mutate(data = 'aggregated data', time_secs = t3, estimator = \"GEE\"),\n    # GLM results\n    broom::tidy(mod1_sim_glm) %>%\n      mutate(data = 'raw data', time_secs = NA_real_, estimator = \"GLM\"),\n    broom::tidy(mod2_sim_glm) %>%\n      mutate(data = 'aggregated data', time_secs = NA_real_, estimator = \"GLM\")\n  ) %>%\n    mutate(n = n, n_clus = n_clus, seed = seed) %>%\n    filter(!stringr::str_detect(term, pattern = \"id\"))\n\n  return(results)\n}\n\nHere’s what the output from this function looks like:\n\nsimulate_data_and_get_estimates(n = 100, n_clus = 5, seed = 3)\n\n# A tibble: 6 × 11\n  term       estim…¹ std.e…² stati…³ p.value data  time_s…⁴ estim…⁵     n n_clus\n  <chr>        <dbl>   <dbl>   <dbl>   <dbl> <chr>    <dbl> <chr>   <dbl>  <dbl>\n1 (Intercep…  -1.59    0.409 15.0    1.07e-4 raw …  0.00748 GEE       100      5\n2 x           -0.144   0.383  0.141  7.07e-1 raw …  0.00748 GEE       100      5\n3 (Intercep…  -1.73    0.395 19.1    1.27e-5 aggr…  0.00326 GEE       100      5\n4 x           -0.107   0.402  0.0702 7.91e-1 aggr…  0.00326 GEE       100      5\n5 x           -0.266   0.596 -0.446  6.55e-1 raw … NA       GLM       100      5\n6 x           -0.266   0.596 -0.446  6.55e-1 aggr… NA       GLM       100      5\n# … with 1 more variable: seed <dbl>, and abbreviated variable names ¹​estimate,\n#   ²​std.error, ³​statistic, ⁴​time_secs, ⁵​estimator\n\n\n\nsimulation_parameters <- expand_grid(\n  n = c(100, 500, 1000, 2500, 5000, 7500, 10000),\n  n_clus = c(5, 50, 250),\n  seed = 1:20\n) %>% \n  # remove runs where number of clusters is larger than total sample size\n  filter(n_clus < n)\n\n# set up multicore processing\nplan(multisession, workers = 15)\n\n# parallelize purrr::pmap_dfr by using the {furrr} package\nsimulation_results <- future_pmap_dfr(\n  .l = simulation_parameters,\n  # have to pass args as ..1 or ..2, else it fails\n  .f = ~ simulate_data_and_get_estimates(n = ..1, n_clus = ..2, seed = ..3),\n  .options = furrr_options(seed = NULL)\n)\n\nplan(sequential) # turn off multicore\n\nThis chunk can take a while to run for some of the parameter combinations, so pre-computed results are loaded and analyzed further.\n\nsimulation_results <- read_csv(file = \"gee_glm_sim.csv\", show_col_types = FALSE) %>% \n  mutate(\n    n_clus_raw = n_clus,\n    n_clus = factor(paste0(\"# clusters: \", n_clus),\n                    levels = paste0(\"# clusters: \", c(5, 50, 250))),\n    term = case_when(\n      term == \"(Intercept)\" ~ \"Intercept\",\n      term == \"x\" ~ \"Slope\", \n      TRUE ~ term\n  ))\n\nglimpse(simulation_results)\n\nRows: 2,400\nColumns: 12\n$ term       <chr> \"Intercept\", \"Slope\", \"Intercept\", \"Slope\", \"Slope\", \"Slope…\n$ estimate   <dbl> -7.17e-01, -7.38e-02, -5.35e-01, -1.14e-01, -5.92e-02, -5.9…\n$ std.error  <dbl> 4.69e-01, 1.75e-01, 4.09e-01, 1.53e-01, 4.86e-01, 4.86e-01,…\n$ statistic  <dbl> 2.3402, 0.1772, 1.7056, 0.5518, -0.1216, -0.1216, 7.1290, 4…\n$ p.value    <dbl> 1.26e-01, 6.74e-01, 1.92e-01, 4.58e-01, 9.03e-01, 9.03e-01,…\n$ data       <chr> \"raw data\", \"raw data\", \"aggregated data\", \"aggregated data…\n$ time_secs  <dbl> 0.01389, 0.01389, 0.00420, 0.00420, NA, NA, 0.00928, 0.0092…\n$ estimator  <chr> \"GEE\", \"GEE\", \"GEE\", \"GEE\", \"GLM\", \"GLM\", \"GEE\", \"GEE\", \"GE…\n$ n          <dbl> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,…\n$ n_clus     <fct> # clusters: 5, # clusters: 5, # clusters: 5, # clusters: 5,…\n$ seed       <dbl> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4,…\n$ n_clus_raw <dbl> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…"
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html#computation-time",
    "href": "posts/gee-on-aggregated-data/index.html#computation-time",
    "title": "GEE on aggregated data?",
    "section": "Computation time",
    "text": "Computation time\nFirst, the time taken to fit the model to the raw dataset is shown as a function of total sample size \\(n\\) where there are \\(n / n_{clus}\\) observations per cluster. Each panel has a different y-axis to allow reading values directly from the plot.\n\nsimulation_results %>%\n  filter(data == \"raw data\", estimator == \"GEE\") %>%\n  # distinct here since the data frame contains a row for\n  # each of the intercept and slope terms but the runtimes are the same\n  # for both these terms\n  distinct(seed, time_secs, n, n_clus) %>%\n  ggplot(aes(x = n, y = time_secs, group = n_clus)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, degree = 3)) +\n  geom_smooth(method = \"lm\", formula = y ~ I(x ^ 3), color = \"forestgreen\") +\n  stat_summary(aes(group = interaction(n_clus, n)), geom = \"point\", \n               fun = mean, color = \"darkorange\", size = 2) + \n  xlab(\"Total sample size (n)\") +\n  ylab(\"Runtime (in seconds)\") +\n  facet_wrap(~ n_clus, scales = \"free\")\n\n\n\n\nThe blue line fits a cubic polynomial \\[time = \\beta_0 + \\beta_1 n + \\beta_2 n^2 + \\beta_3 n^3 + \\varepsilon\\] to the run times as a function of sample size. The green line fits the same model but without the linear and quadratic terms, i.e, \\(time = \\alpha_0 + \\alpha_1 n^3 + \\varepsilon\\). The yellow points are the mean run times for each level of \\(n\\) within each panel.\nWhen the number of clusters is very small, an increase in the sample size within the largest cluster leads to a cubic increase in expected run times. The closeness of the blue and green lines in the leftmost panel indicates that the cubic term in the execution time model dominates the lower order terms. In the second and third panels, the \\(O(n^3)\\) limit hasn’t hit yet, as the green and the blue lines are in disagreement.\nFor 10000 samples split into 2000 samples in each of the five clusters, it takes approximately 15 minutes per run. The same total sample size split into 250 samples in each of the 40 clusters, each model takes about 7 seconds to run, and even less time when there are 250 clusters. This is a massive difference in time between the different settings."
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html#convergence-issues",
    "href": "posts/gee-on-aggregated-data/index.html#convergence-issues",
    "title": "GEE on aggregated data?",
    "section": "Convergence issues",
    "text": "Convergence issues\nHowever, unfortunately some of the models run into convergence issues, where the point estimates and the standard errors explode\n\nsimulation_results %>% \n  filter(estimator == \"GEE\") %>% \n  arrange(desc(std.error)) %>% \n  head(n = 10)\n\n# A tibble: 10 × 12\n   term      estimate std.e…¹ stati…² p.value data  time_…³ estim…⁴     n n_clus\n   <chr>        <dbl>   <dbl>   <dbl>   <dbl> <chr>   <dbl> <chr>   <dbl> <fct> \n 1 Intercept  4.24e16 1.22e16  12.1   5.12e-4 aggr… 0.00516 GEE     10000 # clu…\n 2 Intercept  1.13e17 8.91e15 161.    0       aggr… 0.00621 GEE      7500 # clu…\n 3 Intercept -1.99e17 7.88e15 641.    0       aggr… 0.0175  GEE      1000 # clu…\n 4 Intercept  2.32e15 7.25e15   0.102 7.49e-1 aggr… 0.00667 GEE      7500 # clu…\n 5 Intercept  5.32e15 6.59e15   0.650 4.20e-1 aggr… 0.00582 GEE       100 # clu…\n 6 Slope     -1.69e15 2.25e15   0.562 4.53e-1 aggr… 0.0110  GEE      2500 # clu…\n 7 Intercept  9.69e14 2.12e15   0.210 6.47e-1 aggr… 0.00558 GEE     10000 # clu…\n 8 Slope     -1.03e15 2.09e15   0.241 6.23e-1 aggr… 0.00763 GEE      7500 # clu…\n 9 Slope     -1.94e15 2.05e15   0.896 3.44e-1 aggr… 0.00503 GEE     10000 # clu…\n10 Slope      3.82e15 1.88e15   4.13  4.23e-2 aggr… 0.00817 GEE      5000 # clu…\n# … with 2 more variables: seed <dbl>, n_clus_raw <dbl>, and abbreviated\n#   variable names ¹​std.error, ²​statistic, ³​time_secs, ⁴​estimator\n\n\nThis seems to be a problem when training GEE models on aggregated data when the number of clusters is low irrespective of the total sample size, which leads to about 21% of the estimates from the aggregated data being convergence failures. An estimate is flagged as convergence failure when the stdandard error of the coefficients > 5 on the logit scale.\n\nsimulation_results <- simulation_results %>%\n  mutate(high_se = as.numeric(std.error > 5)) \n\nsimulation_results %>%\n  filter(estimator == \"GEE\") %>% \n  select(-estimate, -std.error, -statistic, -p.value, -time_secs) %>%\n  # put intercept and slope estimates in one row\n  pivot_wider(everything(), names_from = term, values_from = high_se) %>%\n  # if at least one of the intercept or slope terms have a very high se\n  mutate(high_se = pmin(Intercept + Slope, 1)) %>%\n  group_by(data, n_clus, n) %>%\n  summarize(n_total = n(), n_failed = sum(high_se), .groups = \"drop_last\") %>%\n  #print(n = Inf) %>%\n  summarize(n_total = sum(n_total), n_failed = sum(n_failed), .groups = \"drop\") %>%\n  mutate(percent_failed = 100 * n_failed / n_total)\n\n# A tibble: 6 × 5\n  data            n_clus          n_total n_failed percent_failed\n  <chr>           <fct>             <int>    <dbl>          <dbl>\n1 aggregated data # clusters: 5       140       29           20.7\n2 aggregated data # clusters: 50      140        0            0  \n3 aggregated data # clusters: 250     120        0            0  \n4 raw data        # clusters: 5       140        0            0  \n5 raw data        # clusters: 50      140        0            0  \n6 raw data        # clusters: 250     120        0            0"
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html#raw-vs-aggregated-data-estimates",
    "href": "posts/gee-on-aggregated-data/index.html#raw-vs-aggregated-data-estimates",
    "title": "GEE on aggregated data?",
    "section": "Raw vs aggregated data estimates",
    "text": "Raw vs aggregated data estimates\nThese rows with convergence failures can be removed and the agreement between the estimates from the raw vs the aggregate datasets can be assessed visually\n\nslope_plot_data <- simulation_results %>%\n  filter(term == \"Slope\", high_se == 0) %>%\n  select(seed, term, data, estimate, n_clus, n, estimator) %>%\n  tidyr::pivot_wider(id_cols = c(seed, term, n_clus, n, estimator),\n                     names_from = data, values_from = estimate) %>%\n  mutate(`Total sample size` = factor(n), \n         id = interaction(estimator, n_clus, sep = \", \", lex.order = TRUE)) \n\nslope_plot_data %>% \n  filter(estimator == \"GEE\") %>% \n  ggplot(aes(x = `raw data`, y = `aggregated data`, color = `Total sample size`)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  # geom_point(data = tibble(x = 0.3, y = 0.3), \n  #            aes(x = x, y = y), \n  #            color = \"black\", size = 3, inherit.aes = FALSE) +\n  xlab(\"Slope coefficient from the full dataset\") +\n  ylab(\"Slope coefficient from the aggregated dataset\") +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 1)) + \n  facet_wrap(~ id, ncol = 3, scales = \"fixed\")\n\nWarning: Removed 29 rows containing missing values (geom_point).\n\n\n\n\n\nThis plot compares the estimated slope coefficient (true value 0.3) from the GEE model on aggregated data vs the estimated slope coefficient from the model on the raw data while varying the number of clusters as well as the total sample size. The agreement between estimates increases as the number of clusters increases (going from the top left panel to the top right panel).\nOn the other hand, the estimates from running GLMs on the same aggregated and raw datasets are identical, as indicated by the points lying precisely on the black line in the following plot\n\nslope_plot_data %>% \n  filter(estimator == \"GLM\") %>% \n  ggplot(aes(x = `raw data`, y = `aggregated data`, color = `Total sample size`)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  # geom_point(data = tibble(x = 0.3, y = 0.3), \n  #            aes(x = x, y = y), \n  #            color = \"black\", size = 3, inherit.aes = FALSE) +\n  xlab(\"Slope coefficient from the full dataset\") +\n  ylab(\"Slope coefficient from the aggregated dataset\") +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 1)) + \n  facet_wrap(~ id, ncol = 3, scales = \"fixed\")\n\n\n\n\nAs expected, the estimates for \\(n = 10,000\\) are tightly clustered around the true value compared to \\(n = 100\\) in both sets of plots.\nOk so the estimates aren’t identical for the GEE models unfortunately, but are they approximately similar across the raw and aggregated datasets? This is assessed visually via box plots below for the case of 5 clusters in the data\n\ndist_plot_data <- simulation_results %>% \n  mutate(\n    ci_lower = estimate - (qnorm(0.975) * std.error),\n    ci_upper = estimate + (qnorm(0.975) * std.error), \n    ci_width = ci_upper - ci_lower, \n    ci_upper_half_width = ci_upper - estimate\n  ) %>% \n  filter(term == \"Slope\", high_se == 0, estimator == \"GEE\") %>% \n  select(\n    Estimate = estimate, \n    `Std. Error` = std.error, \n    `95% CI LL` = ci_lower, \n    `95% CI UL` = ci_upper,\n    `95% CI Width` = ci_width,\n    `95% CI Upper HW` = ci_upper_half_width,\n    n, n_clus_raw, n_clus, data, seed) %>% \n  tidyr::pivot_longer(cols = Estimate:`95% CI Upper HW`, \n                      names_to = \"statistic\", \n                      values_to = \"values\") %>% \n  mutate(\n    statistic = factor(statistic,\n                       levels = c(\"Estimate\", \"Std. Error\", \n                                  \"95% CI LL\", \"95% CI UL\", \n                                  \"95% CI Width\", \"95% CI Upper HW\")), \n    n = factor(n, ordered = TRUE), \n    data = stringr::str_to_title(data)\n  )\n\nglimpse(dist_plot_data)\n\nRows: 4,626\nColumns: 7\n$ n          <ord> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,…\n$ n_clus_raw <dbl> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ n_clus     <fct> # clusters: 5, # clusters: 5, # clusters: 5, # clusters: 5,…\n$ data       <chr> \"Raw Data\", \"Raw Data\", \"Raw Data\", \"Raw Data\", \"Raw Data\",…\n$ seed       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3,…\n$ statistic  <fct> Estimate, Std. Error, 95% CI LL, 95% CI UL, 95% CI Width, 9…\n$ values     <dbl> -0.0738, 0.1753, -0.4174, 0.2698, 0.6871, 0.3436, -0.1137, …\n\n\n\ndist_plot_data %>% \n  filter(n_clus_raw == 5) %>% \n  ggplot(aes(x = n, y = values, color = data)) + \n  #geom_point(position = position_dodge(width = 0.2)) +\n  geom_boxplot(position = position_dodge(width = 1)) +\n  xlab(\"Total sample size (n)\") + \n  ylab(\"\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  facet_wrap(~ statistic, ncol = 2, scales = \"free\")\n\n\n\n\nSo based on these box plots, it seems like most of the sampling distributions for these statistics – point estimate, standard errors, CI width, etc. – are pretty comparable but not identical."
  },
  {
    "objectID": "posts/hello-world/index.html",
    "href": "posts/hello-world/index.html",
    "title": "Hello, World!",
    "section": "",
    "text": "This first post is to check whether the features I want for this blog work as desired.\nThese can be summarized in a non-exhaustive list as:"
  },
  {
    "objectID": "posts/hello-world/index.html#math",
    "href": "posts/hello-world/index.html#math",
    "title": "Hello, World!",
    "section": "Math",
    "text": "Math\nThe OLS estimator is given by the equation \\(\\hat\\beta_\\text{OLS} = (X^\\mathsf{T} X)^{-1} X^\\mathsf{T} y\\).\nOn the other hand, the ridge estimator is given by the following formula\n\\[\\hat\\beta_\\text{ridge} = (X^\\mathsf{T} X + \\lambda I)^{-1} X^\\mathsf{T} y\\]\nwhere \\(\\lambda \\in [0, \\infty)\\) controls the amount of shrinkage applied to the coefficients."
  },
  {
    "objectID": "posts/hello-world/index.html#r-code",
    "href": "posts/hello-world/index.html#r-code",
    "title": "Hello, World!",
    "section": "R code",
    "text": "R code\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\np <- iris %>% \n  ggplot(aes(x = Petal.Length, y = Petal.Width, color = Species)) + \n  geom_point() + \n  theme_classic()\n\nvanilla ggplot\n\nplot(p)\n\n\n\n\nplotly plot\n\nplotly::ggplotly(p)"
  },
  {
    "objectID": "posts/how-many-ratings-do-i-need/index.html",
    "href": "posts/how-many-ratings-do-i-need/index.html",
    "title": "How Many Five-Star Ratings Do I Need?",
    "section": "",
    "text": "The screenshot indicates an average1 rating of 4.5 stars out of a total of \\(n = 363\\) reviews, and additionally lists the percent of time a rating between 1-5 was given.1 Unweighted, I assume.\nIt seemed like an easy enough problem – perfect to explore on a rainy, gray Saturday in November – so I decided to have a crack at it.\nAfter doing some trivial algebra, somewhat successfully writing a for-loop, and adequately pleased with the solution, I posted the write-up for my initial approach on RPubs and sent off the answer2 to my friend.2 Spoiler: she needed between 47-50 5-star ratings to pull up the average rating to 4.6. Apologies if you were eagerly waiting for the end to find out what the answer was.\nHowever, while going through this document to clean it up for the blog (in 2022), I realized there’s a simpler way of solving this problem. Before I describe it further, I’m going to load some R packages and extract the data from the image into R objects.\n\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nn <- 363\n\n# percent ratings scaled to [0,1]\np <- c(0.06, 0.01, 0.02, 0.12, 0.79) %>%\n  set_names(nm = 1:5) %>% \n  print()\n\n   1    2    3    4    5 \n0.06 0.01 0.02 0.12 0.79 \n\n\nWhat made me rethink my approach was that I previously ended up with the wrong ratings vector after converting the (rounded) percentages into counts3.3 Although I mostly worked around it afterwards via simulation.\nSo for example, 79% out of 363 ratings were 5-star ratings, which translates to 286.77 and rounded to the nearest integer becomes 287. But this isn’t the only integer that rounds to 79% when divided by 363. Any integer \\(k\\) when divided by 363 that ends up in the (open) interval (78.5%, 79.5%) would be a possible candidate.\n\nseq(284, 289, 1) %>% \n  set_names(nm = ~ .x) %>% \n  map_dbl(.f = ~ round(100 * .x / 363))\n\n284 285 286 287 288 289 \n 78  79  79  79  79  80 \n\n\nSo any one of 285-288 5-star ratings are compatible with the information in the screenshot. We can get the same range for the other ratings (i.e., 1-4).\n\nplausible_counts_per_rating <- p %>% map(.f = function(prop) {\n  approx_val <- round(prop * n)\n  possible_vals <- seq(from = approx_val - 10, to = approx_val + 10, by = 1) %>% \n    set_names(nm = ~ .x) %>% \n    map_dbl(.f = ~ round(.x / n, 2)) %>% \n    keep(.p = ~ .x == prop) %>% \n    names()\n}) %>% \n  as_tibble(.name_repair = ~ paste('x', .x, sep = \"\"))\n\nplausible_counts_per_rating\n\n# A tibble: 4 × 5\n  x1    x2    x3    x4    x5   \n  <chr> <chr> <chr> <chr> <chr>\n1 20    2     6     42    285  \n2 21    3     7     43    286  \n3 22    4     8     44    287  \n4 23    5     9     45    288  \n\n\nEach column shows the plausible values for the number of times a rating was provided. We can create all possible combinations of these values to identify the subset of \\(4 ^ 5 = 1024\\) possible combinations that are compatible with the information in the screenshot, i.e., a mean of 4.5 when rounded to 1 decimal place and a total of 363 ratings.\n\nrating_combinations <- plausible_counts_per_rating %>%\n  # create all combinations of all values in all columns\n  expand(crossing(x1, x2, x3, x4, x5)) %>%\n  mutate(across(.fns = as.integer), \n         total_ratings = x1 + x2 + x3 + x4 + x5, \n         sum_ratings = x1 + (2 * x2) + (3 * x3) + (4 * x4) + (5 * x5), \n         mean_rating = round(sum_ratings / 363, 1)) %>% \n  print(n = 10)\n\n# A tibble: 1,024 × 8\n      x1    x2    x3    x4    x5 total_ratings sum_ratings mean_rating\n   <int> <int> <int> <int> <int>         <int>       <dbl>       <dbl>\n 1    20     2     6    42   285           355        1635         4.5\n 2    20     2     6    42   286           356        1640         4.5\n 3    20     2     6    42   287           357        1645         4.5\n 4    20     2     6    42   288           358        1650         4.5\n 5    20     2     6    43   285           356        1639         4.5\n 6    20     2     6    43   286           357        1644         4.5\n 7    20     2     6    43   287           358        1649         4.5\n 8    20     2     6    43   288           359        1654         4.6\n 9    20     2     6    44   285           357        1643         4.5\n10    20     2     6    44   286           358        1648         4.5\n# … with 1,014 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\npossible_ratings <- rating_combinations %>% \n  filter(total_ratings == 363, mean_rating == 4.5) %>% \n  print(n = Inf)\n\n# A tibble: 3 × 8\n     x1    x2    x3    x4    x5 total_ratings sum_ratings mean_rating\n  <int> <int> <int> <int> <int>         <int>       <dbl>       <dbl>\n1    23     4     9    42   285           363        1651         4.5\n2    23     5     7    43   285           363        1651         4.5\n3    23     5     8    42   285           363        1650         4.5\n\n\nSo one of these three possible vectors is used to produce the statistics shown in the screenshot.\nThe formula for computing the (arithmetic) mean can be rearranged to easily calculate the required number of five-star ratings to bring the mean from 4.5 to 4.6.\nLet \\(n_1\\) denote the number of additional five-star ratings, \\(y\\) the (weighted) sum of the rating counts, and \\(n\\) the current number of ratings (i.e., 363) in the following equation:\n\\[\\frac{y + (n_1 \\times 5)}{n + n_1} = 4.6\\]\nThis can be rewritten as\n\\[5n_1 = 4.6 \\times (n + n_1) - y\\]\nand simplified to yield\n\\[n_1 = \\frac{(4.6 \\times n) - y}{0.4}\\]\nand coded up as an R function\n\nnum_five_star <- function(sum_ratings, total_ratings) {\n  ((4.6 * total_ratings) - sum_ratings) / 0.4\n}\n\nApplying this function to the possible ratings vector leads to\n\npossible_ratings %>% \n  mutate(extra_5_stars = ceiling(num_five_star(sum_ratings, total_ratings))) %>% \n  select(-total_ratings, -mean_rating)\n\n# A tibble: 3 × 7\n     x1    x2    x3    x4    x5 sum_ratings extra_5_stars\n  <int> <int> <int> <int> <int>       <dbl>         <dbl>\n1    23     4     9    42   285        1651            47\n2    23     5     7    43   285        1651            47\n3    23     5     8    42   285        1650            50\n\n\nso 47-50 additional 5-star ratings are needed to pull up the average4 to 4.6, assuming that the future ratings are all five-star ratings.4 exact average, not an average resulting from rounding 4.57 (say) to 4.6. In the latter case, fewer than 47 five-star ratings would be required."
  },
  {
    "objectID": "posts/jigger-volume-estimation/index.html",
    "href": "posts/jigger-volume-estimation/index.html",
    "title": "Determining the volume of my (cocktail) jigger",
    "section": "",
    "text": "In the fall of 2022, I was gifted a cocktail mixing set for my birthday1 which included a cocktail jigger. A jigger is an essential tool for mixing cocktails as it makes it relatively easy to measure the quantity of drinks that go into a cocktail (e.g., 40 ml of vodka, 10 ml of sugar syrup, etc.).1 It was put to use immediately and some delicious whiskey sours were made.\nAfter a couple of months of not having to use my cocktail set due to travelling, I couldn’t remember the volume of either side (i.e., basin) of the jigger, nor could I find the remains of the packaging material for reference. Googling showed that jiggers come in different sizes, so I decided to measure the volume myself."
  },
  {
    "objectID": "posts/jigger-volume-estimation/index.html#method-1-weight-based",
    "href": "posts/jigger-volume-estimation/index.html#method-1-weight-based",
    "title": "Determining the volume of my (cocktail) jigger",
    "section": "Method 1: Weight-based",
    "text": "Method 1: Weight-based\nThe digital kitchen scale I own has this nifty feature where it can account for and automatically subtract the weight of the container so that only the weight of the object of interest is shown.\nThe scale measures weight in grams (g), but I was interested in the volume in milliliters (ml). Quick Googling reminded me of the nifty fact I learned and forgot a long time ago, that the weight of 1 g of water is approximately equal to the volume occupied by 1 ml of water.\nPlacing the jigger on the scale and ensuring that the scale read zero g, I filled up the larger basin to the top with tap water. The scale showed 51g, which was odd as I was expecting it to show a number that would be a multiple of five (i.e., a number ending with a 0 or a 5)3.3 although some jigger images online show possible measurements like 22.5 ml (which is 0.75 oz)\nSo obviously4 the next logical step was to take multiple measurements. After making sure to drain the jigger, clean the scale of any spilled water droplets, and ensuring the scale read zero, I took the next measurement which read 48 g. Excellent. I love consistent results.4 obvious to a Statistician or a Data Scientist\nSo I took another 8 measurements, and repeated this process for the smaller basin of the jigger as well. These measurements are visualized below.\nEach time I filled it to what I perceived to be the top but I got different measurements. I don’t think that this variability is necessarily due to the variability of the digital scale itself but most likely due to the variability in my perception of what counts as ‘filled to the brim’.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\nvol_big <- c(51, 48, 48, 52, 49, 50, 51, 52, 50, 50)\nvol_small <- c(31, 28, 32, 30, 30, 29, 32, 32, 31, 31)\n\ntibble(\n  Basin = c(rep(\"Smaller basin\", 10), rep(\"Larger basin\", 10)),\n  `Volume (in ml)` = c(vol_small, vol_big)\n) %>%\n  mutate(Basin = forcats::fct_rev(Basin)) %>% \n  ggplot(aes(x = `Volume (in ml)`, fill = Basin, color = Basin)) +\n  geom_dotplot(binwidth = 1, stroke = 2, dotsize = 0.7) +\n  theme_classic() +\n  theme(legend.position = \"none\", \n        axis.title.x = element_text(size = 16), \n        axis.text.x = element_text(size = 16), \n        strip.text = element_text(size = 16)) + \n  scale_y_continuous(NULL, breaks = NULL) + \n  scale_fill_manual(values = c(\"#E69F00\", \"darkgreen\")) + \n  scale_color_manual(values = c(\"#E69F00\", \"darkgreen\")) + \n  facet_wrap(~ Basin, scales = 'free')\n\n\n\n\n\nAveraging these measurements gave me a volume of 50.1 ml for the big basin and 30.6 ml for the smaller basin.\nInterestingly5, it also indicates6 that I usually put anywhere between 48-52 (or 28-32) ml of a drink when I’m supposed to add 50 (or 30) ml.5 to me and literally nobody else6 assuming the digital scale is not causing this variability"
  },
  {
    "objectID": "posts/jigger-volume-estimation/index.html#method-2-dimension-based",
    "href": "posts/jigger-volume-estimation/index.html#method-2-dimension-based",
    "title": "Determining the volume of my (cocktail) jigger",
    "section": "Method 2: Dimension-based",
    "text": "Method 2: Dimension-based\nThe shape of a jigger reminded me of a (partial) cone, so I measured7 the height \\(h\\) from the mouth of each basin to the point where it joins the other basin, and the diameter \\(2R\\) of the mouth of each basin.7 using a pair of digital calipers I totally had lying around the house and did not use this exercise as an excuse to buy\nFor the smaller basin, the height \\(h_s\\) was measured as 3.48 cm, and a diameter of 3.9 cm (so a radius \\(R_s\\) of 1.95 cm).\nFor the larger basin, the height \\(h_l\\) was measured as 5.28 cm, and a diameter of 4.18 cm (so a radius \\(R_l\\) of 2.09 cm).\nThe radius \\(r\\) of the base where the two basins are attached to each other is \\(2.78 / 2 = 1.39\\).\nPlugging these into the formula for a partial cone88 here’s a derivation (and another) of the volume of a cone using calculus, which I’ve probably derived in school or university and long since forgotten\n\\[V = \\frac{1}{3} \\times \\pi \\times h \\times \\Big(R^2 + Rr + r^2\\Big)\\]\n\n\nCode\ncone <- function(h, r, R) {\n  (pi * h * (R^2 + (R * r) + r^2)) / 3\n}\n\n\ngave a volume9 of 30.8 ml for the smaller basin and 50.9 for the larger basin which was very similar to the numbers from the other method, but not exactly equal to 30 and 50 ml due to (human) measurement error.9 using the conversion factor of 1 cm3 to 1 ml for volume of water\nA few days after doing all these measurements, I came across a similar (in spirit?) post on a blog I frequent, which inspired me to write this up."
  },
  {
    "objectID": "posts/one-reason-why-a-glm-coefficient-is-NA/index.html",
    "href": "posts/one-reason-why-a-glm-coefficient-is-NA/index.html",
    "title": "One reason why a (g)lm coefficient is NA",
    "section": "",
    "text": "Recently while fitting a logistic regression model, some of the coefficients estimated by the model were NA. Initially I thought it was due to separation1, as that’s the most common issue I usually face when fitting unregularized models on data.1 see this Wikipedia article, or this stats.stackexchange.com thread (and the associated links in the sidebar)\nHowever, googling2 threw up many threads on multicollinearity and anyway, separation usually leads to nonsensical estimates like \\(1.5 \\times 10^8\\) instead of NA.2 in this day and age of ChatGPT, I know\nAfter combing through many stackexchange threads, I discovered the alias function in R from this thread, which was pretty handy at identifying the problematic column(s).\nIt’s interesting that the alias documentation doesn’t mention anything about GLMs (glm()) but this does work on glm(..., family = \"binomial\") model objects3.3 possibly since the class of a glm object is c(\"glm\", \"lm\")\nThe rest of this post explores this issue and its resolution using aggregated test data, where the city variable is intentionally nested within the country variable.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nsimulated_data <- tribble(\n  ~age, ~city, ~country, ~y, ~N,\n  \"< 30\", \"Paris\", \"France\", 30, 100,\n  \"< 30\", \"Nice\", \"France\", 20, 100,\n  \"< 30\", \"Berlin\", \"Germany\", 23, 100,\n  \"30+\", \"Paris\", \"France\", 12, 100,\n  \"30+\", \"Nice\", \"France\", 11, 100,\n  \"30+\", \"Berlin\", \"Germany\", 27, 100\n) %>% \n  mutate(y = y / N)\n\nmodel <- glm(y ~ age + city + country, weights = N, data = simulated_data, family = \"binomial\")\n\nsummary(model)\n\n\nCall:\nglm(formula = y ~ age + city + country, family = \"binomial\", \n    data = simulated_data, weights = N)\n\nDeviance Residuals: \n      1        2        3        4        5        6  \n 1.1459   0.3555  -1.4512  -1.4069  -0.4309   1.5443  \n\nCoefficients: (1 not defined because of singularities)\n               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)     -0.8733     0.1876  -4.656 3.23e-06 ***\nage30+          -0.4794     0.2062  -2.325   0.0201 *  \ncityNice        -0.6027     0.2558  -2.356   0.0185 *  \ncityParis       -0.2286     0.2395  -0.954   0.3399    \ncountryGermany       NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 19.2591  on 5  degrees of freedom\nResidual deviance:  8.0953  on 2  degrees of freedom\nAIC: 43.492\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe estimate for Germany is NA. Calling the alias function on this GLM model shows that the dummy variable of Germany is linearly dependent on (a subset of) the other columns.\n\nalias(model)\n\nModel :\ny ~ age + city + country\n\nComplete :\n               (Intercept) age30+ cityNice cityParis\ncountryGermany  1           0     -1       -1       \n\n\nThis means that the column for Germany is redundant in this design matrix (or the model matrix), as the values of Germany (the pattern of 0s and 1s) can be perfectly predicted / recreated by combining the Intercept, Nice, and Paris columns using the coefficients from the output of alias(). This is why the perfect multicollinearity in this case leads to an NA coefficient.\n\n# countryGermany and Germany are identical\nmodel.matrix(~ ., data = simulated_data) %>% \n  as_tibble() %>% \n  select(-y, -N, -`age30+`) %>% \n  mutate(Germany = `(Intercept)` - cityNice - cityParis)\n\n# A tibble: 6 × 5\n  `(Intercept)` cityNice cityParis countryGermany Germany\n          <dbl>    <dbl>     <dbl>          <dbl>   <dbl>\n1             1        0         1              0       0\n2             1        1         0              0       0\n3             1        0         0              1       1\n4             1        0         1              0       0\n5             1        1         0              0       0\n6             1        0         0              1       1\n\n\nAnother interesting observation is that changing the order of the variables\n\n# here country comes before city\nglm(y ~ age + country + city, weights = N, data = simulated_data, family = \"binomial\") %>% \n  alias()\n\nModel :\ny ~ age + country + city\n\nComplete :\n          (Intercept) age30+ countryGermany cityNice\ncityParis  1           0     -1             -1      \n\n\nleads to different estimates being NA, i.e., the estimate for Paris is now NA and is linearly dependent on the Intercept, country (Germany), and another city (Nice).\nThe simplest solution here is to drop one of the city or country columns, or to build two separate models – one without country, and one without city.\nIf the goal is to estimate coefficients for both the city and country variables, then a mixed model with nested effects might be the right rabbit hole to go down, assuming they both have more than 10 levels or so. See the following links:\n\nhttps://stats.stackexchange.com/questions/197977/analyzing-nested-categorical-data\nhttps://stats.stackexchange.com/questions/79360/mixed-effects-model-with-nesting\nhttps://stats.stackexchange.com/questions/372257/how-do-you-deal-with-nested-variables-in-a-regression-model\nSecond bullet point from the answer: https://stats.stackexchange.com/questions/243811/how-to-model-nested-fixed-factor-with-glmm\nhttps://stackoverflow.com/questions/70537291/lmer-model-failed-to-converge-with-1-negative-eigenvalue\nhttps://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\nhttps://stackoverflow.com/questions/40723196/why-do-i-get-na-coefficients-and-how-does-lm-drop-reference-level-for-interact"
  }
]