[
  {
    "objectID": "posts/tweedie-with-identity-link-and-offset/index.html",
    "href": "posts/tweedie-with-identity-link-and-offset/index.html",
    "title": "Specifying an offset in a Tweedie model with identity link",
    "section": "",
    "text": "This post explores several things:\nIn this article, the words duration, offset, exposure, and weight are used interchangeably."
  },
  {
    "objectID": "posts/tweedie-with-identity-link-and-offset/index.html#introduction",
    "href": "posts/tweedie-with-identity-link-and-offset/index.html#introduction",
    "title": "Specifying an offset in a Tweedie model with identity link",
    "section": "Introduction",
    "text": "Introduction\nData coming from fields like ecology, insurance, epidemiology – such as number of new cases of a disease in multiple cities with very different population sizes, number of claims and total claim amounts from insurance policies with different durations, etc. – need to have the varying exposures accounted for as offsets while building models. These might look something like the following simulated dataset with an exposure variable (w) and a response variable (y):\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nn &lt;- 1e6\n\n# tweedie distribution parameters\np &lt;- 1.3 # variance power\nmu &lt;- 200 # mean\nphi &lt;- 350 # dispersion, so variance = phi * mu ^ p\n\nset.seed(45)\nsim_data &lt;- tibble(\n  w = runif(n, min = 0.5, max = 3),\n  y = tweedie::rtweedie(n = n, mu = mu, phi = phi / w, power = p)\n)\n\n# summary(sim_data)\n\nsim_data %&gt;% slice_head(n = 3)\n\n# A tibble: 3 × 2\n      w     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1  2.08     0\n2  1.29     0\n3  1.10     0\n\n\nFor readability, let’s pretend that this response variable is the individual claim amounts for an insurance company with a client base of 1 million customers. Additionally, for each individual the duration of how long they’ve been insured is also recorded. So the first customer has been insured for about 2 years and 1 month, and has not filed a claim at all. The third customer has been insured for a little over a year and has also not filed any claims."
  },
  {
    "objectID": "posts/tweedie-with-identity-link-and-offset/index.html#ratio-of-means-or-mean-of-ratios",
    "href": "posts/tweedie-with-identity-link-and-offset/index.html#ratio-of-means-or-mean-of-ratios",
    "title": "Specifying an offset in a Tweedie model with identity link",
    "section": "Ratio of means or mean of ratios?",
    "text": "Ratio of means or mean of ratios?\nThe quantity of interest from such datasets may be the mean per unit exposure – expected claim amount per individual per year, expected number of new cases of a disease per year per 100,000 people, etc. There are two separate quantities that can be calculated as expected values – the ratio of the means of amount and duration variables, or the mean of the individual ratios. The differences between these two quantities is explained pretty well in this stackexchange thread.\nThe ratio estimator estimates the first quantity \\(R = \\sum_i{y_i} / \\sum_i{w_i}\\). This answers the following question: given a population (or a group of individuals) who were followed-up / observed for a given amount of time, and generated a total amount of claims, how much of the total claim amount can be attributed per person per unit exposure?\nThe weighted sample mean of the individual ratios estimates the second quantity \\(\\mathbb{E} = \\sum_i{w_i y_i} / \\sum_i{w_i}\\). This answers the question: given each individual’s claim amount and exposure, what was the average individual’s claim amount per unit exposure?\nIn the latter case, the link between claim amount and duration at the individual level is preserved, whereas for the former, the numerator and denominators are totals at the population level.\nIf everyone had exactly the same weight \\(w_i\\) of 1 year, the denominator would sum to the sample size of 1 million for the data above, and both the estimates would be the same. If the weights were different, then the two results would differ.\nIn epidemiology, the ratio estimator is used for incidence rate calculations. On the other hand, the insurance papers on Tweedie models mentioned at the bottom of this post all seem to use the weighted mean approach.\nFor the simulated data above, the two estimates are quite different:\n\nsum(sim_data$y) / sum(sim_data$w)\n\n[1] 114.5185\n\n\n\nweighted.mean(sim_data$y, w = sim_data$w)\n\n[1] 200.4661\n\n\nIn this case, the second estimate matches the true mean value chosen for the simulation, which makes sense given that the data are simulated from a Tweedie model with varying exposures (\\(Y_i \\sim \\text{Tweedie}(\\mu_i, \\phi / w_i, p)\\)) but with the mean-variance relationship preserved under unit exposure. This is described in part A of the supplement to the Yang et al 2016 paper.\nHowever, both the numbers are right, as they answer different questions."
  },
  {
    "objectID": "posts/tweedie-with-identity-link-and-offset/index.html#building-a-regression-model",
    "href": "posts/tweedie-with-identity-link-and-offset/index.html#building-a-regression-model",
    "title": "Specifying an offset in a Tweedie model with identity link",
    "section": "Building a regression model",
    "text": "Building a regression model\nThis formula is easy enough to apply if there are no predictors, but a regression model is needed to estimate these quantities when there are predictors in the data.\nThere are two ways of specifying the model. The first method corresponds to passing the claim amounts unmodified, and passing the exposures as weights:\n\n# weighted mean estimator\nsummary(glm(y ~ 1, weights = w, data = sim_data,\n            family = statmod::tweedie(var.power = 1.3, link.power = 1)))\n\n\nCall:\nglm(formula = y ~ 1, family = statmod::tweedie(var.power = 1.3, \n    link.power = 1), data = sim_data, weights = w)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 200.4661     0.4431   452.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 349.4572)\n\n    Null deviance: 240499666  on 999999  degrees of freedom\nResidual deviance: 240499666  on 999999  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe estimated intercept and dispersion correspond to the same parameters (\\(\\mu = 200\\), \\(\\phi = 350\\)) picked for the simulation. This estimate also coincides with the weighted mean estimate. link.power = 1 indicates that the identity link function is used, and var.power = 1.3 or \\(p = 1.3\\) is assumed to be known here. Usually the profile likelihood approach is used to pick the best value on the interval \\(p \\in (1, 2)\\) for data with exact zeroes.\nOnce \\(p\\) is chosen, \\(\\mu\\) can be estimated independently of \\(\\phi\\), and finally dispersion \\(\\phi\\) can be estimated using an optimization algorithm, or one of the estimators described in section 6.8 of the Dunn and Smyth book. The glm function in R uses the Pearson estimator:\n\\[\n\\phi = \\frac{1}{N-p'} \\sum_{i=1}^{N} \\frac{w_i (y_i - \\hat\\mu_i) ^ 2}{\\hat\\mu_i^p}\n\\]\nwhere \\(p'\\) is the number of parameters in the model and \\(\\text{Var}(\\mu_i) = \\mu_i^p\\) is the variance function.\n\n# pearson estimate of dispersion\n(sum((sim_data$w * ((sim_data$y - 200.4661)^2)) / (200.4661^1.3))) / (n - 1)\n\n[1] 349.4573\n\n\nThe second method rescales the response variable with the weights, and passes exposures as weights to the model:\n\n# ratio estimator\nsummary(glm(y / w ~ 1, weights = w, data = sim_data,\n            family = statmod::tweedie(var.power = 1.3, link.power = 1)))\n\n\nCall:\nglm(formula = y/w ~ 1, family = statmod::tweedie(var.power = 1.3, \n    link.power = 1), data = sim_data, weights = w)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 114.5185     0.3655   313.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 492.3754)\n\n    Null deviance: 177557174  on 999999  degrees of freedom\nResidual deviance: 177557174  on 999999  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n\n\nThis estimate corresponds to the same output as the ratio estimator from the previous section. The estimate of dispersion \\(\\phi\\) is different from the true value of 350, which makes sense as dispersion is a function of the estimated value of the mean \\(\\mu\\)."
  },
  {
    "objectID": "posts/tweedie-with-identity-link-and-offset/index.html#what-does-the-math-look-like-behind-these-two-models",
    "href": "posts/tweedie-with-identity-link-and-offset/index.html#what-does-the-math-look-like-behind-these-two-models",
    "title": "Specifying an offset in a Tweedie model with identity link",
    "section": "What does the math look like behind these two models?",
    "text": "What does the math look like behind these two models?\nTo fit a Tweedie model to the data with weights to calculate the two different values in the previous section, the following log-likelihood function – taken from the Yang et al paper – can be maximized.\n\\[\nl(\\mu, \\phi, p | \\{y, x, w\\}_{i=1}^n) = \\sum_{i = 1}^{n}{ \\frac{w_i}{\\phi}} \\Bigg(y_i \\frac{\\mu_i^{1-p}}{1-p} - \\frac{\\mu_i^{2-p}}{2-p} \\Bigg) + log\\ a(y_i, \\phi / w_i, p)\n\\]\nFor this simulated dataset, \\(w_i\\) and \\(y_i\\) vary for each individual, but there are no predictors so an identity link function can be used 1, the overall mean is of interest so the \\(\\mu_i\\) term can be collapsed into a single \\(\\mu\\) parameter to be optimized, and the log a(...) term can be ignored as it’s not a function of \\(\\mu\\). Since the regression equation for an intercept only term is \\(\\mu_i = \\beta_0\\), we can replace the \\(\\mu\\) with \\(\\beta_0\\).\n1 which should be avoided if the model will be used for out-of-sample prediction to avoid predicting values below 0This makes it easy to compute the closed form solution, which is calculated by differentiating this function to produce the score function, and setting it to 0.\n\\[\n\\frac{\\partial l}{\\partial \\beta} = \\sum_{i = 1}^{n}{ \\frac{w_i}{\\phi}} (y_i \\beta_0^{-p} - \\beta_0^{1-p}) = 0\n\\]\n\\(\\phi\\) and \\(\\beta_0^{-p}\\) are non-zero constants, so can be pulled out of the summation and absorbed into the zero on the right-hand side to give\n\\[\n\\sum_{i = 1}^{n}{ w_i (y_i - \\beta_0) } = 0\n\\]\nFor glm(y ~ 1, weights = w, ...), this equals\n\\[\n\\sum_{i = 1}^{n} w_i y_i - \\beta_0 \\sum_{i = 1}^{n} w_i = 0 \\\\\n\\Rightarrow \\beta_0 = \\frac{\\sum_{i = 1}^{n} {w_i y_i}}{\\sum_{i = 1}^{n} w_i}\n\\]\nand for glm(y / w ~ 1, weights = w, ...), this equals\n\\[\n\\sum_{i = 1}^{n} w_i \\frac{y_i}{w_i} - \\beta_0 \\sum_{i = 1}^{n} w_i = 0 \\\\\n\\Rightarrow \\beta_0 = \\frac{\\sum_{i = 1}^{n} {y_i}}{\\sum_{i = 1}^{n} w_i}\n\\]"
  },
  {
    "objectID": "posts/tweedie-with-identity-link-and-offset/index.html#a-method-that-works-for-poisson-but-not-for-tweedie",
    "href": "posts/tweedie-with-identity-link-and-offset/index.html#a-method-that-works-for-poisson-but-not-for-tweedie",
    "title": "Specifying an offset in a Tweedie model with identity link",
    "section": "A method that works for Poisson but not for Tweedie",
    "text": "A method that works for Poisson but not for Tweedie\nWhat initially led me down this rabbit hole was finding this stackexchange post for specifying the offset with identity link in a poisson model, and naively (and incorrectly) fitting the Tweedie glm like this\n\nsummary(glm(y ~ w - 1, data = sim_data,\n            family = statmod::tweedie(var.power = 1.3, link.power = 1)))\n\n\nCall:\nglm(formula = y ~ w - 1, family = statmod::tweedie(var.power = 1.3, \n    link.power = 1), data = sim_data)\n\nCoefficients:\n  Estimate Std. Error t value Pr(&gt;|t|)    \nw 121.7328     0.4064   299.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 465.8885)\n\n    Null deviance:       NaN  on 1000000  degrees of freedom\nResidual deviance: 158130565  on  999999  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n\n\nand trying to figure out why this didn’t coincide with the ratio estimate of 114.5185, which is the case for the poisson models\n\n# ratio estimator\nsum(sim_data$y) / sum(sim_data$w)\n\n[1] 114.5185\n\n# generates warnings because we're passing a non-discrete response\n# point estimates are the same though\nunname(coef(suppressWarnings(glm(y ~ w - 1, family = poisson(link = \"identity\"), data = sim_data))))\n\n[1] 114.5185\n\nunname(coef(suppressWarnings(glm(y / w ~ 1, weights = w, family = poisson(link = \"identity\"), data = sim_data))))\n\n[1] 114.5185\n\n\nThis happens because substituting \\(\\mu_i = w_i \\beta_0\\) instead of \\(\\mu_i = \\beta_0\\) in the log-likelihood function\n\\[\nl(\\beta_0, \\phi, p | \\{y, x, w\\}_{i=1}^n) = \\sum_{i = 1}^{n}{ \\frac{w_i}{\\phi}} \\Bigg(y_i \\frac{(w_i \\beta_0)^{1-p}}{1-p} - \\frac{(w_i\\beta_0)^{2-p}}{2-p} \\Bigg) + log\\ a(y_i, \\phi / w_i, p)\n\\]\nleads to the following score function\n\\[\n\\frac{\\partial l}{\\partial \\beta} = \\sum_{i = 1}^{n} \\frac{w_i}{\\phi} (y_i w_i^{1-p} \\beta_0^{-p} - w_i^{2-p} \\beta_0^{1-p}) = 0\n\\]\nThe constant \\(\\phi\\) and \\(\\beta_0^{-p}\\) terms can be dropped from the equation, and the \\(w_i\\) outside the bracket are all 1 because no weights are passed to the glm call, so the following equation is solved\n\\[\n\\sum_{i = 1}^{n} y_i w_i^{1-p} - w_i^{2-p} \\beta_0 = 0\n\\]\nwhich can be simplified by pulling \\(w_i^{1-p}\\) as a common term\n\\[\n\\sum_{i = 1}^{n} w_i^{1-p} (y_i  - w_i \\beta_0) = 0\n\\]\nThis shows where the logical error happens, as well as how the correct estimate can be obtained, i.e., by passing weights = w ^ (p - 1) to the glm call, so that the \\(w_i^{1-p}\\) term cancels out\n\nsummary(glm(y ~ w - 1, weights = I(w^(1.3 - 1)), data = sim_data,\n            family = statmod::tweedie(var.power = 1.3, link.power = 1)))\n\n\nCall:\nglm(formula = y ~ w - 1, family = statmod::tweedie(var.power = 1.3, \n    link.power = 1), data = sim_data, weights = I(w^(1.3 - 1)))\n\nCoefficients:\n  Estimate Std. Error t value Pr(&gt;|t|)    \nw 114.5185     0.3655   313.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 492.3754)\n\n    Null deviance:       NaN  on 1000000  degrees of freedom\nResidual deviance: 177557174  on  999999  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n\n\nNow the point estimates, standard errors, and dispersion parameters correspond to the model where the ratio estimator is correctly specified.\nHere’s the code and the plot of the score equations for the different models:\n\n\nCode\nparameter_grid &lt;- seq(50, 300, 5)\n\nestimating_equation_curves &lt;- tibble(\n  parameter = parameter_grid,\n   `Mean of ratios` = map_dbl(.x = parameter_grid, .f = ~ {\n    sim_data %&gt;%\n      mutate(s = w * (y - .x)) %&gt;%\n      pull(s) %&gt;%\n      sum()\n  }),\n  `Ratio of means` = map_dbl(.x = parameter_grid, .f = ~ {\n    sim_data %&gt;%\n      summarize(across(.cols = c(w, y), .fns = sum)) %&gt;%\n      mutate(s = y - (.x * w)) %&gt;%\n      pull(s)\n  }),\n  Incorrect = map_dbl(.x = parameter_grid, .f = ~ {\n    sim_data %&gt;%\n      mutate(s = (w ^ (1 - 1.3)) * (y - w * .x)) %&gt;%\n      pull(s) %&gt;%\n      sum()\n  })\n)\n\nestimating_equation_curves %&gt;%\n  pivot_longer(-parameter) %&gt;%\n  ggplot(aes(x = parameter, y = value, linetype = name)) +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype = \"dotdash\") +\n  # geom_vline(xintercept = 114.5185, linetype = 2) +\n  # geom_vline(xintercept = 121.7328, linetype = 1) +\n  # geom_vline(xintercept = 200, linetype = 11) +\n  theme_bw() +\n  ylab(\"Score function\") +\n  xlab(\"Parameter\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())"
  },
  {
    "objectID": "posts/tweedie-with-identity-link-and-offset/index.html#references",
    "href": "posts/tweedie-with-identity-link-and-offset/index.html#references",
    "title": "Specifying an offset in a Tweedie model with identity link",
    "section": "References",
    "text": "References\n\nYang et al 2016 paper and supplement (link)\nDelong et al 2021 paper (link)\nChapter 12 on Tweedie models from the Dunn and Smyth book on GLMs\n\nOthers\n\nZhang 2013 (link)\nStan code for Tweedie\n\nhttps://discourse.mc-stan.org/t/tweedie-likelihood-compound-poisson-gamma-in-stan/14636\nthis gist"
  },
  {
    "objectID": "posts/simulating-survival-data-with-non-PH-and-cure/index.html",
    "href": "posts/simulating-survival-data-with-non-PH-and-cure/index.html",
    "title": "Simulating data from a time-to-event experiment with non-proportional hazards, random censoring, and cure",
    "section": "",
    "text": "This post is about simulating data from a log-logistic accelerated failure time mixture cure model with censoring. It also visually compares the estimated cumulative incidence curves obtained from several time-to-event estimators on a large simulated dataset.\nIt’s been quite a while since I’ve worked with time-to-event data and models. Also known as survival analysis, it is ideally suited for situations where the interest is not only in predicting whether an event will occur, but how long it will take for it to occur.\nSo I’m writing this post to reacquaint myself with some of the key concepts — and to learn some new things along the way — by simulating and analyzing data from a fictitious experiment motivated by a real experiment I worked on many moons ago.\nBefore writing any code, the following packages are attached:\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(flexsurvcure)\n\nLoading required package: survival\nLoading required package: flexsurv\n\nlibrary(ggsurvfit)\n\ntheme_set(theme_bw())\nSuppose a marketing manager is interested in the time it takes for an upgraded product to be adopted after launch among users of the previous version of the product. This rate-of-upgrade could be potentially hastened via a marketing campaign, which can be assumed to have a transient response1 — that is, the impact of this campaign decreases with time once the campaign has concluded."
  },
  {
    "objectID": "posts/simulating-survival-data-with-non-PH-and-cure/index.html#key-concepts",
    "href": "posts/simulating-survival-data-with-non-PH-and-cure/index.html#key-concepts",
    "title": "Simulating data from a time-to-event experiment with non-proportional hazards, random censoring, and cure",
    "section": "Key concepts",
    "text": "Key concepts\nOne of the key objects in survival analysis is the hazard function (\\(h(t)\\)), i.e., the rate at which the event of interest (e.g., product purchase) occurs as a function of time. Mathematically, it is expressed for continuous-time models as\n\\[\nh(t) =\n\\lim_{\\Delta t \\to 0} \\frac{P(t \\le T \\lt t + \\Delta t | T \\ge t)}{\\Delta t}\n\\]\nwhere \\(T\\) indicates the event time of interest which varies from individual to individual, \\(t\\) is a specific value of time, and \\(\\Delta t\\) is some very small increment in time. This event rate function cannot be interpreted as a probability but only as a rate, since it can have values larger than 1.\nIn case of discrete-time models, the hazard rate can be interpreted as the (conditional) probability \\(P(T = t | T \\ge t)\\).\nFor our experiment, suppose the hazards in the treatment and control groups look like this:\n\n\nCode\nhazard_function_loglogistic &lt;- function(alpha, beta, \n                               min_t = 0, \n                               max_t = 24, \n                               step_t = 0.1) {\n  \n  time &lt;- seq(min_t, max_t, step_t)\n  numerator &lt;- (beta / alpha) * ((time / alpha) ^ (beta - 1))\n  denominator &lt;- 1 + ((time / alpha) ^ beta)\n\n  tibble(\n    alpha = alpha,\n    beta = beta,\n    parameters = paste0(\"LL(\", alpha, \n                        \", \", beta, \")\"),\n    time = time,\n    hazard = numerator / denominator\n  )\n}\n\nloglogistic_attributes &lt;- bind_rows(\n  hazard_function_loglogistic(alpha = 4, beta = 7),\n  hazard_function_loglogistic(alpha = 5, beta = 7)\n) %&gt;% \n  mutate(\n    parameters = case_when(\n      alpha == 4 ~ \"Treatment group (T ~ LL(4, 7))\",\n      alpha == 5 ~ \"Control group (T ~ LL(5, 7))\"\n    )\n  ) \n\nloglogistic_attributes %&gt;% \n  ggplot(aes(x = time, y = hazard, group = parameters, color = parameters)) +\n  geom_line(linewidth = 1.1) +\n  # campaign end date\n  geom_vline(xintercept = 6, linetype = \"dotted\", color = \"gray40\") + \n  annotate(geom = \"text\", x = 6.2, y = 0.6, color = \"gray40\",\n           label = \"Campaign\\nends after\\n6 weeks\", hjust = \"left\") + \n  # impact of treatment disappears by 2.5 months\n  geom_vline(xintercept = 10, linetype = \"dotted\", color = \"gray40\") + \n  annotate(geom = \"text\", x = 9.8, y = 0.1, color = \"gray40\",\n           label = \"Impact of\\ntreatment\\ndies down\", hjust = \"right\") + \n  # analysis date (right-censoring at 3 months)\n  geom_vline(xintercept = 12, linetype = \"dotted\", color = \"gray40\") + \n  annotate(geom = \"text\", x = 12.2, y = 0.1, color = \"gray40\",\n           label = glue::glue(\"Cut-off date for analysis\\n\", \n                              \"(administrative right-censoring \", \n                              \"at 3 months)\"), \n           hjust = \"left\") + \n  scale_color_manual(values = c(\"gray20\", \"forestgreen\")) +\n  scale_x_continuous(breaks = seq(0, 24, 4)) +\n  scale_y_continuous(breaks = seq(0, 1.2, 0.2)) + \n  labs(x = \"Time since product launch (in weeks)\", y = \"Hazard rate\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nThese hazard functions can be expressed mathematically as\n\\[\nh(t; \\alpha, \\beta) = \\frac{(\\beta/\\alpha) (t / \\alpha)^{\\beta - 1}}{1 + (t / \\alpha)^\\beta}\n\\]\nand correspond to a log-logistic distribution (abbreviated as \\(\\text{LL}(\\alpha, \\beta)\\)) for the event times with scale parameter \\(\\alpha = 4\\) for the treatment group, \\(\\alpha = 5\\) for the control group, and shape parameter \\(\\beta = 7\\) identical in both groups.\nThe motivation for picking the \\(\\text{LL}(\\alpha, \\beta)\\) model for this experiment is that it’s a well known example of an accelerated failure time (AFT) model. The impact of treatment is assumed to be positive and transient, where the treatment “accelerates” the event process compared to the control group, with this “accelerating” treatment group peak occurring before the peak in the control group. The hazard function in the treatment group tapers off after some time (8 weeks in this case). The non-monotonic nature — i.e., of the function first increasing and then decreasing with time — can refer to the time it takes for the information about the new product to spread through the population.\nThe AFT model can be contrasted with the proportional hazards (PH) model, where the hazard rates for the two groups are assumed to be proportional over time. The Weibull distribution is a prominent example, although it can be parameterized as an AFT model as well. Compared to the log-logistic distribution, it has a monotonic hazard function\n\\[\nh(t; \\alpha, \\beta) = \\frac{\\beta}{\\alpha} \\Bigg(\\frac{x}{\\alpha}\\Bigg) ^ {\\beta - 1}\n\\]\nwith shape parameter \\(\\beta\\), and scale parameter \\(\\alpha\\), so is not suited for settings where the hazard is expected to go up and then down (or other non-monotonic shapes).\nThe hazard rate is a bit hard to interpret due to the fact that 1) it’s a conditional measure, 2) is a rate rather than a probability (so can have values &gt; 1), and 3) depends on the chosen unit of time.\nA related quantity known as the survivor function (aka survival curve) is the marginal probability of being event-free (“surviving”) beyond time \\(t\\). Mathematically,\n\\[\nS(t) = P[T &gt; t] = \\text{exp} \\Bigg[ - \\int_0^t h(u) du \\Bigg]\n\\]\nwhere \\(h(u)\\) is the hazard function (defined above).\nWhen the outcome is death due to disease (in healthcare), or machine failure (in engineering), it makes sense to look at factors (or treatments) that prolong survival. In the scenario where the event occurring faster is seen as positive, the survivor function can be flipped to get the cumulative incidence curve (CIC)\n\\[\nP[T \\le t] = F(t) = 1 - S(t)\n\\]\nwhere the curve traces the probability of experiencing the event before some time \\(t\\), and \\(F(t)\\) is the CDF of the event times. The cumulative incidence curve corresponding to \\(\\text{LL}(\\alpha, \\beta)\\) is given by \\(F(t) = [ 1 + (t / \\alpha)^ {-\\beta}] ^ {-1}\\) (with the survivor function \\(S(t) = [1 + (t / \\alpha)^ {\\beta}] ^ {-1}\\) — notice the sign difference for \\(\\beta\\))\n\n\nCode\nsurvivor_loglogistic &lt;- function(alpha, beta, time) {\n  1 / (1 + ((time / alpha) ^ beta))\n}\n\nloglogistic_attributes &lt;- loglogistic_attributes %&gt;% \n  mutate(\n    survival = survivor_loglogistic(alpha, beta, time),\n    cdf = 1 - survival\n  ) \n\ncumulative_incidence_plot &lt;- loglogistic_attributes %&gt;% \n  ggplot(aes(x = time, y = cdf, group = parameters, color = parameters)) + \n  geom_line(linewidth = 1.1) + \n  # campaign end date\n  geom_vline(xintercept = 6, linetype = \"dotted\", color = \"gray40\") + \n  annotate(geom = \"text\", x = 6.2, y = 0.6, color = \"gray40\",\n           label = \"Campaign\\nends after\\n6 weeks\", hjust = \"left\") + \n  # impact of treatment disappears by 2.5 months\n  geom_vline(xintercept = 10, linetype = \"dotted\", color = \"gray40\") + \n  annotate(geom = \"text\", x = 9.8, y = 0.1, color = \"gray40\",\n           label = \"Impact of\\ntreatment\\ndies down\", hjust = \"right\") + \n  # analysis date (right-censoring at 3 months)\n  geom_vline(xintercept = 12, linetype = \"dotted\", color = \"gray40\") + \n  annotate(geom = \"text\", x = 12.2, y = 0.1, color = \"gray40\",\n           label = glue::glue(\"Cut-off date for analysis\\n\", \n                              \"(administrative right-censoring \", \n                              \"at 3 months)\"), \n           hjust = \"left\") + \n  scale_color_manual(values = c(\"gray20\", \"forestgreen\")) +\n  scale_y_continuous(labels = scales::label_percent()) +\n  labs(x = \"Time since product launch (in weeks)\", \n       y = \"Cumulative incidence\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\ncumulative_incidence_plot + \n  scale_x_continuous(breaks = seq(0, 24, 4))\n\n\n\n\n\n\n\n\n\nAs expected of CDFs, the CIC starts at 0% for both groups at \\(t = 0\\) and keeps increasing over time until it reaches 100%."
  },
  {
    "objectID": "posts/simulating-survival-data-with-non-PH-and-cure/index.html#simulate-true-event-times",
    "href": "posts/simulating-survival-data-with-non-PH-and-cure/index.html#simulate-true-event-times",
    "title": "Simulating data from a time-to-event experiment with non-proportional hazards, random censoring, and cure",
    "section": "Simulate true event times",
    "text": "Simulate true event times\nFinally, we can get around to simulating some data from our experiment. Normally I’d use either the coxed::sim.survdata() function or the simsurv::simsurv() function, but in the interest of learning2 I’m going to do this manually using the cumulative hazard inversion method (usually attributed to this paper and concisely described in the simsurv package vignette here).\n2 These packages don’t support log-logistic models out-of-the-box anyway.For simulating data from a Weibull PH model as opposed to the log-logistic AFT model, see this, this, or the papers in the previous paragraph. This paper is also pretty useful for simulating data from more complex settings.\nPlugging in the value of \\(F(T_i)\\) for the log-logistic distribution, and doing some algebra gives the function for simulating the true (or latent) event time \\(T_i\\) for the \\(i\\)-th individual from a log-logistic distribution (matches equation 8 from this paper).\n\\[\n\\begin{align*}\nS(T_i) = 1 - F(T_i) &= U_i \\sim \\text{Uniform}(0, 1) \\\\\n1 - \\frac{1}{1 + (T_i / \\alpha)^{-\\beta}} &= U_i \\\\\n1 + \\Bigg(\\frac{T_i}{\\alpha}\\Bigg)^{-\\beta} &= \\frac{1}{1 - U_i} \\\\\n\\Bigg(\\frac{T_i}{\\alpha}\\Bigg)^{-\\beta} &= \\frac{U_i}{1 - U_i} \\\\\n\\Bigg(\\frac{\\alpha}{T_i}\\Bigg)^{\\beta} &= \\frac{1 - U_i}{U_i} \\\\\n\\frac{\\alpha}{T_i} &= \\Bigg(\\frac{1 - U_i}{U_i}\\Bigg)^{1 / \\beta} \\\\\nT_i &= \\alpha (U_i^{-1} - 1)^{-1 / \\beta} \\\\\n\\end{align*}\n\\]\nThe Wikipedia page for log-logistic distribution mentions that \\(\\text{log}(\\alpha_i) = X_i^T \\gamma\\) can be used to specify covariates that influence the scale parameter \\(\\alpha_i\\) — with \\(\\gamma\\) the (\\(p\\)-dimensional row) vector of coefficients, and \\(X_i\\) the (\\(p\\)-dimensional row) vector of covariates for individual \\(i\\). The shape parameter \\(\\beta\\) is assumed to be constant across covariate levels, but it’s possible to model \\(\\beta\\) as a function of covariates as well with \\(\\text{log}(\\beta_i) = Z_i^T \\delta\\) where \\(Z\\) and \\(X\\) can be identical, partly overlapping, or completely disjoint covariate sets, and \\(\\delta\\) the associated coefficients. Using logs ensures that both \\(\\alpha_i\\) and \\(\\beta_i\\) will have values greater than zero. This gives\n\\[\nT_i = \\text{exp}(X_i^T \\gamma)(U_i^{-1} - 1)^{-1 / \\text{exp}(Z_i^T \\delta)}\n\\]\nWe can draw simulated event times across 30 experiments each with about 1000 individuals split evenly between the treatment and control groups.\n\nn_sim &lt;- 30\nn_sample &lt;- 1000\nn_total &lt;- n_sim * n_sample\n\nset.seed(43)\nloglogistic_samples &lt;- tibble(\n  sim_id = rep(1:n_sim, each = n_sample),\n  id = rep(1:n_sample, times = n_sim),\n  treatment = sample(c(0, 1), size = n_total,\n                     replace = TRUE, prob = c(0.5, 0.5)),\n  parameters = case_when(\n    treatment == 1 ~ \"Treatment group (T ~ LL(4, 7))\",\n    treatment == 0 ~ \"Control group (T ~ LL(5, 7))\"\n  ),\n  # -log(1.25) corresponds to alpha = 5 in control\n  # and alpha = 4 in treatment\n  linear_predictor_alpha = log(5) - log(1.25) * treatment,\n  linear_predictor_beta =log(7),\n  u = runif(n_total, min = 0, max = 1),\n  time = (exp(linear_predictor_alpha) *\n            (((1 / u) - 1) ^ (-1 / exp(linear_predictor_beta))))\n)\n\nloglogistic_samples %&gt;% glimpse(width = 80)\n\nRows: 30,000\nColumns: 8\n$ sim_id                 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ id                     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …\n$ treatment              &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,…\n$ parameters             &lt;chr&gt; \"Treatment group (T ~ LL(4, 7))\", \"Control grou…\n$ linear_predictor_alpha &lt;dbl&gt; 1.386294, 1.609438, 1.386294, 1.609438, 1.38629…\n$ linear_predictor_beta  &lt;dbl&gt; 1.94591, 1.94591, 1.94591, 1.94591, 1.94591, 1.…\n$ u                      &lt;dbl&gt; 0.74638903, 0.60531731, 0.35898984, 0.93116119,…\n$ time                   &lt;dbl&gt; 4.666927, 5.315004, 3.682061, 7.253854, 4.86455…\n\n\nThe estimated CDFs for each sample are added to a zoomed-in version of cumulative incidence figure above.\n\n\nCode\ncumulative_incidence_plot_samples &lt;- cumulative_incidence_plot \n\n# for this hacky solution to overlay the true curves over\n# the sampled curves, see \n# https://stackoverflow.com/questions/20249653/insert-layer-underneath-existing-layers-in-ggplot2-object\ncumulative_incidence_plot_samples$layers &lt;- c(\n  stat_ecdf(\n    data = loglogistic_samples, \n    aes(x = time, linetype = parameters, \n        group = interaction(sim_id, parameters)), \n    inherit.aes = FALSE, \n    color = \"gray70\"\n  ), \n  cumulative_incidence_plot$layers\n)\n\ncumulative_incidence_plot_samples + \n  coord_cartesian(xlim = c(2, 10)) + \n  scale_x_continuous(breaks = seq(2, 10, 1)) + \n  guides(\n    color = guide_legend(nrow = 2), \n    linetype = guide_legend(nrow = 2)\n  )\n\n\n\n\n\n\n\n\n\nThe empirical CDFs of survival times for each sample fluctuate around the true curve in each group, which is reassuring. These simulated event times are the true individual event times for that specific sample of individuals. These are not always observed, which brings us to our next point."
  },
  {
    "objectID": "posts/simulating-survival-data-with-non-PH-and-cure/index.html#add-censoring",
    "href": "posts/simulating-survival-data-with-non-PH-and-cure/index.html#add-censoring",
    "title": "Simulating data from a time-to-event experiment with non-proportional hazards, random censoring, and cure",
    "section": "Add censoring",
    "text": "Add censoring\nOne complication I’ve ignored so far is (right-)censoring. Individuals are said to be right-censored if they haven’t had the event of interest yet at the time of analysis and therefore only a lower bound on their true event time is observed.\nThe most common data structure for survival analysis is the tuple \\((Y_i, \\delta_i)\\) where \\(Y_i = \\text{min}(T_i, C_i)\\) is the smaller of the event time \\(T_i\\) or the censoring time \\(C_i\\), and \\(\\delta_i = I(T_i \\le C_i)\\) is the event indicator with value 1 indicating the true event time is observed, and 0 indicating that the observation is censored. The censoring indicator is \\(1 - \\delta_i\\).\nNon-informative administrative (or Type-1) right censoring is the simplest type of censoring where the censoring time is the same for all individuals. If two analyses are carried out — one after three weeks, and another after twelve weeks — then the same individual with true \\(T_i = 4.5\\) is recorded as \\((Y_i = 3, \\delta_i = 0)\\) in the first analysis and \\((Y_i = 4.5, \\delta_i = 1)\\) in the second analysis.\nThis can be applied to the simulated event times easily enough using simple conditionals\n\nloglogistic_samples %&gt;% \n  select(time) %&gt;% \n  mutate(\n    latent = time,\n    time3 = pmin(time, 3.0),\n    cens3 = as.numeric(time &gt; 3.0),\n    time6 = pmin(time, 6.0),\n    cens6 = as.numeric(time &gt; 6.0),\n    time12 = pmin(time, 12.0),\n    cens12 = as.numeric(time &gt; 12.0)\n  ) %&gt;% \n  select(-time) %&gt;% \n  summary(digits = 2)\n\n     latent          time3          cens3          time6          cens6     \n Min.   : 0.84   Min.   :0.84   Min.   :0.00   Min.   :0.84   Min.   :0.00  \n 1st Qu.: 3.73   1st Qu.:3.00   1st Qu.:1.00   1st Qu.:3.73   1st Qu.:0.00  \n Median : 4.45   Median :3.00   Median :1.00   Median :4.45   Median :0.00  \n Mean   : 4.64   Mean   :2.97   Mean   :0.93   Mean   :4.49   Mean   :0.13  \n 3rd Qu.: 5.33   3rd Qu.:3.00   3rd Qu.:1.00   3rd Qu.:5.33   3rd Qu.:0.00  \n Max.   :20.05   Max.   :3.00   Max.   :1.00   Max.   :6.00   Max.   :1.00  \n     time12          cens12      \n Min.   : 0.84   Min.   :0.0000  \n 1st Qu.: 3.73   1st Qu.:0.0000  \n Median : 4.45   Median :0.0000  \n Mean   : 4.64   Mean   :0.0015  \n 3rd Qu.: 5.33   3rd Qu.:0.0000  \n Max.   :12.00   Max.   :1.0000  \n\n\nThe overall true event time distribution — in the latent column — goes up to 20 weeks, but due to censoring, 93% of observations are censored at 3 weeks, 13% at 6 weeks, and fewer than 1% at twelve weeks, with the maximum time equal to the censoring time. The larger the proportion of censoring, the larger the difference between the true mean and the mean of the censored event time distribution.\nIn more complex settings, censoring times can be simulated from (say) an exponential or gamma distribution that is assumed to be independent of the event times, or the censoring distribution can vary as a function of covariates. The next plot overlays the true event time densities in each treatment group on top of the censoring distribution \\(C_i \\sim \\text{Gamma}(6, 1)\\)\n\n\nCode\nset.seed(43)\nloglogistic_samples %&gt;%\n  select(parameters, time) %&gt;%\n  bind_rows(\n    tibble(\n      parameters = \"Censoring times ~ Gamma(6, 1)\",\n      time = rgamma(floor(n_total / 2), shape = 6, scale = 1)\n    )\n  ) %&gt;%\n  ggplot(aes(x = time, fill = parameters, color = parameters)) +\n  geom_density(alpha = 0.6) +\n  xlab(\"Time since product launch (in weeks)\") +\n  theme(legend.title = element_blank(), legend.position = \"bottom\") + \n  scale_color_manual(values = c(\"purple3\", \"gray20\", \"forestgreen\")) +\n  scale_fill_manual(values = c(\"purple3\", \"gray20\", \"forestgreen\"))\n\n\n\n\n\n\n\n\n\nIn this case, the code is nearly the same.\n\nset.seed(43)\nloglogistic_samples %&gt;%\n  select(parameters, latent = time) %&gt;%\n  mutate(\n    censoring_time = rgamma(n_total, shape = 6, scale = 1),\n    observed_time = pmin(latent, censoring_time),\n    cens = as.numeric(latent &gt; censoring_time)\n  ) %&gt;%\n  summarize(\n    across(\n      .cols = everything(), \n      .fns = ~ round(mean(.x), 2), \n      .names = \"mean_{.col}\"\n    ), \n    .by = parameters\n  )\n\n# A tibble: 2 × 5\n  parameters        mean_latent mean_censoring_time mean_observed_time mean_cens\n  &lt;chr&gt;                   &lt;dbl&gt;               &lt;dbl&gt;              &lt;dbl&gt;     &lt;dbl&gt;\n1 Treatment group …        4.12                5.99               3.81      0.25\n2 Control group (T…        5.16                6                  4.46      0.4 \n\n\nSummary of the simulated data shows that 25% of the individuals in the treatment group are censored compared to 40% of the individuals in the control group over the full range of time values (about 20 weeks)."
  },
  {
    "objectID": "posts/simulating-survival-data-with-non-PH-and-cure/index.html#add-cure",
    "href": "posts/simulating-survival-data-with-non-PH-and-cure/index.html#add-cure",
    "title": "Simulating data from a time-to-event experiment with non-proportional hazards, random censoring, and cure",
    "section": "Add cure",
    "text": "Add cure\nAnother complication is the presence of (statistical) cure. Some individuals will never purchase the product irrespective of whichever group they fall in. Which means that if the follow-up period is extended from 12 weeks to 150 or 500 weeks, these so-called “cured” individuals will still show up as censored because their true survival time is effectively infinite. Ignoring cure and using a cox model can lead to biased estimates of the quantities we’re interested in, as figure 2 of this paper visually demonstrates. It also gives an overview of the different types of (major) cure models. Another reference I found useful was this one3.\n3 This is the link to the Bertand and Legrand book chapter mentioned in the references below.The family of mixture cure model has the following survivor function form\n\\[\nS_{\\text{pop}}(t|I, L) = (1 - \\pi(I)) + \\pi(I) S_u(t|L)\n\\]\nwith two parts: the first part is based on the incidence which is whether an individual is cured or not (\\(\\pi(I) = P[B = 1 | I]\\) is the probability of being cured as a function of covariate \\(I\\), and \\(B\\) is the latent cure status); and the second part is concerned with latency, which is about how long it takes for the event to occur among the uncured. \\(L\\) is the set of factors that affect latency. Overlap between \\(I\\) and \\(L\\) is possible.\nThe other major family of cure models is the promotion time cure model with the survivor function\n\\[\nS_{\\text{pop}}(t|x) = \\text{exp}[-\\theta(x)F(t)]\n\\]\nwhich has a PH interpretation. In this model, parameters affecting cure and survival are not separated like they are in the mixture cure model.\nFinally, this paper mentions relative survival / excess mortality models as well, which considers individuals in the treatment or control group to be cured when the hazard rate decreases to background levels.\nThe simplest cure model is the constant one, where the cure fraction is the same across all groups. This can be simulated by generating a latent “cure” variable \\(B_i \\sim \\text{Bernoulli}(\\pi_i)\\) (with \\(\\pi_i = \\pi = 0.3\\) denoting the proportion of cured individuals in the next code chunk)\n\nset.seed(43)\nloglogistic_samples %&gt;%\n  mutate(\n    cured = rbinom(n_total, 1, 0.3),\n    time = ifelse(cured == 1, 10000, time), \n    time6 = pmin(time, 6.0),\n    cens6 = as.numeric(time &gt; 6.0)\n  ) %&gt;% \n  count(cured, cens6) %&gt;% \n  mutate(p = round(100 * (n / sum(n)), 1))\n\n# A tibble: 3 × 4\n  cured cens6     n     p\n  &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1     0     0 18898  63  \n2     0     1  2117   7.1\n3     1     1  8985  30  \n\n\nIf six weeks is used as the cutoff time for analysis, 63% of the individuals overall have experienced the event of interest, and 37% are censored of which 7% are the censored uncured observations, and 30% are the censored cured individuals.\nThe logistic model for incidence can be more elaborate — the following code generates data where the cure fraction varies as a function of (say) age\n\nloglogistic_samples %&gt;%\n  mutate(\n    age = {\n      rgamma(n = n_total, shape = 40, rate = 0.7) %&gt;% \n        pmin(., 90) %&gt;% \n        pmax(20, .) %&gt;% \n        round(., 1)\n    },\n    cured = rbinom(n_total, 1, plogis(4 - 0.07 * age)),\n    time = ifelse(cured == 1, 10000, time),\n    time6 = pmin(time, 6.0),\n    cens6 = as.numeric(time &gt; 6.0),\n    event6 = 1 - cens6,\n  )"
  },
  {
    "objectID": "posts/simulating-survival-data-with-non-PH-and-cure/index.html#putting-it-all-together",
    "href": "posts/simulating-survival-data-with-non-PH-and-cure/index.html#putting-it-all-together",
    "title": "Simulating data from a time-to-event experiment with non-proportional hazards, random censoring, and cure",
    "section": "Putting it all together",
    "text": "Putting it all together\nAll of the respective code chunks can be combined into a single call with a total sample size of \\(n = 50,000\\).\n\nn_sim &lt;- 50\nn_sample &lt;- 1000\nn_total &lt;- n_sim * n_sample\n\nset.seed(43)\n\nsimulated_data &lt;- tibble(\n  #\n  # simulate id variables and covariates\n  #\n  sim_id = rep(1:n_sim, each = n_sample),\n  id = rep(1:n_sample, times = n_sim),\n  treatment = sample(c(0, 1), size = n_total,\n                     replace = TRUE, prob = c(0.5, 0.5)),\n  parameters = case_when(\n    treatment == 1 ~ \"Treatment group (T ~ LL(4, 7))\",\n    treatment == 0 ~ \"Control group (T ~ LL(5, 7))\"\n  ),\n  age = {\n      rgamma(n = n_total, shape = 40, rate = 0.7) %&gt;% \n        pmin(., 90) %&gt;% \n        pmax(20, .) %&gt;% \n        round(., 1)\n    },\n  #\n  # simulate latent event times as a function of covariates\n  #\n  linear_predictor_alpha = log(5) - log(1.25) * treatment,\n  linear_predictor_beta =log(7),\n  u = runif(n_total, min = 0, max = 1),\n  uncured_time = (exp(linear_predictor_alpha) *\n            (((1 / u) - 1) ^ (-1 / exp(linear_predictor_beta)))), \n  # \n  # simulate the cure / incidence part from a logistic model\n  #\n  cured = rbinom(n_total, 1, plogis(4 - 0.07 * age)),\n  latent_time = ifelse(cured == 1, 10000, uncured_time),\n  #\n  # simulate censoring times and censor some individuals\n  # \n  random_censoring = rgamma(n_total, shape = 6, scale = 1),\n  # keep the smallest of the random censoring \n  # and administrative censoring (at t = 6) times\n  censoring_time = pmin(random_censoring, 6),\n  time = pmin(latent_time, censoring_time),\n  cens = as.numeric(latent_time &gt; censoring_time), \n  event = 1 - cens\n)\n\nsimulated_data %&gt;% glimpse(width = 80)\n\nRows: 50,000\nColumns: 16\n$ sim_id                 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ id                     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …\n$ treatment              &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,…\n$ parameters             &lt;chr&gt; \"Treatment group (T ~ LL(4, 7))\", \"Control grou…\n$ age                    &lt;dbl&gt; 63.7, 44.8, 62.5, 58.0, 66.1, 39.5, 66.5, 54.3,…\n$ linear_predictor_alpha &lt;dbl&gt; 1.386294, 1.609438, 1.386294, 1.609438, 1.38629…\n$ linear_predictor_beta  &lt;dbl&gt; 1.94591, 1.94591, 1.94591, 1.94591, 1.94591, 1.…\n$ u                      &lt;dbl&gt; 0.83141130, 0.70736178, 0.03092926, 0.12105442,…\n$ uncured_time           &lt;dbl&gt; 5.024099, 5.671901, 2.445388, 3.766801, 2.51775…\n$ cured                  &lt;int&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,…\n$ latent_time            &lt;dbl&gt; 5.024099, 5.671901, 2.445388, 3.766801, 2.51775…\n$ random_censoring       &lt;dbl&gt; 6.513541, 9.204403, 3.982804, 12.196175, 8.3553…\n$ censoring_time         &lt;dbl&gt; 6.000000, 6.000000, 3.982804, 6.000000, 6.00000…\n$ time                   &lt;dbl&gt; 5.024099, 5.671901, 2.445388, 3.766801, 2.51775…\n$ cens                   &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,…\n$ event                  &lt;dbl&gt; 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,…"
  },
  {
    "objectID": "posts/simulating-survival-data-with-non-PH-and-cure/index.html#sanity-check",
    "href": "posts/simulating-survival-data-with-non-PH-and-cure/index.html#sanity-check",
    "title": "Simulating data from a time-to-event experiment with non-proportional hazards, random censoring, and cure",
    "section": "Sanity check",
    "text": "Sanity check\nAs a sanity check, the log-logistic mixture cure model can be fit to the simulated data using the flexsurvcure package to see whether we can recover the parameters used to generate data.\n\nsimulated_data %&gt;% \n  # rename the treatment variable to make the printed summary nicer looking\n  rename(trt = treatment) %&gt;% \n  flexsurvcure(\n    formula = Surv(time, event, type = \"right\") ~ age,\n    data = .,\n    dist = \"llogis\",\n    link = \"logistic\",\n    anc = list(\n      scale = ~ trt,\n      shape = ~ trt\n    ),\n    mixture = TRUE\n  )\n\nThe code takes a minute or two to run, so the saved model can be read back in.\n\n\nCode\nread_rds(\"sanity-check-model.rds\")\n\n\nCall:\nflexsurvcure(formula = Surv(time, event, type = \"right\") ~ age, \n    data = ., dist = \"llogis\", link = \"logistic\", mixture = TRUE, \n    anc = list(scale = ~trt, shape = ~trt))\n\nEstimates: \n            data mean  est       L95%      U95%      se        exp(est)\ntheta             NA    0.98025   0.97650   0.98341        NA        NA\nshape             NA    7.14335   6.97237   7.31851   0.08829        NA\nscale             NA    4.98067   4.94343   5.01819   0.01907        NA\nage         57.15674   -0.06809  -0.07126  -0.06492   0.00162   0.93418\nshape(trt)   0.50436   -0.01890  -0.04914   0.01134   0.01543   0.98127\nscale(trt)   0.50436   -0.21891  -0.22755  -0.21027   0.00441   0.80340\n            L95%      U95%    \ntheta             NA        NA\nshape             NA        NA\nscale             NA        NA\nage          0.93122   0.93714\nshape(trt)   0.95204   1.01140\nscale(trt)   0.79648   0.81037\n\nN = 50000,  Events: 15676,  Censored: 34324\nTotal time at risk: 228001.9\nLog-likelihood = -42997.34, df = 6\nAIC = 86006.67\n\n\nThe coefficients for the incidence submodel are usually on the logit scale, and the coefficients for the latency submodel are usually on the log scale. The printed output is a little confusing at first glance since the coefficients in the est column are either untransformed (age, shape and scale parameters in the treatment group), exponentiated (scale and shape parameters in the control group), or inverse-logit transformed (theta). It may have been better to split the printed output into two sections — one for the incidence model, and the other for latency. Similar to how mgcv gives separate output for the parametric and the spline effects.\nThe theta parameter is the intercept in the incidence submodel, with a true value of 4 on the logit scale, and therefore \\(P[B = 1 | \\text{age} = 0] = 0.982\\) (calling plogis(4) in R or calculating \\(1 / (1 + \\text{exp}(-4))\\)). The estimated value is 0.9802, or qlogis(0.98025) = 3.904 on the logit scale. The age coefficient is estimated to be -0.068 on the logit scale with the true value set to -0.07. This corresponds to an odds ratio of \\(\\text{exp}(-0.06809) = 0.934\\).\nThe shape parameter \\(\\beta\\) of the log-logistic distribution in the latency submodel is very close to the true value of 7. The shape(trt) value is very close to the true value of 0, since \\(\\beta\\) is assumed to be the same in each arm.\nThe scale parameter \\(\\alpha\\) in the control group is very close (4.98) to the true value of 5. The coefficient for \\(\\alpha\\) in the treatment group — with the true value of 4 — can be derived by either 1) multiplying the alpha in the control group by exp(scale(trt)) (so \\(4.98067 \\times 0.80340 \\approx 4\\)), or 2) by adding these up on the log scale and exponentiating the result (\\(\\text{exp}(\\text{log}(4.98067) - 0.21891)\\))."
  },
  {
    "objectID": "posts/simulating-survival-data-with-non-PH-and-cure/index.html#comparing-arm-specific-cumulative-incidence-estimates",
    "href": "posts/simulating-survival-data-with-non-PH-and-cure/index.html#comparing-arm-specific-cumulative-incidence-estimates",
    "title": "Simulating data from a time-to-event experiment with non-proportional hazards, random censoring, and cure",
    "section": "Comparing arm-specific cumulative incidence estimates",
    "text": "Comparing arm-specific cumulative incidence estimates\nThis post could very well end now, but I already wrote the code for this section and I don’t feel like making another post, so I’ve included just the plots here with minimal code. The full code for this section can be found here.\nEssentially, I’m simulating data with administrative censoring at three and twelve weeks, with a constant cure fraction of 30%, and visually comparing the derived cumulative incidence curves in each of the treatment and control groups from the survivor functions estimated via 1) a Kaplan-Meier fit, 2) a stratified cox model, 3) the casebase sampling approach described in this paper and implemented in the casebase package4, and 4) a log-logistic mixture cure model with the treatment status as a variable in both the incidence and latency submodels.\n4 which I find extra attractive because it supports fitting penalized splines via the mgcv packageThe following plot shows \\(\\text{log}(-\\text{log}({\\hat{S}(t)}))\\) vs \\(\\text{log}(t)\\) in the left panel, and \\(\\text{log}(\\hat{S}(t) / (1 - \\hat{S}(t)))\\) vs \\(\\text{log}(t)\\) in the right panel. The former is the log-minus-log-survival plot for assessing the proportional hazards assumption (parallel straight lines indicate a Weibull model may be a good fit), and the latter is the log-survival-odds plot for assessing the proportional odds assumption (straight parallel lines together imply a log-logistic AFT model). Chapter 7 from the third edition of the Kleinbaum and Klein book is a good reference on how to pick parametric models based on the information in these plots.\n\n\nCode\n# very weird behaviour when seed is set to 43 here\n# cure fraction ends up being 0 in the treatment group\n# and 60% in the control group. Overall it's correct with value of 0.3\n# this goes away by removing the set.seed(43) call before generating\n# or using any other seed\nset.seed(49)\ncensored_and_cured_samples &lt;- loglogistic_samples %&gt;%\n  mutate(\n    # add cure status and modify time distribution accordingly\n    latent = time,\n    cured = rbinom(n(), 1, 0.3),\n    time = ifelse(cured == 1, 10000, time),\n    # add censoring indicator\n    time3 = pmin(time, 3.0),\n    cens3 = as.numeric(time &gt; 3.0),\n    event3 = 1 - cens3,\n    time6 = pmin(time, 6.0),\n    cens6 = as.numeric(time &gt; 6.0),\n    event6 = 1 - cens6,\n    time12 = pmin(time, 12.0),\n    cens12 = as.numeric(time &gt; 12.0),\n    event12 = 1 - cens12,\n    across(starts_with(\"event\"), as.integer)\n  )\n\ntwelve_weeks_data &lt;- censored_and_cured_samples %&gt;%\n  select(group = parameters, time = time12, event = event12)\n\nthree_weeks_data &lt;- censored_and_cured_samples %&gt;%\n  select(group = parameters, time = time3, event = event3)\n\n# plots for assessing the proportional hazards and proportional odds assumptions\ntwelve_weeks_data %&gt;%\n  survfit2(Surv(time, event, type = \"right\") ~ group, data = .) %&gt;%\n  tidy_survfit(time = seq(0, 12, 0.05), type = \"survival\") %&gt;%\n  select(time, strata, estimate) %&gt;%\n  mutate(\n    lntime = log(time),\n    `Log Minus Log Survival` = log(-log(estimate)),\n    `Log Survival Odds` = log(estimate / (1 - estimate))\n  ) %&gt;%\n  pivot_longer(cols = starts_with(\"Log \")) %&gt;%\n  ggplot(aes(x = lntime, y = value, color = strata, group = interaction(name, strata))) +\n  geom_step() +\n  labs(x = \"Log time\", y = \"\") +\n  scale_color_manual(values = c(\"gray20\", \"forestgreen\")) +\n  coord_cartesian(xlim = c(-0.5, 2.5)) +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  facet_wrap(~name, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\nThe next figure shows the estimated curves from the different model overlaid on top of the true curve when analyzing data using the information available at twelve weeks. All the methods produce the same estimates and are nearly indistinguishable from the true curves.\n\n\nCode\nread_rds(file = \"plot-data-12-weeks.rds\") %&gt;% \n  ggplot(aes(x = time, y = CI_est, color = group)) +\n  geom_line(aes(linetype = type, linewidth = type)) +\n  # add cure fraction line and text\n  geom_hline(yintercept = 0.7, color = \"gray40\", linetype = \"solid\") +\n  annotate(geom = \"text\", x = 0.5, y = 0.76, color = \"gray40\",\n           label = \"Cure fraction of 70% in both groups\", hjust = \"left\") +\n  labs(x = \"Time since product launch (in weeks)\",\n       y = \"Cumulative incidence\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  scale_color_manual(values = c(\"gray20\", \"forestgreen\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dotdash\", \"dashed\", \"dotted\", \"longdash\")) +\n  scale_linewidth_manual(values = c(0.4, 1.1, 1.1, 1.1, 1.1)) +\n  scale_x_continuous(breaks = seq(0, 12, 1)) +\n  scale_y_continuous(labels = scales::label_percent(), breaks = seq(0, 1, 0.2)) +\n  coord_cartesian(xlim = c(0, 12), ylim = c(0, 1)) +\n  guides(\n    color = guide_legend(nrow = 2),\n    linetype = guide_legend(nrow = 3)\n  )\n\n\n\n\n\n\n\n\n\nThe true cumulative incidence curves are scaled from \\([0, 1]\\) to the interval \\([0, 0.7]\\) to correspond to a cure fraction of 0.3 using the usual rescaling formula for mapping \\([x_{\\text{min}}, x_{\\text{}max}] \\rightarrow [a,b]\\) (with \\(a = 0\\), \\(b = 0.7\\), \\(x_{\\text{max}} = 1\\), \\(x_{\\text{min}} = 0\\), and \\(x\\) the probability \\(P[T \\le t]\\) among the uncured)\n\\[\nx_{\\text{normalized}} = a + (b - a) \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}\n\\]\nwhich just leads to multiplying the CDF in each group by 0.7.\nDoing the analysis after three weeks shows some interesting results\n\n\nCode\nplot_data_three_weeks &lt;- read_rds(file = \"plot-data-3-weeks.rds\")\n\nplot_data_three_weeks %&gt;%\n  filter(time &lt;= 3) %&gt;%\n  ggplot(aes(x = time, y = CI_est, color = group)) +\n  geom_line(aes(linetype = type, linewidth = type)) +\n  labs(x = \"Time since product launch (in weeks)\",\n       y = \"Cumulative incidence\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  scale_color_manual(values = c(\"gray20\", \"forestgreen\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dotdash\", \"dashed\", \"dotted\", \"longdash\")) +\n  scale_linewidth_manual(values = c(0.4, 1.1, 1.1, 1.1, 1.1)) +\n  scale_x_continuous(breaks = seq(0, 3, 1)) +\n  scale_y_continuous(labels = scales::label_percent(), breaks = seq(0, 0.1, 0.02)) +\n  guides(\n    color = guide_legend(nrow = 2),\n    linetype = guide_legend(nrow = 3)\n  ) + \n  coord_cartesian(xlim = c(0, 3), ylim = c(0, 0.1))\n\n\n\n\n\n\n\n\n\nAll the curves are close. Facetting by estimator and plotting the pointwise confidence bands for week 2 shows that the mixture cure model runs into some difficulties\n\n\nCode\nplot_data_three_weeks %&gt;%\n  filter(time &lt;= 3, type != \"Truth\") %&gt;%\n  ggplot(aes(x = time, y = CI_est)) +\n  # plot the true line on each panel\n  geom_line(\n    data = {\n      plot_data_three_weeks %&gt;%\n        filter(type == \"Truth\", time &lt;= 3) %&gt;%\n        select(-type)\n    },\n    inherit.aes = FALSE,\n    aes(x = time, y = CI_est, group = group, color = group),\n    linetype = \"solid\", linewidth = 1\n  ) +\n  # plot estimated curves and their confidence intervals\n  geom_line(aes(color = group), linetype = \"dashed\", linewidth = 1.1) +\n  geom_ribbon(aes(ymin = CI_lcl, ymax = CI_ucl, \n                  color = group, fill = group), alpha = 0.3) +\n  labs(x = \"Time since product launch (in weeks)\",\n       y = \"Cumulative incidence\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  scale_color_manual(values = c(\"gray20\", \"forestgreen\")) +\n  scale_fill_manual(values = c(\"gray20\", \"forestgreen\")) +\n  scale_x_continuous(breaks = seq(2, 3, 0.2)) +\n  scale_y_continuous(labels = scales::label_percent(), \n                     breaks = seq(0, 0.1, 0.02)) +\n  facet_wrap(~type) +\n  coord_cartesian(xlim = c(2, 3), ylim = c(0, 0.1))\n\n\n\n\n\n\n\n\n\nIn this case, based on the lack of plateauing of the KM fit at three weeks, assuming a cure model would not be justifiable, so one of the other three methods used here are probably better for estimating the CIC. At twelve weeks, the KM (or the CIC) curves have plateaued so a cure model might be a decent option, and the parameters of the mixture cure model can be identified.\nFinally, I’m going to end this post with an interesting quote I came across in the Hanley and Miettinen paper from the inventor of the highly popular Cox model preferring to use a parametric model over the semi-parametric Cox model\n\nParticularly notable are Cox’s own reflections (Reid, 1994) on the uses of his model:\n\nReid: How do you feel about the cottage industry that’s grown up around it [the Cox model]?\nCox: Don’t know, really. In the light of some of the further results one knows since, I think I would normally want to tackle problems parametrically, so I would take the underlying hazard to be a Weibull or something. I’m not keen on nonparametric formulations usually.\nReid: So if you had a set of censored survival data today, you might rather fit a parametric model, even though there was a feeling among the medical statisticians that that wasn’t quite right.\nCox: That’s right, but since then various people have shown that the answers are very insensitive to the parametric formulation of the underlying distribution [see, e.g., Cox and Oakes, Analysis of Survival Data, Chapter 8.5]. And if you want to do things like predict the outcome for a particular patient, it’s much more convenient to do that parametrically."
  },
  {
    "objectID": "posts/simulating-survival-data-with-non-PH-and-cure/index.html#references",
    "href": "posts/simulating-survival-data-with-non-PH-and-cure/index.html#references",
    "title": "Simulating data from a time-to-event experiment with non-proportional hazards, random censoring, and cure",
    "section": "References",
    "text": "References\n\nBender, R., Augustin, T. and Blettner, M. (2005), Generating survival times to simulate Cox proportional hazards models. Statist. Med., 24: 1713-1723. https://doi.org/10.1002/sim.2059\nCrowther, M.J. and Lambert, P.C. (2013), Simulating biologically plausible complex survival data. Statist. Med., 32: 4118-4134. https://doi.org/10.1002/sim.5823\nAl-Shomrani, A.A., Shawky, A.I., Arif, O.H. et al. Log-logistic distribution for survival data analysis using MCMC. SpringerPlus 5, 1774 (2016). https://doi.org/10.1186/s40064-016-3476-7\nLambert, P. C. (2007). Modeling of the Cure Fraction in Survival Studies. The Stata Journal, 7(3), 351-375. https://doi.org/10.1177/1536867X0700700304\nAmico, M. and Van Keilegom, I. (2018). Cure Models in Survival Analysis. Annual Review of Statistics and Its Application, Vol. 5:311-342.\nhttps://doi.org/10.1146/annurev-statistics-031017-100101\nLegrand, C. and Bertrand, A. (2019). Cure Models in Oncology Clinical Trials. (Seems to be a book chapter from this book. Non-paywalled link here.)\nKleinbaum, D.G. and Klein, M. (2012) Survival Analysis: A Self-Learning Text. 3rd Edition, Springer, New York.https://doi.org/10.1007/978-1-4419-6646-9\nHanley, J. & Miettinen, O. (2009). Fitting Smooth-in-Time Prognostic Risk Functions via Logistic Regression. The International Journal of Biostatistics, 5(1). https://doi.org/10.2202/1557-4679.1125"
  },
  {
    "objectID": "posts/one-reason-why-a-glm-coefficient-is-NA/index.html",
    "href": "posts/one-reason-why-a-glm-coefficient-is-NA/index.html",
    "title": "One reason why a (g)lm coefficient is NA",
    "section": "",
    "text": "TL;DR: Use the alias function in R to check if you have nested factors (predictors) in your data\nRecently while fitting a logistic regression model, some of the coefficients estimated by the model were NA. Initially I thought it was due to separation1, as that’s the most common issue I usually face when fitting unregularized models on data.\n1 see this Wikipedia article, or this stats.stackexchange.com thread (and the associated links in the sidebar)2 in this day and age of ChatGPT, I knowHowever, googling2 threw up many threads on multicollinearity and anyway, separation usually leads to nonsensical estimates like \\(1.5 \\times 10^8\\) instead of NA.\nAfter combing through many stackexchange threads, I discovered the alias function in R from this thread, which was pretty handy at identifying the problematic column(s).\nIt’s interesting that the alias documentation doesn’t mention anything about GLMs (glm()) but this does work on glm(..., family = \"binomial\") model objects3.\n3 possibly since the class of a glm object is c(\"glm\", \"lm\")The rest of this post explores this issue and its resolution using aggregated test data, where the city variable is intentionally nested within the country variable.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nsimulated_data &lt;- tribble(\n  ~age, ~city, ~country, ~y, ~N,\n  \"&lt; 30\", \"Paris\", \"France\", 30, 100,\n  \"&lt; 30\", \"Nice\", \"France\", 20, 100,\n  \"&lt; 30\", \"Berlin\", \"Germany\", 23, 100,\n  \"30+\", \"Paris\", \"France\", 12, 100,\n  \"30+\", \"Nice\", \"France\", 11, 100,\n  \"30+\", \"Berlin\", \"Germany\", 27, 100\n) %&gt;% \n  mutate(y = y / N)\n\nmodel &lt;- glm(y ~ age + city + country, weights = N, data = simulated_data, family = \"binomial\")\n\nsummary(model)\n\n\nCall:\nglm(formula = y ~ age + city + country, family = \"binomial\", \n    data = simulated_data, weights = N)\n\nCoefficients: (1 not defined because of singularities)\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.8733     0.1876  -4.656 3.23e-06 ***\nage30+          -0.4794     0.2062  -2.325   0.0201 *  \ncityNice        -0.6027     0.2558  -2.356   0.0185 *  \ncityParis       -0.2286     0.2395  -0.954   0.3399    \ncountryGermany       NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 19.2591  on 5  degrees of freedom\nResidual deviance:  8.0953  on 2  degrees of freedom\nAIC: 43.492\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe estimate for Germany is NA. Calling the alias function on this GLM model shows that the dummy variable of Germany is linearly dependent on (a subset of) the other columns.\n\nalias(model)\n\nModel :\ny ~ age + city + country\n\nComplete :\n               (Intercept) age30+ cityNice cityParis\ncountryGermany  1           0     -1       -1       \n\n\nThis means that the column for Germany is redundant in this design matrix (or the model matrix), as the values of Germany (the pattern of 0s and 1s) can be perfectly predicted / recreated by combining the Intercept, Nice, and Paris columns using the coefficients from the output of alias(). This is why the perfect multicollinearity in this case leads to an NA coefficient.\n\n# countryGermany and Germany are identical\nmodel.matrix(~ ., data = simulated_data) %&gt;% \n  as_tibble() %&gt;% \n  select(-y, -N, -`age30+`) %&gt;% \n  mutate(Germany = `(Intercept)` - cityNice - cityParis)\n\n# A tibble: 6 × 5\n  `(Intercept)` cityNice cityParis countryGermany Germany\n          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1             1        0         1              0       0\n2             1        1         0              0       0\n3             1        0         0              1       1\n4             1        0         1              0       0\n5             1        1         0              0       0\n6             1        0         0              1       1\n\n\nAnother interesting observation is that changing the order of the variables\n\n# here country comes before city\nglm(y ~ age + country + city, weights = N, data = simulated_data, family = \"binomial\") %&gt;% \n  alias()\n\nModel :\ny ~ age + country + city\n\nComplete :\n          (Intercept) age30+ countryGermany cityNice\ncityParis  1           0     -1             -1      \n\n\nleads to different estimates being NA, i.e., the estimate for Paris is now NA and is linearly dependent on the Intercept, country (Germany), and another city (Nice). This depends on which term enters the model first.\nThe simplest solution here is to drop one of the city or country columns, or to build two separate models – one without country, and one without city.\nIf the goal is to estimate coefficients for both the city and country variables, then a mixed model with nested effects might be the right rabbit hole to go down, assuming they both have more than 10 levels or so. See the following links:\n\nhttps://stats.stackexchange.com/questions/197977/analyzing-nested-categorical-data\nhttps://stats.stackexchange.com/questions/79360/mixed-effects-model-with-nesting\nhttps://stats.stackexchange.com/questions/372257/how-do-you-deal-with-nested-variables-in-a-regression-model\nSecond bullet point from the answer: https://stats.stackexchange.com/questions/243811/how-to-model-nested-fixed-factor-with-glmm\nhttps://stackoverflow.com/questions/70537291/lmer-model-failed-to-converge-with-1-negative-eigenvalue\nhttps://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\nhttps://stackoverflow.com/questions/40723196/why-do-i-get-na-coefficients-and-how-does-lm-drop-reference-level-for-interact"
  },
  {
    "objectID": "posts/jigger-volume-estimation/index.html",
    "href": "posts/jigger-volume-estimation/index.html",
    "title": "Determining the volume of my (cocktail) jigger",
    "section": "",
    "text": "In the fall of 2022, I was gifted a cocktail mixing set for my birthday1 which included a cocktail jigger. A jigger is an essential tool for mixing cocktails as it makes it relatively easy to measure the quantity of drinks that go into a cocktail (e.g., 40 ml of vodka, 10 ml of sugar syrup, etc.).\nAfter a couple of months of not having to use my cocktail set due to travelling, I couldn’t remember the volume of either side (i.e., basin) of the jigger, nor could I find the remains of the packaging material for reference. Googling showed that jiggers come in different sizes, so I decided to measure the volume myself.\nOver the next hour or so, I came up with two methods2:"
  },
  {
    "objectID": "posts/jigger-volume-estimation/index.html#method-1-weight-based",
    "href": "posts/jigger-volume-estimation/index.html#method-1-weight-based",
    "title": "Determining the volume of my (cocktail) jigger",
    "section": "Method 1: Weight-based",
    "text": "Method 1: Weight-based\nThe digital kitchen scale I own has this nifty feature where it can account for and automatically subtract the weight of the container so that only the weight of the object of interest is shown.\nThe scale measures weight in grams (g), but I was interested in the volume in milliliters (ml). Quick Googling reminded me of the nifty fact I learned and forgot a long time ago, that the weight of 1 g of water is approximately equal to the volume occupied by 1 ml of water.\nPlacing the jigger on the scale and ensuring that the scale read zero g, I filled up the larger basin to the top with tap water. The scale showed 51g, which was odd as I was expecting it to show a number that would be a multiple of five (i.e., a number ending with a 0 or a 5)3.\n3 although some jigger images online show possible measurements like 22.5 ml (which is 0.75 oz)4 obvious to a Statistician or a Data ScientistSo obviously4 the next logical step was to take multiple measurements. After making sure to drain the jigger, clean the scale of any spilled water droplets, and ensuring the scale read zero, I took the next measurement which read 48 g. Excellent. I love consistent results.\nSo I took another 8 measurements, and repeated this process for the smaller basin of the jigger as well. These measurements are visualized below.\nEach time I filled it to what I perceived to be the top but I got different measurements. I don’t think that this variability is necessarily due to the variability of the digital scale itself but most likely due to the variability in my perception of what counts as ‘filled to the brim’.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\nvol_big &lt;- c(51, 48, 48, 52, 49, 50, 51, 52, 50, 50)\nvol_small &lt;- c(31, 28, 32, 30, 30, 29, 32, 32, 31, 31)\n\ntibble(\n  Basin = c(rep(\"Smaller basin\", 10), rep(\"Larger basin\", 10)),\n  `Volume (in ml)` = c(vol_small, vol_big)\n) %&gt;%\n  mutate(Basin = forcats::fct_rev(Basin)) %&gt;% \n  ggplot(aes(x = `Volume (in ml)`, fill = Basin, color = Basin)) +\n  geom_dotplot(binwidth = 1, stroke = 2, dotsize = 0.7) +\n  theme_classic() +\n  theme(legend.position = \"none\", \n        axis.title.x = element_text(size = 16), \n        axis.text.x = element_text(size = 16), \n        strip.text = element_text(size = 16)) + \n  scale_y_continuous(NULL, breaks = NULL) + \n  scale_fill_manual(values = c(\"#E69F00\", \"darkgreen\")) + \n  scale_color_manual(values = c(\"#E69F00\", \"darkgreen\")) + \n  facet_wrap(~ Basin, scales = 'free')\n\n\n\n\n\n\n\n\n\nAveraging these measurements gave me a volume of 50.1 ml for the big basin and 30.6 ml for the smaller basin.\nInterestingly5, it also indicates6 that I usually put anywhere between 48-52 (or 28-32) ml of a drink when I’m supposed to add 50 (or 30) ml.\n5 to me and literally nobody else6 assuming the digital scale is not causing this variability"
  },
  {
    "objectID": "posts/jigger-volume-estimation/index.html#method-2-dimension-based",
    "href": "posts/jigger-volume-estimation/index.html#method-2-dimension-based",
    "title": "Determining the volume of my (cocktail) jigger",
    "section": "Method 2: Dimension-based",
    "text": "Method 2: Dimension-based\nThe shape of a jigger reminded me of a (partial) cone, so I measured7 the height \\(h\\) from the mouth of each basin to the point where it joins the other basin, and the diameter \\(2R\\) of the mouth of each basin.\n7 using a pair of digital calipers I totally had lying around the house and did not use this exercise as an excuse to buyFor the smaller basin, the height \\(h_s\\) was measured as 3.48 cm, and a diameter of 3.9 cm (so a radius \\(R_s\\) of 1.95 cm).\nFor the larger basin, the height \\(h_l\\) was measured as 5.28 cm, and a diameter of 4.18 cm (so a radius \\(R_l\\) of 2.09 cm).\nThe radius \\(r\\) of the base where the two basins are attached to each other is \\(2.78 / 2 = 1.39\\).\nPlugging these into the formula for a partial cone8\n8 here’s a derivation (and another) of the volume of a cone using calculus, which I’ve probably derived in school or university and long since forgotten\\[V = \\frac{1}{3} \\times \\pi \\times h \\times \\Big(R^2 + Rr + r^2\\Big)\\]\n\n\nCode\ncone &lt;- function(h, r, R) {\n  (pi * h * (R^2 + (R * r) + r^2)) / 3\n}\n\n\ngave a volume9 of 30.8 ml for the smaller basin and 50.9 for the larger basin which was very similar to the numbers from the other method, but not exactly equal to 30 and 50 ml due to (human) measurement error.\n9 using the conversion factor of 1 cm3 to 1 ml for volume of waterA few days after doing all these measurements, I came across a similar (in spirit?) post on a blog I frequent, which inspired me to write this up."
  },
  {
    "objectID": "posts/how-many-ratings-do-i-need/index.html",
    "href": "posts/how-many-ratings-do-i-need/index.html",
    "title": "How Many Five-Star Ratings Do I Need?",
    "section": "",
    "text": "In the fall of 2021, a friend who runs her own business sent me this picture with the accompanying question:\n\nThe screenshot indicates an average1 rating of 4.5 stars out of a total of \\(n = 363\\) reviews, and additionally lists the percent of time a rating between 1-5 was given.\n1 Unweighted, I assume.It seemed like an easy enough problem – perfect to explore on a rainy, gray Saturday in November – so I decided to have a crack at it.\nAfter doing some trivial algebra, somewhat successfully writing a for-loop, and adequately pleased with the solution, I posted the write-up for my initial approach on RPubs and sent off the answer2 to my friend.\n2 Spoiler: she needed between 47-50 5-star ratings to pull up the average rating to 4.6. Apologies if you were eagerly waiting for the end to find out what the answer was.However, while going through this document to clean it up for the blog (in 2022), I realized there’s a simpler way of solving this problem. Before I describe it further, I’m going to load some R packages and extract the data from the image into R objects.\n\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nn &lt;- 363\n\n# percent ratings scaled to [0,1]\np &lt;- c(0.06, 0.01, 0.02, 0.12, 0.79) %&gt;%\n  set_names(nm = 1:5) %&gt;% \n  print()\n\n   1    2    3    4    5 \n0.06 0.01 0.02 0.12 0.79 \n\n\nWhat made me rethink my approach was that I previously ended up with the wrong ratings vector after converting the (rounded) percentages into counts3.\n3 Although I mostly worked around it afterwards via simulation.So for example, 79% out of 363 ratings were 5-star ratings, which translates to 286.77 and rounded to the nearest integer becomes 287. But this isn’t the only integer that rounds to 79% when divided by 363. Any integer \\(k\\) when divided by 363 that ends up in the (open) interval (78.5%, 79.5%) would be a possible candidate.\n\nseq(284, 289, 1) %&gt;% \n  set_names(nm = ~ .x) %&gt;% \n  map_dbl(.f = ~ round(100 * .x / 363))\n\n284 285 286 287 288 289 \n 78  79  79  79  79  80 \n\n\nSo any one of 285-288 5-star ratings are compatible with the information in the screenshot. We can get the same range for the other ratings (i.e., 1-4).\n\nplausible_counts_per_rating &lt;- p %&gt;% map(.f = function(prop) {\n  approx_val &lt;- round(prop * n)\n  possible_vals &lt;- seq(from = approx_val - 10, to = approx_val + 10, by = 1) %&gt;% \n    set_names(nm = ~ .x) %&gt;% \n    map_dbl(.f = ~ round(.x / n, 2)) %&gt;% \n    keep(.p = ~ .x == prop) %&gt;% \n    names()\n}) %&gt;% \n  as_tibble(.name_repair = ~ paste('x', .x, sep = \"\"))\n\nplausible_counts_per_rating\n\n# A tibble: 4 × 5\n  x1    x2    x3    x4    x5   \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 20    2     6     42    285  \n2 21    3     7     43    286  \n3 22    4     8     44    287  \n4 23    5     9     45    288  \n\n\nEach column shows the plausible values for the number of times a rating was provided. We can create all possible combinations of these values to identify the subset of \\(4 ^ 5 = 1024\\) possible combinations that are compatible with the information in the screenshot, i.e., a mean of 4.5 when rounded to 1 decimal place and a total of 363 ratings.\n\nrating_combinations &lt;- plausible_counts_per_rating %&gt;%\n  # create all combinations of all values in all columns\n  expand(crossing(x1, x2, x3, x4, x5)) %&gt;%\n  mutate(across(.fns = as.integer), \n         total_ratings = x1 + x2 + x3 + x4 + x5, \n         sum_ratings = x1 + (2 * x2) + (3 * x3) + (4 * x4) + (5 * x5), \n         mean_rating = round(sum_ratings / 363, 1)) %&gt;% \n  print(n = 10)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(.fns = as.integer)`.\nCaused by warning:\n! Using `across()` without supplying `.cols` was deprecated in dplyr 1.1.0.\nℹ Please supply `.cols` instead.\n\n\n# A tibble: 1,024 × 8\n      x1    x2    x3    x4    x5 total_ratings sum_ratings mean_rating\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;         &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1    20     2     6    42   285           355        1635         4.5\n 2    20     2     6    42   286           356        1640         4.5\n 3    20     2     6    42   287           357        1645         4.5\n 4    20     2     6    42   288           358        1650         4.5\n 5    20     2     6    43   285           356        1639         4.5\n 6    20     2     6    43   286           357        1644         4.5\n 7    20     2     6    43   287           358        1649         4.5\n 8    20     2     6    43   288           359        1654         4.6\n 9    20     2     6    44   285           357        1643         4.5\n10    20     2     6    44   286           358        1648         4.5\n# ℹ 1,014 more rows\n\n\n\npossible_ratings &lt;- rating_combinations %&gt;% \n  filter(total_ratings == 363, mean_rating == 4.5) %&gt;% \n  print(n = Inf)\n\n# A tibble: 3 × 8\n     x1    x2    x3    x4    x5 total_ratings sum_ratings mean_rating\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;         &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1    23     4     9    42   285           363        1651         4.5\n2    23     5     7    43   285           363        1651         4.5\n3    23     5     8    42   285           363        1650         4.5\n\n\nSo one of these three possible vectors is used to produce the statistics shown in the screenshot.\nThe formula for computing the (arithmetic) mean can be rearranged to easily calculate the required number of five-star ratings to bring the mean from 4.5 to 4.6.\nLet \\(n_1\\) denote the number of additional five-star ratings, \\(y\\) the (weighted) sum of the rating counts, and \\(n\\) the current number of ratings (i.e., 363) in the following equation:\n\\[\\frac{y + (n_1 \\times 5)}{n + n_1} = 4.6\\]\nThis can be rewritten as\n\\[5n_1 = 4.6 \\times (n + n_1) - y\\]\nand simplified to yield\n\\[n_1 = \\frac{(4.6 \\times n) - y}{0.4}\\]\nand coded up as an R function\n\nnum_five_star &lt;- function(sum_ratings, total_ratings) {\n  ((4.6 * total_ratings) - sum_ratings) / 0.4\n}\n\nApplying this function to the possible ratings vector leads to\n\npossible_ratings %&gt;% \n  mutate(extra_5_stars = ceiling(num_five_star(sum_ratings, total_ratings))) %&gt;% \n  select(-total_ratings, -mean_rating)\n\n# A tibble: 3 × 7\n     x1    x2    x3    x4    x5 sum_ratings extra_5_stars\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n1    23     4     9    42   285        1651            47\n2    23     5     7    43   285        1651            47\n3    23     5     8    42   285        1650            50\n\n\nso 47-50 additional 5-star ratings are needed to pull up the average4 to 4.6, assuming that the future ratings are all five-star ratings.\n\n\n\n\n4 exact average, not an average resulting from rounding 4.57 (say) to 4.6. In the latter case, fewer than 47 five-star ratings would be required."
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html",
    "href": "posts/gee-on-aggregated-data/index.html",
    "title": "GEE on aggregated data?",
    "section": "",
    "text": "TL;DR: The geepack R package doesn’t do what it doesn’t claim to do"
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html#model",
    "href": "posts/gee-on-aggregated-data/index.html#model",
    "title": "GEE on aggregated data?",
    "section": "Model",
    "text": "Model\nThe code below simulates data from the following mixed-effects model2 with random-intercept terms where person \\(j\\) is nested within cluster \\(i\\). Each cluster has a different proportion of treatment uptake \\(p_{treated,j}\\) varying between 40-60% where \\(X_{ij}\\) indicates whether person \\(j\\) in cluster \\(i\\) received the treatment or not.\n2 The model syntax is inspired by the statistical rethinking book\\[\n\\begin{gather}\nY_{ij} \\sim \\text{Bernoulli}(p_{ij}) \\\\\n\\text{logit}(p_{ij}) = -1.4 + 0.3 \\times X_{ij} + \\alpha_i \\\\\n\\alpha_i \\sim \\text{Normal}(0, 1) \\\\\nX_{ij} \\sim \\text{Bernoulli}(p_{treated,i}) \\\\\np_{treated,i} \\sim \\text{Uniform(0.4, 0.6)}\n\\end{gather}\n\\]\nThe time taken to train the model on the raw as well as the aggregate data, the point estimates, and standard errors for the coefficients are stored for analysis."
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html#code",
    "href": "posts/gee-on-aggregated-data/index.html#code",
    "title": "GEE on aggregated data?",
    "section": "Code",
    "text": "Code\n\nsimulate_data_and_get_estimates &lt;- function(n, n_clus, seed) {\n  set.seed(seed)\n  # cluster random intercept\n  rand_intercept &lt;- rnorm(n_clus, mean = 0, sd = 1)\n\n  set.seed(seed)\n  sim_data &lt;- map_dfr(.x = 1:n_clus, .f = ~ {\n    tibble(\n      id = .x,\n      x = rbinom(round(n / n_clus), size = 1, prob = runif(1, 0.4, 0.6)),\n      y = rbinom(round(n / n_clus), size = 1, prob = plogis(-1.4 + 0.3 * x + rand_intercept[[.x]]))\n    )\n  })\n\n  # fit model to raw data\n  t0 &lt;- Sys.time()\n  mod1_sim &lt;- geeglm(\n    y ~ x,\n    data = sim_data,\n    family = \"binomial\", id = id, corstr = \"exchangeable\"\n  )\n  t1 &lt;- Sys.time()\n  t1 &lt;- as.numeric(difftime(t1, t0, units = \"secs\"))\n\n  # fit GLM model to raw data\n  mod1_sim_glm &lt;- glm(y ~ factor(id) + x - 1,\n                      data = sim_data, family = \"binomial\")\n\n  # fit model to aggregated data\n  agg_data &lt;- sim_data %&gt;%\n    group_by(id, x) %&gt;%\n    summarize(n = n(), y = sum(y), .groups = \"drop\") %&gt;%\n    mutate(y_prob = y / n)\n\n  t2 &lt;- Sys.time()\n  mod2_sim &lt;- geeglm(\n    y_prob ~ x,\n    data = agg_data, weights = n,\n    family = \"binomial\", id = id, corstr = \"exchangeable\"\n  )\n  t3 &lt;- Sys.time()\n  t3 &lt;- as.numeric(difftime(t3, t2, units = \"secs\"))\n\n  # fit GLM model to aggregated data\n  mod2_sim_glm &lt;- glm(y_prob ~ factor(id) + x - 1, data = agg_data,\n                      weights = n, family = \"binomial\")\n\n  results &lt;- bind_rows(\n    # GEE results\n    broom::tidy(mod1_sim) %&gt;%\n      mutate(data = 'raw data', time_secs = t1, estimator = \"GEE\"),\n    broom::tidy(mod2_sim) %&gt;%\n      mutate(data = 'aggregated data', time_secs = t3, estimator = \"GEE\"),\n    # GLM results\n    broom::tidy(mod1_sim_glm) %&gt;%\n      mutate(data = 'raw data', time_secs = NA_real_, estimator = \"GLM\"),\n    broom::tidy(mod2_sim_glm) %&gt;%\n      mutate(data = 'aggregated data', time_secs = NA_real_, estimator = \"GLM\")\n  ) %&gt;%\n    mutate(n = n, n_clus = n_clus, seed = seed) %&gt;%\n    filter(!stringr::str_detect(term, pattern = \"id\"))\n\n  return(results)\n}\n\nHere’s what the output from this function looks like:\n\nsimulate_data_and_get_estimates(n = 100, n_clus = 5, seed = 3)\n\n# A tibble: 6 × 11\n  term       estim…¹ std.e…² stati…³ p.value data  time_s…⁴ estim…⁵     n n_clus\n  &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercep…  -1.59    0.409 15.0    1.07e-4 raw …  0.00748 GEE       100      5\n2 x           -0.144   0.383  0.141  7.07e-1 raw …  0.00748 GEE       100      5\n3 (Intercep…  -1.73    0.395 19.1    1.27e-5 aggr…  0.00326 GEE       100      5\n4 x           -0.107   0.402  0.0702 7.91e-1 aggr…  0.00326 GEE       100      5\n5 x           -0.266   0.596 -0.446  6.55e-1 raw … NA       GLM       100      5\n6 x           -0.266   0.596 -0.446  6.55e-1 aggr… NA       GLM       100      5\n# … with 1 more variable: seed &lt;dbl&gt;, and abbreviated variable names ¹​estimate,\n#   ²​std.error, ³​statistic, ⁴​time_secs, ⁵​estimator\n\n\n\nsimulation_parameters &lt;- expand_grid(\n  n = c(100, 500, 1000, 2500, 5000, 7500, 10000),\n  n_clus = c(5, 50, 250),\n  seed = 1:20\n) %&gt;% \n  # remove runs where number of clusters is larger than total sample size\n  filter(n_clus &lt; n)\n\n# set up multicore processing\nplan(multisession, workers = 15)\n\n# parallelize purrr::pmap_dfr by using the {furrr} package\nsimulation_results &lt;- future_pmap_dfr(\n  .l = simulation_parameters,\n  # have to pass args as ..1 or ..2, else it fails\n  .f = ~ simulate_data_and_get_estimates(n = ..1, n_clus = ..2, seed = ..3),\n  .options = furrr_options(seed = NULL)\n)\n\nplan(sequential) # turn off multicore\n\nThis chunk can take a while to run for some of the parameter combinations, so pre-computed results are loaded and analyzed further.\n\nsimulation_results &lt;- read_csv(file = \"gee_glm_sim.csv\", show_col_types = FALSE) %&gt;% \n  mutate(\n    n_clus_raw = n_clus,\n    n_clus = factor(paste0(\"# clusters: \", n_clus),\n                    levels = paste0(\"# clusters: \", c(5, 50, 250))),\n    term = case_when(\n      term == \"(Intercept)\" ~ \"Intercept\",\n      term == \"x\" ~ \"Slope\", \n      TRUE ~ term\n  ))\n\nglimpse(simulation_results)\n\nRows: 2,400\nColumns: 12\n$ term       &lt;chr&gt; \"Intercept\", \"Slope\", \"Intercept\", \"Slope\", \"Slope\", \"Slope…\n$ estimate   &lt;dbl&gt; -7.17e-01, -7.38e-02, -5.35e-01, -1.14e-01, -5.92e-02, -5.9…\n$ std.error  &lt;dbl&gt; 4.69e-01, 1.75e-01, 4.09e-01, 1.53e-01, 4.86e-01, 4.86e-01,…\n$ statistic  &lt;dbl&gt; 2.3402, 0.1772, 1.7056, 0.5518, -0.1216, -0.1216, 7.1290, 4…\n$ p.value    &lt;dbl&gt; 1.26e-01, 6.74e-01, 1.92e-01, 4.58e-01, 9.03e-01, 9.03e-01,…\n$ data       &lt;chr&gt; \"raw data\", \"raw data\", \"aggregated data\", \"aggregated data…\n$ time_secs  &lt;dbl&gt; 0.01389, 0.01389, 0.00420, 0.00420, NA, NA, 0.00928, 0.0092…\n$ estimator  &lt;chr&gt; \"GEE\", \"GEE\", \"GEE\", \"GEE\", \"GLM\", \"GLM\", \"GEE\", \"GEE\", \"GE…\n$ n          &lt;dbl&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,…\n$ n_clus     &lt;fct&gt; # clusters: 5, # clusters: 5, # clusters: 5, # clusters: 5,…\n$ seed       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4,…\n$ n_clus_raw &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…"
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html#computation-time",
    "href": "posts/gee-on-aggregated-data/index.html#computation-time",
    "title": "GEE on aggregated data?",
    "section": "Computation time",
    "text": "Computation time\nFirst, the time taken to fit the model to the raw dataset is shown as a function of total sample size \\(n\\) where there are \\(n / n_{clus}\\) observations per cluster. Each panel has a different y-axis to allow reading values directly from the plot.\n\nsimulation_results %&gt;%\n  filter(data == \"raw data\", estimator == \"GEE\") %&gt;%\n  # distinct here since the data frame contains a row for\n  # each of the intercept and slope terms but the runtimes are the same\n  # for both these terms\n  distinct(seed, time_secs, n, n_clus) %&gt;%\n  ggplot(aes(x = n, y = time_secs, group = n_clus)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, degree = 3)) +\n  geom_smooth(method = \"lm\", formula = y ~ I(x ^ 3), color = \"forestgreen\") +\n  stat_summary(aes(group = interaction(n_clus, n)), geom = \"point\", \n               fun = mean, color = \"darkorange\", size = 2) + \n  xlab(\"Total sample size (n)\") +\n  ylab(\"Runtime (in seconds)\") +\n  facet_wrap(~ n_clus, scales = \"free\")\n\n\n\n\n\n\n\n\nThe blue line fits a cubic polynomial \\[time = \\beta_0 + \\beta_1 n + \\beta_2 n^2 + \\beta_3 n^3 + \\varepsilon\\] to the run times as a function of sample size. The green line fits the same model but without the linear and quadratic terms, i.e, \\(time = \\alpha_0 + \\alpha_1 n^3 + \\varepsilon\\). The yellow points are the mean run times for each level of \\(n\\) within each panel.\nWhen the number of clusters is very small, an increase in the sample size within the largest cluster leads to a cubic increase in expected run times. The closeness of the blue and green lines in the leftmost panel indicates that the cubic term in the execution time model dominates the lower order terms. In the second and third panels, the \\(O(n^3)\\) limit hasn’t hit yet, as the green and the blue lines are in disagreement.\nFor 10000 samples split into 2000 samples in each of the five clusters, it takes approximately 15 minutes per run. The same total sample size split into 250 samples in each of the 40 clusters, each model takes about 7 seconds to run, and even less time when there are 250 clusters. This is a massive difference in time between the different settings."
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html#convergence-issues",
    "href": "posts/gee-on-aggregated-data/index.html#convergence-issues",
    "title": "GEE on aggregated data?",
    "section": "Convergence issues",
    "text": "Convergence issues\nHowever, unfortunately some of the models run into convergence issues, where the point estimates and the standard errors explode\n\nsimulation_results %&gt;% \n  filter(estimator == \"GEE\") %&gt;% \n  arrange(desc(std.error)) %&gt;% \n  head(n = 10)\n\n# A tibble: 10 × 12\n   term      estimate std.e…¹ stati…² p.value data  time_…³ estim…⁴     n n_clus\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;fct&gt; \n 1 Intercept  4.24e16 1.22e16  12.1   5.12e-4 aggr… 0.00516 GEE     10000 # clu…\n 2 Intercept  1.13e17 8.91e15 161.    0       aggr… 0.00621 GEE      7500 # clu…\n 3 Intercept -1.99e17 7.88e15 641.    0       aggr… 0.0175  GEE      1000 # clu…\n 4 Intercept  2.32e15 7.25e15   0.102 7.49e-1 aggr… 0.00667 GEE      7500 # clu…\n 5 Intercept  5.32e15 6.59e15   0.650 4.20e-1 aggr… 0.00582 GEE       100 # clu…\n 6 Slope     -1.69e15 2.25e15   0.562 4.53e-1 aggr… 0.0110  GEE      2500 # clu…\n 7 Intercept  9.69e14 2.12e15   0.210 6.47e-1 aggr… 0.00558 GEE     10000 # clu…\n 8 Slope     -1.03e15 2.09e15   0.241 6.23e-1 aggr… 0.00763 GEE      7500 # clu…\n 9 Slope     -1.94e15 2.05e15   0.896 3.44e-1 aggr… 0.00503 GEE     10000 # clu…\n10 Slope      3.82e15 1.88e15   4.13  4.23e-2 aggr… 0.00817 GEE      5000 # clu…\n# … with 2 more variables: seed &lt;dbl&gt;, n_clus_raw &lt;dbl&gt;, and abbreviated\n#   variable names ¹​std.error, ²​statistic, ³​time_secs, ⁴​estimator\n\n\nThis seems to be a problem when training GEE models on aggregated data when the number of clusters is low irrespective of the total sample size, which leads to about 21% of the estimates from the aggregated data being convergence failures. An estimate is flagged as convergence failure when the stdandard error of the coefficients &gt; 5 on the logit scale.\n\nsimulation_results &lt;- simulation_results %&gt;%\n  mutate(high_se = as.numeric(std.error &gt; 5)) \n\nsimulation_results %&gt;%\n  filter(estimator == \"GEE\") %&gt;% \n  select(-estimate, -std.error, -statistic, -p.value, -time_secs) %&gt;%\n  # put intercept and slope estimates in one row\n  pivot_wider(everything(), names_from = term, values_from = high_se) %&gt;%\n  # if at least one of the intercept or slope terms have a very high se\n  mutate(high_se = pmin(Intercept + Slope, 1)) %&gt;%\n  group_by(data, n_clus, n) %&gt;%\n  summarize(n_total = n(), n_failed = sum(high_se), .groups = \"drop_last\") %&gt;%\n  #print(n = Inf) %&gt;%\n  summarize(n_total = sum(n_total), n_failed = sum(n_failed), .groups = \"drop\") %&gt;%\n  mutate(percent_failed = 100 * n_failed / n_total)\n\n# A tibble: 6 × 5\n  data            n_clus          n_total n_failed percent_failed\n  &lt;chr&gt;           &lt;fct&gt;             &lt;int&gt;    &lt;dbl&gt;          &lt;dbl&gt;\n1 aggregated data # clusters: 5       140       29           20.7\n2 aggregated data # clusters: 50      140        0            0  \n3 aggregated data # clusters: 250     120        0            0  \n4 raw data        # clusters: 5       140        0            0  \n5 raw data        # clusters: 50      140        0            0  \n6 raw data        # clusters: 250     120        0            0"
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html#raw-vs-aggregated-data-estimates",
    "href": "posts/gee-on-aggregated-data/index.html#raw-vs-aggregated-data-estimates",
    "title": "GEE on aggregated data?",
    "section": "Raw vs aggregated data estimates",
    "text": "Raw vs aggregated data estimates\nThese rows with convergence failures can be removed and the agreement between the estimates from the raw vs the aggregate datasets can be assessed visually\n\nslope_plot_data &lt;- simulation_results %&gt;%\n  filter(term == \"Slope\", high_se == 0) %&gt;%\n  select(seed, term, data, estimate, n_clus, n, estimator) %&gt;%\n  tidyr::pivot_wider(id_cols = c(seed, term, n_clus, n, estimator),\n                     names_from = data, values_from = estimate) %&gt;%\n  mutate(`Total sample size` = factor(n), \n         id = interaction(estimator, n_clus, sep = \", \", lex.order = TRUE)) \n\nslope_plot_data %&gt;% \n  filter(estimator == \"GEE\") %&gt;% \n  ggplot(aes(x = `raw data`, y = `aggregated data`, color = `Total sample size`)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  # geom_point(data = tibble(x = 0.3, y = 0.3), \n  #            aes(x = x, y = y), \n  #            color = \"black\", size = 3, inherit.aes = FALSE) +\n  xlab(\"Slope coefficient from the full dataset\") +\n  ylab(\"Slope coefficient from the aggregated dataset\") +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 1)) + \n  facet_wrap(~ id, ncol = 3, scales = \"fixed\")\n\nWarning: Removed 29 rows containing missing values (geom_point).\n\n\n\n\n\n\n\n\n\nThis plot compares the estimated slope coefficient (true value 0.3) from the GEE model on aggregated data vs the estimated slope coefficient from the model on the raw data while varying the number of clusters as well as the total sample size. The agreement between estimates increases as the number of clusters increases (going from the top left panel to the top right panel).\nOn the other hand, the estimates from running GLMs on the same aggregated and raw datasets are identical, as indicated by the points lying precisely on the black line in the following plot\n\nslope_plot_data %&gt;% \n  filter(estimator == \"GLM\") %&gt;% \n  ggplot(aes(x = `raw data`, y = `aggregated data`, color = `Total sample size`)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  # geom_point(data = tibble(x = 0.3, y = 0.3), \n  #            aes(x = x, y = y), \n  #            color = \"black\", size = 3, inherit.aes = FALSE) +\n  xlab(\"Slope coefficient from the full dataset\") +\n  ylab(\"Slope coefficient from the aggregated dataset\") +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 1)) + \n  facet_wrap(~ id, ncol = 3, scales = \"fixed\")\n\n\n\n\n\n\n\n\nAs expected, the estimates for \\(n = 10,000\\) are tightly clustered around the true value compared to \\(n = 100\\) in both sets of plots.\nOk so the estimates aren’t identical for the GEE models unfortunately, but are they approximately similar across the raw and aggregated datasets? This is assessed visually via box plots below for the case of 5 clusters in the data\n\ndist_plot_data &lt;- simulation_results %&gt;% \n  mutate(\n    ci_lower = estimate - (qnorm(0.975) * std.error),\n    ci_upper = estimate + (qnorm(0.975) * std.error), \n    ci_width = ci_upper - ci_lower, \n    ci_upper_half_width = ci_upper - estimate\n  ) %&gt;% \n  filter(term == \"Slope\", high_se == 0, estimator == \"GEE\") %&gt;% \n  select(\n    Estimate = estimate, \n    `Std. Error` = std.error, \n    `95% CI LL` = ci_lower, \n    `95% CI UL` = ci_upper,\n    `95% CI Width` = ci_width,\n    `95% CI Upper HW` = ci_upper_half_width,\n    n, n_clus_raw, n_clus, data, seed) %&gt;% \n  tidyr::pivot_longer(cols = Estimate:`95% CI Upper HW`, \n                      names_to = \"statistic\", \n                      values_to = \"values\") %&gt;% \n  mutate(\n    statistic = factor(statistic,\n                       levels = c(\"Estimate\", \"Std. Error\", \n                                  \"95% CI LL\", \"95% CI UL\", \n                                  \"95% CI Width\", \"95% CI Upper HW\")), \n    n = factor(n, ordered = TRUE), \n    data = stringr::str_to_title(data)\n  )\n\nglimpse(dist_plot_data)\n\nRows: 4,626\nColumns: 7\n$ n          &lt;ord&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,…\n$ n_clus_raw &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ n_clus     &lt;fct&gt; # clusters: 5, # clusters: 5, # clusters: 5, # clusters: 5,…\n$ data       &lt;chr&gt; \"Raw Data\", \"Raw Data\", \"Raw Data\", \"Raw Data\", \"Raw Data\",…\n$ seed       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3,…\n$ statistic  &lt;fct&gt; Estimate, Std. Error, 95% CI LL, 95% CI UL, 95% CI Width, 9…\n$ values     &lt;dbl&gt; -0.0738, 0.1753, -0.4174, 0.2698, 0.6871, 0.3436, -0.1137, …\n\n\n\ndist_plot_data %&gt;% \n  filter(n_clus_raw == 5) %&gt;% \n  ggplot(aes(x = n, y = values, color = data)) + \n  #geom_point(position = position_dodge(width = 0.2)) +\n  geom_boxplot(position = position_dodge(width = 1)) +\n  xlab(\"Total sample size (n)\") + \n  ylab(\"\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  facet_wrap(~ statistic, ncol = 2, scales = \"free\")\n\n\n\n\n\n\n\n\nSo based on these box plots, it seems like most of the sampling distributions for these statistics – point estimate, standard errors, CI width, etc. – are pretty comparable but not identical."
  },
  {
    "objectID": "posts/fitting-tweedie-models-to-claims-data/index.html",
    "href": "posts/fitting-tweedie-models-to-claims-data/index.html",
    "title": "Sampling from the poor man’s posterior distribution of parameters from models fitted to claims data",
    "section": "",
    "text": "This post is about fitting a handful of different models to a subset of a popular car-insurance claims data available online. Plausible future data values can be simulated from the fitted models and used for downstream tasks. Simulated values from the fitted models can also be compared with the actual observed data as a sanity check.\nI’ve never worked in the field of insurance, but I’ve been wanting to dive into Tweedie models for a while, since non-negative (response) variables with lots of zeroes and positive skew are pretty common and show up in many diverse disciplines such across the social sciences, insurance, biology, etc.\nWhat piqued my interest in insurance data is that the response variable can additionally have very large “outliers”. In other fields, outliers resulting from corrupted data or measurement errors can be discarded from the analysis, or robust estimators / loss functions can be used for modelling. However, in such settings, it may not necessarily make sense to discard such values because they likely represent the true cost of damages from accidents1 — which an insurer may be on the hook for.\nThis is also a bit of an unusual post in the sense that I’m using ideas I’ve encountered in Bayesian statistics but with frequentist methods. Sure, I could just fit Bayesian models, but that’s not the point here. To wrap my head around the ideas utilized in this post, I’m keeping things relatively simple by eschewing more complex predictive models like (my favourite) boosted trees, and using a single model for the data instead of hurdle models that split the response variable into a zero and a non-zero part and model them separately.\nBefore going further, I’m going to load some packages and the claims data used for this post.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# functions from these packages are used via namespace::fun() \n# library(tweedie)\n# library(magrittr, include.only = \"%$%\") # importing the exposition pipe\n# library(glue)\n# library(scales)\n# library(broom)\n# library(boot)\n# library(ggExtra)\n# library(statmod)\n\ntheme_set(theme_bw())\n\nclaims &lt;- read_csv(\"claims_subset.csv\", show_col_types = FALSE) %&gt;%\n  glimpse()\n\nRows: 60,000\nColumns: 13\n$ IDpol       &lt;dbl&gt; 1, 3, 5, 10, 11, 13, 15, 17, 18, 21, 25, 27, 30, 32, 35, 3…\n$ ClaimNb     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Exposure    &lt;dbl&gt; 0.10, 0.77, 0.75, 0.09, 0.84, 0.52, 0.45, 0.27, 0.71, 0.15…\n$ Area        &lt;chr&gt; \"D\", \"D\", \"B\", \"B\", \"B\", \"E\", \"E\", \"C\", \"C\", \"B\", \"B\", \"C\"…\n$ VehPower    &lt;dbl&gt; 5, 5, 6, 7, 7, 6, 6, 7, 7, 7, 7, 7, 4, 4, 4, 9, 6, 6, 6, 6…\n$ VehAge      &lt;dbl&gt; 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 1, 0, 9, 0, 2, 2, 2, 2…\n$ DrivAge     &lt;dbl&gt; 55, 55, 52, 46, 46, 38, 38, 33, 33, 41, 41, 56, 27, 27, 23…\n$ BonusMalus  &lt;dbl&gt; 50, 50, 50, 50, 50, 50, 50, 68, 68, 50, 50, 50, 90, 90, 10…\n$ VehBrand    &lt;chr&gt; \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B12\", \"B…\n$ VehGas      &lt;chr&gt; \"Regular\", \"Regular\", \"Diesel\", \"Diesel\", \"Diesel\", \"Regul…\n$ Density     &lt;dbl&gt; 1217, 1217, 54, 76, 76, 3003, 3003, 137, 137, 60, 60, 173,…\n$ Region      &lt;chr&gt; \"R82\", \"R82\", \"R22\", \"R72\", \"R72\", \"R31\", \"R31\", \"R91\", \"R…\n$ ClaimAmount &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\nThe most important variables are IDpol — which uniquely identifies individual policies (denoted with a subscript \\(i\\)), Exposure — which indicates the duration (in years) that the policy is in effect (denoted by \\(w_i\\)), and ClaimAmount — which is the total monetary amount of the claims filed by each policyholder (denoted by \\(z_i\\)). Some of the other variables will be later used for building richer models.\nLooking at the deciles of the exposure distribution for these 60,000 policies\nclaims %&gt;% \n  pull(Exposure) %&gt;% \n  quantile(., probs = seq(0, 1, 0.1)) %&gt;% \n  as_tibble(rownames = \"percentile\") %&gt;% \n  print(n = Inf)\n\n# A tibble: 11 × 2\n   percentile   value\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 0%         0.00273\n 2 10%        0.07   \n 3 20%        0.16   \n 4 30%        0.27   \n 5 40%        0.42   \n 6 50%        0.57   \n 7 60%        0.76   \n 8 70%        1      \n 9 80%        1      \n10 90%        1      \n11 100%       1\nmany of the values are less than one, which means those policies were in effect for less than a year. All of these are non-zero as they should be, but more than 60% have a duration of less than one year. The smallest exposure is for a day, since \\(1/365 \\approx 0.02739\\).\nAssuming a closed cohort — i.e., all these contracts get renewed the following year and no new contracts are issued — the goal is to predict the total claim amount for each policy for the following year. The distribution of individual claim amounts is highly skewed and has a lot of zeroes (\\(\\approx 94\\%\\)), as assessed by some quantiles\nclaims %&gt;% \n  pull(ClaimAmount) %&gt;% \n  quantile(., probs = c(0, 0.5, 0.94, 0.95, 0.99, 0.999, 1.0)) %&gt;% \n  round()\n\n     0%     50%     94%     95%     99%   99.9%    100% \n      0       0       0      54    1636   10839 1404186"
  },
  {
    "objectID": "posts/fitting-tweedie-models-to-claims-data/index.html#the-simplest-model",
    "href": "posts/fitting-tweedie-models-to-claims-data/index.html#the-simplest-model",
    "title": "Sampling from the poor man’s posterior distribution of parameters from models fitted to claims data",
    "section": "The simplest “model”",
    "text": "The simplest “model”\nSince I’ve worked in epidemiology these past few years, a natural quantity (i.e., estimand) similar to incidence rates (i.e., the number of events generated from some population divided by the total follow-up time for individuals with different follow-up durations or periods) seems to be a good starting step. This can be expressed as the sum of the individual claim amounts \\(z_i\\) divided by the sum of policy durations \\(w_i\\), and denoted by the expectation operator \\(\\mathbb{E}\\).\n\\[\n\\mathbb{E}\\big[\\text{Claim Amount}_i\\big] = \\frac{\\sum_i z_i}{\\sum_i w_i}\n\\]It’s a way of equally dividing the total claims generated from some population at risk among the individuals in that population. Each of the individual claim amounts is a random variable, so is expected to vary from person to person, as well as from year to year for the same person.\nTaking the regular (arithmetic) mean — which is the same as setting \\(w_i = 1\\) for each individual in the formula above — underestimates the expected claim cost as many individuals are observed for less than a year. It is expected that had they been observed for the full year, their claim amounts would have been larger. A similar argument applies in the case of individuals with \\(w_i &gt; 1\\), although in this case we’d be overestimating instead of underestimating.\nFor the data above, this comes out to about 125 per person per year assuming \\(w_i = 1\\), and to about 218 per person per year using the observed \\(w_i\\)’s.\n\nclaims %&gt;% \n  summarise(\n    total_amount = sum(ClaimAmount), \n    max_claim_amount = max(ClaimAmount),\n    n_policies = n(), \n    person_time = sum(Exposure), \n    mean_person_time = mean(Exposure)\n  ) %&gt;% \n  mutate(\n    mean = total_amount / n_policies, \n    avg_claim_cost = total_amount / person_time,\n    across(.cols = everything(), \n           .fns = ~ as.character(round(.x, 4)))\n  ) %&gt;% \n  pivot_longer(cols = everything())\n\n# A tibble: 7 × 2\n  name             value     \n  &lt;chr&gt;            &lt;chr&gt;     \n1 total_amount     7507466.02\n2 max_claim_amount 1404185.52\n3 n_policies       60000     \n4 person_time      34471.8546\n5 mean_person_time 0.5745    \n6 mean             125.1244  \n7 avg_claim_cost   217.7854  \n\n\nSo now we have an estimate for the expected claim amount per person per year, even though we can expect most policies to generate zero claims, and a few policies to generate some very large claim amounts based on the observed claims distribution. This expected claim cost estimate will be used again, so it’s assigned to a variable here.\n\nexpected_claim_amount &lt;- sum(claims$ClaimAmount) / sum(claims$Exposure)\n\nFor an insurance company, they need to ensure they have enough capital reserves to be able to pay out money for these claims, which they’re legally liable for. For the current year, the total claim amount across the policies was about 7.5 million, from a population that was on averaged exposed for 0.57 years. The total expected claim amount for this population with each person being observed for a full year is \\(217.8 \\times 60,000\\) which comes out to about 13 million.\nIn this case, the sample of customers constitutes the population of interest so it may not make sense to produce uncertainty estimates for this estimated total amount for the current year. However, the claim amounts can be seen as the result of a stochastic process, so superpopulation inference on this parameter can still make sense since next year’s claim amounts may be expected to be similar, but not identical.\nThis is carried out here using a weird Bayesian model (a.k.a. the nonparametric bootstrap). The following code chunk generates \\(B = 10,000\\) resamples and calculates the expected claim amount on each resample.\n\n\nCode\nexpected_claim_cost_fun &lt;- function(data, indx, ...) {\n  data &lt;- data[indx, ]\n\n  expected_value &lt;- sum(data$ClaimAmount) / sum(data$Exposure)\n\n  # this is the single / largest outlier in the data\n  outlier_counts &lt;- nrow(data[data$ClaimAmount == 1404185.52, ])\n\n  return(c(expected_value, outlier_counts))\n}\n\nboot_fun &lt;- function(data, R = 100, parallel = \"snow\") {\n  stopifnot(parallel %in% c(\"no\", \"snow\"))\n\n  # TRUE if using parallelization, otherwise FALSE\n  simple &lt;- parallel == \"snow\"\n\n  boot::boot(\n    data = data,\n    statistic = expected_claim_cost_fun,\n    R = R,\n    sim = \"ordinary\",\n    stype = \"i\",\n    simple = simple,\n    parallel = parallel,\n    ncpus = 18\n  )\n}\n\n# uncomment to run\n# boot_fit &lt;- boot_fun(data = claims, R = 10000)\n\n# uncomment to save the results \n# saveRDS(boot_fit, file = \"bootstrap_expected_claim_cost.rds\")\n\n\nThis can take a while to run (even in parallel), so saved results are read back in and used to produce the following plot of the sampling distribution of the expected claim amount.\n\n\nCode\nboot_fit &lt;- readRDS(\"bootstrap_expected_claim_cost.rds\")\n\n# convert the results into a data frame\nboot_dist &lt;- tibble(\n  expected_claim_cost = boot_fit$t[, 1],\n  outlier_counts = boot_fit$t[, 2],\n  # used for coloring / facetting plots\n  #`Outlier counts` = paste0(boot_fit$t[, 2], \" replicates\")\n  `Outlier counts` = factor(boot_fit$t[, 2])\n)\n\nboot_dist %&gt;%\n  ggplot(aes(x = expected_claim_cost)) +\n  geom_histogram(bins = 100) +\n  geom_vline(\n    xintercept = expected_claim_amount, \n    color = \"orange\", linewidth = 1.2, linetype = \"dashed\"\n  ) +\n  xlab(\n    glue::glue(\"Bootstrap distribution of expected \", \n               \"exposure-adjusted claim amount \", \n               \"per person per year\", \n               \"\\n(Estimate from the full sample in orange)\")\n  ) +\n  ylab(\"Count\")\n\n\n\n\n\n\n\n\n\nThat’s an unusual looking bootstrap distribution. Googling led me to this stackexchange thread which indicates similar behaviour arising due to outlier(s) and small sample sizes. Policies with the largest top-6 claim amounts are\n\n\nCode\nclaims %&gt;% \n  select(ClaimAmount) %&gt;% \n  arrange(desc(ClaimAmount)) %&gt;% \n  slice_head(n = 6) %&gt;% \n  pull(ClaimAmount)\n\n\n[1] 1404185.52  183073.66  152666.39  116318.90  115232.88   96422.32\n\n\nof which the largest at 1.4 million is roughly 8 times larger than the next largest value. It is interesting that despite having a large sample size of 60,000 (or 34,500 policy-years), this outlier is large enough relative to the sample size to cause the multimodality seen here.\nThe function used to calculate the expected claim amount on each bootstrap sample can be modified to also count the number of times the maximum value from the original sample shows up in each bootstrap sample. The bootstrap distribution is plotted again but this time colored by the number of times the maximum amount shows up in a replicate.\n\n\nCode\nboot_dist %&gt;%\n  ggplot(aes(x = expected_claim_cost,\n             fill = `Outlier counts`)) +\n  geom_histogram(bins = 100) +\n  geom_vline(xintercept = expected_claim_amount, color = \"gray40\",\n             linewidth = 1.2, linetype = \"dashed\") +\n  xlab(\n    glue::glue(\"Bootstrap distribution of expected \", \n               \"exposure-adjusted claim amount \", \n               \"per person per year\", \n               \"\\n(Estimate from the full sample in gray)\")\n  ) +\n  ylab(\"Count\") + \n  theme(\n    legend.position = \"inside\",\n    #legend.background = element_blank(),\n    legend.position.inside = c(0.88, 0.65)\n  )\n\n\n\n\n\n\n\n\n\nSo this makes sense. Resamples with more repeats of the maximum claim amount have higher expected claim cost amounts.\nSince the total claim amount for the following year is of interest, the bootstrap distribution can be multiplied by the number of policy holders to get a distribution of plausible values for the total claim amount for this population — which is expected to be around 13 million, but could be anywhere between 8 million to 28 million.\n\n\nCode\nnonparametric_bootstrap_totals &lt;- boot_dist %&gt;%\n  mutate(\n    total_claim_amount = 60000 * expected_claim_cost,\n    total_claim_amount_in_millions = total_claim_amount / 1e6,\n    method = \"Nonparametric Bootstrap (Weighted mean)\"\n  ) %&gt;%\n  select(method, total_claim_amount_in_millions)\n\nnonparametric_bootstrap_totals %&gt;%\n  ggplot(aes(x = total_claim_amount_in_millions)) +\n  stat_ecdf(pad = FALSE) +\n  geom_vline(xintercept = (6e4 * expected_claim_amount) / 1e6,\n             color = \"orange\", linewidth = 1.2, linetype = \"dashed\") +\n  scale_x_continuous(breaks = seq(8, 30, 2)) +\n  scale_y_continuous(labels = scales::label_percent()) +\n  xlab(\n    glue::glue(\"Plausible values for the \", \n                \"following year's\\ntotal claim \", \n                \"amount assuming unit exposure (in millions)\", \n               \"\\n(Estimate from the full sample in orange)\")\n  ) +\n  ylab(\n    glue::glue(\"Empirical distribution function of the\\n\", \n               \"bootstrap distribution of total claim amounts\")\n  )\n\n\n\n\n\n\n\n\n\nThe empirical (cumulative) distribution function (EDF or eCDF) for the bootstrap distribution of the total claim amount from the population under unit exposure is shown above. The strong multimodality shows up as bumps in the eCDF but these seem pretty minor.\nThis approach ignores all the predictors in the data, and assumes that everyone has the same risk and spreads out that risk across every individual equally. However, it’s expected that the risk may differ across factors such as driver’s age, car model characteristics, region, etc. For this, we need more advanced approaches in the form of regression models."
  },
  {
    "objectID": "posts/fitting-tweedie-models-to-claims-data/index.html#tweedie-models",
    "href": "posts/fitting-tweedie-models-to-claims-data/index.html#tweedie-models",
    "title": "Sampling from the poor man’s posterior distribution of parameters from models fitted to claims data",
    "section": "Tweedie models",
    "text": "Tweedie models\nA common probability model used to model claims data — the Tweedie distribution — is characterized by the power-law mean-variance relationship \\(\\text{Var}[Y_i] \\propto \\mathbb{E}[Y_i] ^ p\\) where the variance \\(\\text{Var}[Y_i]\\) is proportional to the mean \\(\\mathbb{E}[Y_i]\\). The Tweedie distribution with \\(p \\in (1, 2)\\) — also known as the compound Poisson(-gamma) distribution — is commonly used to model the total cost of claims from an insurance policy. In the insurance world, it can be derived at the policy level as a model of Poisson claim frequency with the sum of claim amounts (i.e., claim severity) coming from a gamma distribution.\nThe Tweedie distribution has three parameters — mean (\\(\\mu &gt; 0\\)), dispersion (\\(\\sigma &gt; 0\\)) which functions as the constant of proportionality, and variance function power \\(p \\in (-\\infty, 0] \\cup [1, \\infty)\\). Restricting to \\(p \\in (1, 2)\\) make sense here since we have policies with zero claim amounts in the data and \\(p \\notin(1, 2)\\) is not suitable for modelling data with exact zeroes. The extreme of \\(p = 1\\) corresponds to a Poisson distribution (which has support on non-negative integers), and \\(p = 2\\) corresponds to a gamma distribution (which has support on positive real numbers). Special cases are also the Gaussian distribution (\\(p = 0\\)), and the inverse-Gaussian distribution (\\(p = 3\\)).\nSection 4 of this paper has the clearest distinction I’ve seen between three related random variables for the \\(i^{th}\\) policy — \\(Z_i\\) is the observed total claim amount with exposure \\(w_i\\) (we have \\(z_i\\)’s in our data), \\(Y_i = Z_i / w_i \\sim \\text{Tweedie}(\\mu_i, \\phi / w_i, p)\\) is a derived quantity known as the pure premium under exposure \\(w_i\\), and \\(Y^*_i \\sim \\text{Tweedie}(\\mu_i, \\phi, p)\\) is the pure premium under unit exposure (so \\(w_i = 1\\)) which satisfies the mean-variance relationship \\(\\text{Var}[Y_i^*] = \\phi \\mathbb{E}[Y_i^*]^p\\). I’m treating \\(\\phi\\) as a constant here, but it can be modelled as a function of covariates, as is done in this paper for example.\nWhat tripped me up while writing a previous post2 on this topic was that I kept trying to take the weighted mean of the \\(z_i\\) values instead of the \\(y_i\\) values and getting different results compared with the ratio \\(\\sum_i z_i / \\sum_i w_i\\) when they should’ve been identical. Taking the weighted mean of the \\(y_i\\) values leads to the same estimate as the ratio of these sums because the \\(w_i\\)’s cancel out in the numerator.\n2 To be fair, I’d read the Yang et al. paper for that post too, but had overlooked that (subtle?) distinction.\nIntercept-only regression\nThere are two ways of accounting for the varying exposures in a model. The first method uses pure premium \\(y_i\\) as the response variable and the exposure \\(w_i\\)’s are passed as weights to the fitting functions. The second method uses the observed amount \\(z_i\\) with \\(w_i\\) as an offset (i.e., a variable in the model with a fixed coefficient of 1). For the Tweedie model with \\(p \\in (1, 2)\\), these two methods result in different parameter estimates (compared to the Poisson regression case where they give the same estimates). I’m sticking with the first approach here since more modelling packages across languages support model weights compared with offsets.\nThe following code fits an intercept-only Tweedie generalized linear model (GLM) with pure premium as the response variable and exposure as weights, and uses an identity link (link.power = 1). For this simple model, the link function shouldn’t really matter for the parameter estimate for the mean \\(\\mu\\). The variance power is taken to be \\(p = 1.6\\) throughout. This is very close to the chosen value for \\(p\\) from calling tweedie::tweedie.profile() using the model offset formulation described in the previous paragraph. I couldn’t get this function to work for the pure premium model with weights because the log-likelihood estimates were (negative) infinite for all \\(p\\), and \\(p = 1.8\\) was the value that minimized the Tweedie deviance (including on simulated data with known \\(\\mu\\), \\(\\phi\\), and \\(p = 1.6\\)).\n\ntweedie_intercept_only &lt;- glm(\n  I(ClaimAmount / Exposure) ~ 1,\n  weights = Exposure,\n  data = claims,\n  # using link.power = 1 implies identity link for the mean parameter mu\n  family = statmod::tweedie(var.power = 1.6, link.power = 1)\n)\n\nsummary(tweedie_intercept_only)\n\n\nCall:\nglm(formula = I(ClaimAmount/Exposure) ~ 1, family = statmod::tweedie(var.power = 1.6, \n    link.power = 1), data = claims, weights = Exposure)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    217.8       64.4   3.382 0.000721 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 25966.84)\n\n    Null deviance: 2144044  on 59999  degrees of freedom\nResidual deviance: 2144044  on 59999  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n\n\nSo our best guess for the distribution of \\(Y_i^*\\)’s from the Tweedie class of models is \\(\\text{Tweedie}(\\mu = 217.78..., \\phi = 25966.83..., p = 1.6...)\\) (where the ellipses indicate truncation). These estimates for \\(\\hat\\mu_{\\text{int}}\\) and \\(\\hat\\phi_{\\text{int}}\\) will be used again, so they’re stored as variables.\n\nmu &lt;- coef(tweedie_intercept_only)\nphi &lt;- tweedie_intercept_only %&gt;% \n  summary() %&gt;% \n  pluck(\"dispersion\")\n\n\n\nParametric Bootstrap\nOne way of getting a distribution for the statistic of interest \\(T\\) is the use of the parametric bootstrap. This involves drawing \\(B\\) samples each of size \\(n\\) from the fitted model, computing the statistic for each of the \\(B\\) samples, and using this as an estimate of the sampling distribution from which summary statistics (e.g. mean, quantiles, sd, etc.) can be computed.\nThe observed statistics — such as the number of claims with amounts &gt; 0, the total claim amounts, and the largest claim amount — from the one sample of \\(z_i\\)’s that we have can be compared with the sampling distribution from the parametric bootstrap using the observed exposures \\(w_i\\).\nThe following code carries this out with \\(B = 10,000\\) where for each \\(b\\) we draw a vector of \\(n\\) claim amounts \\(Z_{i, b}\\) and the statistic \\(\\hat{T_b}\\) is computed. \\(Z_{i, b}\\) is drawn as \\(Z_{i, b} \\sim w_i \\times \\text{Tweedie}(\\hat\\mu_{\\text{int}}, \\hat\\phi_{\\text{int}} / w_i, 1.6)\\).\n\n\nCode\n# how well does the model fit the data?\n# a form of posterior predictive checking\n# ppc_obs_exposure &lt;- map(\n#   .x = 1:10000,\n#   .f = ~ {\n#     if(.x %% 100 == 0) {\n#       print(.x)\n#     }\n#     set.seed(.x)\n#     draw &lt;- claims$Exposure * tweedie::rtweedie(60000, mu = mu,\n#                                                 phi = phi / claims$Exposure,\n#                                                 power = 1.6)\n#     tibble(prop_zero = mean(draw == 0), sample_total = sum(draw),\n#            sample_max = max(draw), n_nonzero = sum(draw &gt; 0))\n#   }) %&gt;%\n#   list_rbind()\n#\n# saveRDS(ppc_obs_exposure, file = \"ppc_obs_exposure.rds\")\n\n# code takes a while to run, so we can read in the saved results\nppc_obs_exposure &lt;- readRDS(file = \"ppc_obs_exposure.rds\")\n\n# summary statistics on the observed data\nsample_statistics_obs_exposure &lt;- claims %&gt;%\n  summarise(\n    prop_zero = mean(ClaimAmount == 0),\n    sample_total = sum(ClaimAmount),\n    sample_max = max(ClaimAmount),\n    n_nonzero = sum(ClaimAmount &gt; 0)\n  )\n\n# combine the ppc data and the original sample data\nplot_data_obs_exposure &lt;- bind_rows(\n  ppc_obs_exposure %&gt;% \n    mutate(group = \"sampled\", .before = 0),\n  sample_statistics_obs_exposure %&gt;% \n    mutate(group = \"observed\", .before = 0)\n  ) %&gt;%\n  mutate(\n    across(c(sample_total, sample_max), ~ .x / 1e6),\n    prop_zero = 100 * prop_zero\n  ) %&gt;%\n  rename(\n    `% of policies with zero claims` = prop_zero,\n    `Number of policies with non zero claim amounts` = n_nonzero,\n    `Maximum claim amount (in millions)` = sample_max,\n    `Total claim amount (in millions)` = sample_total\n  ) %&gt;%\n  pivot_longer(cols = -group, names_to = \"statistic\", values_to = \"values\")\n\n# mean of the posterior predictive distribution values\nppc_mean_obs_exposure &lt;- plot_data_obs_exposure %&gt;%\n  filter(group == \"sampled\") %&gt;%\n  summarise(values = mean(values), .by = statistic) %&gt;%\n  mutate(across(.cols = where(is.numeric), \n                .fns = ~ round(.x, 2)))\n\n# compare these visually\nplot_data_obs_exposure %&gt;%\n  filter(group == \"sampled\") %&gt;%\n  ggplot(aes(x = values, group = statistic)) +\n  stat_ecdf(pad = FALSE) +\n  # plot the sample statistic\n  geom_vline(data = filter(plot_data_obs_exposure,\n                           group == \"observed\"),\n             aes(xintercept = values, group = statistic),\n             color = \"orange\", linewidth = 1.2, linetype = \"dashed\") +\n  # plot the distribution means\n  geom_vline(data = ppc_mean_obs_exposure,\n             aes(xintercept = values, group = statistic),\n             color = \"red4\", linewidth = 1.2, linetype = \"dashed\") +\n  facet_wrap(~ statistic, scales = \"free\") +\n  scale_y_continuous(labels = scales::label_percent()) +\n  xlab(glue::glue(\"Statistics from the observed data (in orange);\",\n                  \"\\nmean of means from the simulated datasets (in red)\")) +\n  ylab(\"Empirical distribution function\")\n\n\n\n\n\n\n\n\n\nWhat’s really interesting about this plot is that the proportion (and number) of policies with zero claim amounts is really far from the observed proportion (number) of 95% (3000) vs the mean of the sampling distribution (99% and 29). On the other hand, the observed and average total claim amounts are virtually indistinguishable, and the average maximum claim amount isn’t too far off from the observed maximum claim amount.\nGiven that the statistic of interest has been the total claim amount across the policies, this doesn’t seem to be a bad approach, even though the EDFs of the sampled \\(Z_{i, b} &gt; 0\\)’s from the model (10 datasets shown in gray) look pretty far off from the observed distribution of \\(Z_{i, \\text{obs}} &gt; 0\\)’s (observed claim amounts in black)\n\n\nCode\n# plot eCDFs of sampled datasets with the observed data\nmap(\n  .x = 1:10,\n  .f = ~ {\n    set.seed(.x)\n    draw &lt;- claims$Exposure * tweedie::rtweedie(60000, mu = mu,\n                                                phi = phi / claims$Exposure,\n                                                power = 1.6)\n    tibble(sim_id = .x, y = draw, grp = \"Simulated\")\n  }) %&gt;%\n  list_rbind() %&gt;%\n  bind_rows(\n    .,\n    claims %&gt;%\n      mutate(sim_id = 100, grp = \"Observed\") %&gt;%\n      select(sim_id, y = ClaimAmount, grp)\n  ) %&gt;%\n  filter(y &gt; 0) %&gt;%\n  ggplot(aes(x = y, group = sim_id, color = grp)) +\n  stat_ecdf() +\n  xlab(\"Policies with non-zero claim amounts (log10 scale)\") +\n  ylab(\"Empirical distribution function\") +\n  scale_x_log10(\n    breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n    labels = scales::trans_format(\"log10\", scales::math_format(10^.x))\n  ) +\n  annotation_logticks(sides = \"b\") +\n  scale_y_continuous(labels = scales::percent) +\n  scale_color_manual(\n    values = c(\"Simulated\" = \"gray70\", \"Observed\" = \"Black\")\n  ) +\n  theme(\n    legend.title = element_blank(), \n    legend.position = \"inside\",\n    legend.position.inside = c(0.9, 0.2)\n  )\n\n\n\n\n\n\n\n\n\nSimulated claim amounts are orders of magnitude larger than the observed claim amounts, which is how the totals are very similar despite the very different sample sizes of policies with non-zero claim amounts. The very low number of non-zero claim amounts in the simulated datasets is what leads to the jaggedness of the distribution functions.\nIf the goal is to have accurate estimates for the proportion of non-zero claims, then a hurdle model mentioned earlier would be a better approach. The observed distribution has a pretty big jump at 1128.12 which shows up 1169 times in the data, so a mixture model might be more appropriate if want to have a model with approximately the same eCDF.\nThis approach of comparing the statistics on samples drawn from the model vs the statistics from the observed sample is the concept of posterior predictive checking (PPC). If the model adequately describes the data generating process, the distribution of all the statistics from sampled datasets should be approximately centered at the observed statistics.\nFor next year’s claim amount, we can repeat the same step with a slight modification — sample \\(Y_{i, b}^* \\sim \\text{Tweedie}(\\hat\\mu_{\\text{int}}, \\hat\\phi_{\\text{int}}, p = 1.6)\\) and calculate \\(\\hat{T_b} = \\sum_i Y_{i, b}^*\\) which ranges from about 4 to 27.5 million.\n\n\nCode\n# predicted_totals_for_unit_exposure &lt;- map_dbl(.x = 1:10000, .f = ~ {\n#   if(.x %% 100 == 0) {\n#     print(.x)\n#   }\n#   set.seed(.x)\n#   sum(tweedie::rtweedie(60000, mu = mu, phi = phi, power = 1.6))\n# })\n#\n# saveRDS(predicted_totals_for_unit_exposure,\n#         file = \"predicted_totals_for_unit_exposure.rds\")\n\npredicted_totals_for_unit_exposure &lt;- readRDS(\n  file = \"predicted_totals_for_unit_exposure.rds\"\n)\n\nsummary(predicted_totals_for_unit_exposure)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 3904985 11011626 12889186 13044586 14930666 27255385 \n\n\n\n\nNonparametric Bootstrap\nThe parametric bootstrap uses the point estimate to simulate new samples. From a Bayesian point of view, this corresponds to using the Dirac delta posterior distribution with a spike at the point estimate \\((\\hat\\mu_{\\text{int}}, \\hat\\phi_{\\text{int}})\\) and ignoring the uncertainty in the parameter estimates by treating it as zero. This section extends the parametric bootstrap approach by accounting for the uncertainty in the \\((\\hat\\mu_{\\text{int}}, \\hat\\phi_{\\text{int}})\\) values.\nIn the first stage, a separate Tweedie model is fit to each bootstrapped sample from the original data and the estimated \\((\\hat\\mu_{\\text{int}, b}, \\hat\\phi_{\\text{int}, b})_{b = 1}^{10,000}\\) pairs are collected. These 10,000 pairs are a sample from the “poor man’s” posterior distribution for these parameters and are visualized here\n\n\nCode\n# bootstrap and get the joint distribution of (mu, phi)\n# bootstrap_mu_phi &lt;- map_dfr(\n#   .x = 1:10000,\n#   .f = ~ {\n#     if(.x %% 100 == 0) {\n#       print(.x)\n#     }\n#     set.seed(.x)\n#     data &lt;- claims %&gt;%\n#       slice_sample(n = 60000, replace = TRUE)\n#     outlier_counts &lt;- nrow(data[data$ClaimAmount == 1404185.52, ])\n#     mod &lt;- data %&gt;%\n#       glm(\n#         I(ClaimAmount / Exposure) ~ 1,\n#         weights = Exposure,\n#         data = .,\n#         family = statmod::tweedie(var.power = 1.6, link.power = 0)\n#       )\n#\n#     tibble(\n#       mu = exp(coef(mod)),\n#       phi = summary(mod)$dispersion,\n#       outlier_counts = outlier_counts\n#     )\n#   }\n# )\n#\n# saveRDS(bootstrap_mu_phi, file = \"bootstrap_mu_phi.rds\")\n\nbootstrap_mu_phi &lt;- readRDS(\"bootstrap_mu_phi.rds\")\n\nmu_phi_plot &lt;- bootstrap_mu_phi %&gt;%\n  mutate(`Outlier counts` = factor(outlier_counts)) %&gt;%\n  ggplot(aes(x = mu, y = phi, color = `Outlier counts`)) +\n  geom_point() +\n  geom_point(data = tibble(mu = mu, phi = phi),\n             aes(x = mu, y = phi),\n             color = \"gray20\", size = 5, inherit.aes = FALSE) +\n  labs(\n    x = glue::glue(\"Mean (\\u03bc)\\n\", \n                   \"Point estimates for mean and\", \n                   \" dispersion from the full sample \",\n                   \"shown in black\"),\n    y = \"Dispersion (\\u03d5)\"\n  ) +\n  theme(\n    legend.position = \"inside\",\n    legend.background = element_blank(),\n    legend.position.inside = c(0.8, 0.18)\n  ) +\n  guides(color = guide_legend(nrow = 2))\n\n# this thorws warnings that bins are ignored, but \n# the correct behaviour is observed anyway\nggExtra::ggMarginal(\n  p = mu_phi_plot, type = \"densigram\", \n  xparams = list(bins = 100), \n  yparams = list(bins = 100)\n)\n\n\n\n\n\n\n\n\n\nThe clustering of points was initially a bit puzzling. The marginal density for the mean (top) is the same as the multimodal bootstrap distribution from a few sections above. The density for dispersion seems to be a lot less well-behaved. Coloring by the number of times that the original sample maximum claim amount is sampled in the bootstrap datasets, it’s clear that the frequency of occurrence is largely responsible for the clustering observed here.\nIn the second stage, a random pair \\((\\hat\\mu_{\\text{int}, b}, \\hat\\phi_{\\text{int}, b})\\) is drawn first, followed by drawing \\(n\\) values \\(Y_{i, b}^* \\sim \\text{Tweedie}(\\hat\\mu_{\\text{int}, b}, \\hat\\phi_{\\text{int}, b}, p = 1.6)\\), and summing these to get an estimate of the total claim amount \\(\\hat{T_b}\\).\nIt is expected that accounting for uncertainty in the estimation of \\((\\hat\\mu_{\\text{int}}, \\hat\\phi_{\\text{int}})\\) should lead to thicker tails for the distribution of \\(\\hat{T_b}\\)’s. The right tail now extends beyond 28 million and goes up to 34 million.\n\n\nCode\n# sample Y_i from the posterior predictive distribution and sum them\n# do this 10k times to get the posterior distribution\n# posterior_distribution_samples_for_total_claims &lt;- map(\n#   .x = 1:10000,\n#   .f = ~ {\n#     if(.x %% 100 == 0) {\n#       print(.x)\n#     }\n#     set.seed(.x)\n#     draw &lt;- bootstrap_mu_phi %&gt;%\n#       slice_sample(n = 1)\n#\n#     set.seed(.x)\n#     sum_values &lt;- draw %$%\n#       tweedie::rtweedie(n = 60000, mu = mu, phi = phi, power = 1.6) %&gt;%\n#       sum()\n#\n#     draw %&gt;%\n#       mutate(total = sum_values)\n#   }\n# ) %&gt;%\n#   list_rbind()\n#\n# saveRDS(posterior_distribution_samples_for_total_claims,\n#         file = \"posterior_distribution_samples_for_total_claims.rds\")\n\nposterior_distribution_samples_for_total_claims &lt;- readRDS(\n  file = \"posterior_distribution_samples_for_total_claims.rds\"\n)\n\nsummary(posterior_distribution_samples_for_total_claims)\n\n\n       mu             phi              total         \n Min.   :144.3   Min.   :  883.9   Min.   : 3244481  \n 1st Qu.:182.1   1st Qu.:16455.5   1st Qu.:10286795  \n Median :213.6   Median :24801.9   Median :12234351  \n Mean   :218.2   Mean   :23607.4   Mean   :13061045  \n 3rd Qu.:243.8   3rd Qu.:31349.8   3rd Qu.:15142596  \n Max.   :452.7   Max.   :68572.1   Max.   :34585477  \n\n\n\n\nUsing properties of EDMs\nThis section uses two properties — sampling distribution of the weighted average, and scale invariance — of exponential dispersion models (EDMs) of which Tweedie distributions are a special case.\nThe main statistic of interest has been the weighted mean \\(T_{wm}\\) of the pure premium values (\\(Y_i = Z_i / w_i\\))\n\\[\nT_{wm} = w_{\\bullet}^{-1} \\sum_{i = 1}^n w_i Y_i\n\\]\nwhere \\(w_{\\bullet} = \\sum_i w_i\\) is the sum of the exposures. If \\(Y_i \\sim \\text{Tweedie}(\\mu, \\phi / w_i, p)\\), then the sampling distribution of the weighted mean is also a Tweedie distribution with the same mean but with the dispersion \\(\\phi\\) scaled by the total exposure, i.e., \\(T_{wm} \\sim \\text{Tweedie}(\\mu, \\phi / w_{\\bullet}, p)\\).\nThe scale invariance property says that\n\\[\nc \\text{Tweedie}(\\mu, \\phi, p) = \\text{Tweedie}(c \\mu, c^{2-p} \\phi, p)\n\\]\nCombining these two and setting \\(c = 60,000\\) since we’re interested in the total claim amount across the policies, we can write\n\\[\nT_\\text{total} \\sim \\text{Tweedie}(60000 \\mu, 60000^{2-p} \\phi / w_{\\bullet}, p)\n\\]\nThis is much simpler and faster than the parametric bootstrap (for the weighted mean \\(T_{wm}\\)) since we don’t need to sample the vector of \\(Y_{i, b}^*\\)’s as an intermediate step3.\n3 Of course the parametric bootstrap is much more general, so it’s not really a fair comparison.\ntotal_exposure &lt;- claims %&gt;% \n  pull(Exposure) %&gt;% \n  sum()\n\nset.seed(43)\npredicted_totals_tweedie_sampling_dist &lt;- tweedie::rtweedie(\n  n = 10000,\n  mu = 60000 * mu,\n  phi = ((60000 ^ (2 - 1.6)) * phi) / total_exposure,\n  power = 1.6\n)\n\npredicted_totals_tweedie_sampling_dist %&gt;% summary()\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 2848259 10276333 12713295 13030839 15472733 32389786 \n\n\n\n\nGLM with predictors\nSo far, we’ve ignored all other variables in the data that provide information at the level of the policyholder, and modelled the marginal distribution of the pure premiums. Using this so-called collective model can be contrasted with the individual model where information at the policy level can be used for modelling individual risk.\nThe simplest extension to the intercept-only model is the main effects model which includes all the additional variables at the policy level in the model\n\\[\n\\begin{align*}\nY_i &\\sim \\text{Tweedie}(\\mu_i = \\text{exp}(\\text{log}(E[Y_i])), \\phi, p) \\\\\n\\text{log}(E[Y_i]) &= \\beta_0 + \\beta_1 x_{i, 1} + \\dots + \\beta_p x_{i, p}\n\\end{align*}\n\\]\nFor simplicity, linearity and additivity of the predictors are assumed on the log scale, which leads to a multiplicative model on the original scale. This has the additional advantage of ensuring that the expected values can never be less than 0 (since \\(\\mu &gt; 0\\)). Using boosted trees (e.g. LightGBM) would be a natural next step for improvement as they can model interactions, carry out variable selection by dropping terms that don’t impact risk, don’t impose functional form restrictions, etc. and can lead to more accurate predictions.\nThe following code fits a Tweedie GLM to the pure premium values \\(y_i\\) with exposure weights \\(w_i\\) using the log link (link.power = 0) and fixing \\(p = 1.6\\).\n\nclaims_modelling &lt;- claims %&gt;%\n  mutate(\n    across(c(Area, VehPower, VehBrand, VehGas, Region), ~ as.factor(.x)),\n    pure_premium = ClaimAmount / Exposure\n  )\n\nmain_effects_glm &lt;- glm(\n  pure_premium ~ 1 + Area + VehPower + VehAge + DrivAge\n  + BonusMalus + VehBrand + VehGas + Density + Region,\n  weights = Exposure,\n  data = claims_modelling,\n  family = statmod::tweedie(var.power = 1.6, link.power = 0)\n)\n\nGiven the increase in complexity of this model and to eventually try more complex models, it would be good to switch to cross-validation to see how well this model performs on unseen data. I’ll get to this in a future post.\nIt can be informative to explore the fitted values.\n\n\nCode\nfitted_values &lt;- main_effects_glm %&gt;%\n  broom::augment(newdata = claims_modelling, type.predict = \"response\")\n\nmean_fitted &lt;- fitted_values %&gt;%\n  pull(\".fitted\") %&gt;%\n  mean()\n\nfitted_values %&gt;%\n  pull(.fitted) %&gt;%\n  quantile(\n    probs = c(0, 0.25, 0.5, 0.75, 0.95, 0.99, 1.0)\n  ) %&gt;%\n  c(., \"Mean\" = mean_fitted) %&gt;%\n  sort() %&gt;%\n  round(., 2)\n\n\n       0%       25%       50%       75%      Mean       95%       99%      100% \n     6.20     87.99    138.68    247.29    256.30    764.44   1763.00 306296.00 \n\n\nThe least risky policy has an expected value of 6.2 and 50% of the policies have an expected value less than 140. Fewer than 1% of the policies have a risk larger than 2000, but the riskiest policy has an expected value of about 300,000. Since this is a simple main effects model, it’s easy enough to see which term(s) contribute towards these very high values for the top-10 policies with the largest pure premium values\n\n# get the term-wise contribution to the prediction on the\n# link scale for the top k largest predictions\ntop_10_largest_policy_predictions &lt;- fitted_values %&gt;%\n  arrange(desc(.fitted)) %&gt;%\n  slice_head(n = 10)\n\ntop_10_largest_policy_predictions %&gt;% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDpol\nClaimNb\nExposure\nArea\nVehPower\nVehAge\nDrivAge\nBonusMalus\nVehBrand\nVehGas\nDensity\nRegion\nClaimAmount\npure_premium\n.fitted\n\n\n\n\n41847\n1\n0.24\nC\n7\n12\n60\n228\nB1\nRegular\n300\nR53\n1128.12\n4700.500\n306296.00\n\n\n70558\n0\n0.30\nE\n6\n12\n24\n195\nB1\nRegular\n3103\nR82\n0.00\n0.000\n75259.17\n\n\n40910\n0\n0.60\nB\n12\n19\n39\n156\nB11\nRegular\n95\nR82\n0.00\n0.000\n23306.65\n\n\n89044\n0\n0.39\nE\n4\n10\n26\n173\nB1\nRegular\n3744\nR93\n0.00\n0.000\n22336.77\n\n\n62898\n1\n1.00\nE\n7\n10\n80\n173\nB2\nRegular\n3688\nR82\n1128.12\n1128.120\n20961.85\n\n\n113887\n1\n0.26\nD\n6\n2\n28\n156\nB1\nRegular\n1943\nR24\n1128.12\n4338.923\n18147.85\n\n\n41513\n2\n1.00\nD\n5\n3\n30\n196\nB2\nRegular\n1284\nR25\n1830.85\n1830.850\n17029.99\n\n\n102276\n0\n0.75\nE\n6\n8\n25\n156\nB3\nDiesel\n3021\nR53\n0.00\n0.000\n16095.04\n\n\n32880\n0\n1.00\nC\n4\n17\n47\n177\nB2\nRegular\n105\nR24\n0.00\n0.000\n14837.72\n\n\n58572\n0\n0.05\nE\n7\n5\n21\n156\nB1\nRegular\n4762\nR93\n0.00\n0.000\n14193.21\n\n\n\n\ntop_10_largest_policy_predictions &lt;- predict(\n  object = main_effects_glm, \n  newdata = top_10_largest_policy_predictions, \n  type = \"terms\"\n)\n\n# convert the 'terms' data frame to tibble and add constant value\n# then pivot to get the exp(cumsum()) for each policy (i.e. row)\n# then pivot back\n# there's probably some neat function that does \n# this in fewer lines of code\ntop_10_largest_policy_predictions %&gt;%\n  as_tibble() %&gt;%\n  mutate(\n    id = row_number(),\n    # for the calculation of the constant value, see this\n    # https://stackoverflow.com/questions/37963904/what-does-predict-glm-type-terms-actually-do\n    constant = attr(top_10_largest_policy_predictions, \"constant\"),\n    .before = 0\n  ) %&gt;%\n  pivot_longer(cols = -id) %&gt;%\n  group_by(id) %&gt;%\n  mutate(value = exp(cumsum(value))) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(id_cols = id, names_from = name, values_from = value) %&gt;%\n  mutate(across(.cols = everything(), .fns = round)) %&gt;% \n  select(-id) %&gt;%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconstant\nArea\nVehPower\nVehAge\nDrivAge\nBonusMalus\nVehBrand\nVehGas\nDensity\nRegion\n\n\n\n\n157\n169\n141\n125\n142\n188272\n216443\n218766\n230584\n306296\n\n\n157\n216\n311\n274\n234\n75132\n86374\n87300\n81568\n75259\n\n\n157\n116\n275\n201\n193\n11582\n23503\n23755\n25260\n23307\n\n\n157\n216\n143\n134\n116\n14426\n16585\n16763\n15236\n22337\n\n\n157\n216\n180\n168\n223\n27859\n24670\n24935\n22719\n20962\n\n\n157\n153\n220\n254\n224\n13433\n15443\n15609\n15330\n18148\n\n\n157\n153\n126\n142\n127\n42714\n37826\n38231\n38627\n17030\n\n\n157\n216\n311\n306\n262\n15756\n13100\n12922\n12117\n16095\n\n\n157\n169\n112\n87\n89\n13175\n11667\n11792\n12534\n14838\n\n\n157\n216\n180\n192\n160\n9577\n11010\n11128\n9681\n14193\n\n\n\n\n\nLooking at the contribution of each term for a given policy shows that high values of BonusMalus (with a coefficient of 0.043 on the log scale) has the largest impact on pushing up the predicted pure premiums. Obviously, for more complex models, SHAP plots would provide similar information in terms of identifying features that contribute to very high (or low) predicted values for the policies of interest.\nThe distribution of fitted values can be visualized too\n\n\nCode\nfitted_values %&gt;%\n  ggplot(aes(x = .fitted)) +\n  stat_ecdf() +\n  geom_vline(xintercept = mu, color = \"orange\", linetype = \"dashed\") +\n  annotate(geom = \"text\", x = 200, y = 0.8, color = \"orange\",\n           label = \"Marginal mean: 217.8\", hjust = \"right\") +\n  geom_vline(xintercept = mean_fitted, color = \"red4\", linetype = \"dashed\") +\n  annotate(geom = \"text\", x = 275, y = 0.5, color = \"red4\",\n           label = \"Fitted values mean: 256.3\", hjust = \"left\") +\n  scale_x_log10(\n    breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n    labels = scales::trans_format(\"log10\", scales::math_format(10^.x))\n  ) +\n  annotation_logticks(sides = \"b\") +\n  scale_y_continuous(labels = scales::percent) +\n  xlab(\"Fitted pure premium values\") +\n  ylab(\"Empirical distribution function\")\n\n\n\n\n\n\n\n\n\nThe overall mean from the intercept only model and the mean of the fitted values don’t coincide, which I wasn’t expecting given the law of iterated expectation (LIE) where \\(E[Y] = E[E[Y | X]]\\) holds. After puzzling over this for a bit and looking around on the internet, this seems to be due to the use of the non-canonical log-link function \\(g(\\mu) = \\text{log}(\\mu)\\). Using the canonical link function for a Tweedie distribution \\(g(\\mu) = \\mu^{(1 - p)} / (1-p)\\) results in LIE holding, but attempting to fit the main effects model with the canonical link fails in R.\n\nmain_effects_glm %&gt;% \n  update(\n    # the default link function is the canonical link function\n    family = statmod::tweedie(var.power = 1.6)\n  )\n\nError: no valid set of coefficients has been found: please supply starting values\n\n\nThe disadvantages of using the canonical link for Tweedie models are numerical instability, and interpretation issues (since risks are not multiplicative as a function of predictors).\nFor simulating the sampling distribution of total claims across the policies, the same approach as the one from the parametric bootstrap section can be applied using the usual Pearson estimate of scale \\(\\phi\\).\n\nphi_glm &lt;- main_effects_glm %&gt;% \n  summary() %&gt;% \n  pluck(\"dispersion\") %&gt;% \n  print()\n\n[1] 4529.461\n\n\nThe estimated value of \\(\\hat\\phi_{\\text{main}} \\approx 4529\\) is much smaller compared to the intercept-only model with value \\(\\hat\\phi_{\\text{int}} \\approx 25966\\). Sampling 10,000 datasets of size 60,000 with policy-specific conditional means from the individual model \\(\\hat\\mu_i\\)’s and \\(\\hat\\phi_{\\text{main}}\\) and summing the values for each dataset now gives estimates ranging from 9 million to 52 million. The maximum possible claim amount is much much higher here compared to the 35 million from the collective model.\n\n\nCode\n# simulate 10,000 times the total claim amounts for the full year of exposure\n# using the fitted means\n# predicted_totals_from_glm &lt;- map_dbl(.x = 1:10000, .f = ~ {\n#   if(.x %% 100 == 0) {\n#     print(.x)\n#   }\n#   set.seed(.x)\n#   sum(tweedie::rtweedie(60000, mu = fitted_values$.fitted, \n#                         phi = phi_glm, power = 1.6))\n# })\n#\n# saveRDS(predicted_totals_from_glm,\n#         file = \"predicted_totals_from_glm.rds\")\n\npredicted_totals_from_glm &lt;- readRDS(\n  file = \"predicted_totals_from_glm.rds\"\n)\n\nsummary(predicted_totals_from_glm)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 9179960 13812465 15097242 15442905 16590660 52011640 \n\n\nSample statistics for the observed data with exposure \\(w_i\\) can be compared with the statistics from the intercept-only and main-effects GLMs as a form of PPC. Looking at the summary statistics for the main effects model, there are more policies with non-zero claims (266 on average) compared with the PPC from the intercept only model (28 on average), but still very far off from the 3051 that are in the observed data.\n\n\nCode\n# quick check of sampling dist of statistics\n# ppc_glm_obs_exposure &lt;- map(\n#   .x = 1:1000,\n#   .f = ~ {\n#     if(.x %% 50 == 0) {\n#       print(.x)\n#     }\n#     set.seed(.x)\n#     draw &lt;- (fitted_values$Exposure *\n#                tweedie::rtweedie(60000,\n#                                  mu = fitted_values$.fitted,\n#                                  phi = phi_glm, power = 1.6))\n#\n#     tibble(prop_zero = mean(draw == 0), sample_total = sum(draw),\n#            sample_max = max(draw), n_nonzero = sum(draw &gt; 0))\n#   }) %&gt;%\n#   list_rbind()\n#\n# saveRDS(ppc_glm_obs_exposure, file = \"ppc_glm_obs_exposure.rds\")\n\nppc_glm_obs_exposure &lt;- readRDS(file = \"ppc_glm_obs_exposure.rds\")\n\n# function to convert the output of summary.data.frame() to a tibble\ntidy_df_summary &lt;- function(data) {\n  data %&gt;%\n    summary() %&gt;%\n    as.data.frame() %&gt;%\n    separate(col = Freq, into = c(\"Statistic\", \"Value\"), sep = \":\") %&gt;%\n    select(-Var1) %&gt;%\n    rename(Column = Var2) %&gt;%\n    pivot_wider(names_from = Statistic, values_from = Value) %&gt;%\n    mutate(\n      Column = str_trim(Column, side = \"left\"),\n      across(\n        .cols = where(is.character),\n        .fns = ~ str_trim(.x, side = \"both\")\n      )\n    )\n}\n\nlist(\n  \"Intercept\" = ppc_obs_exposure,\n  \"Main effects\" = ppc_glm_obs_exposure,\n  \"Observed\" = sample_statistics_obs_exposure\n) %&gt;%\n  imap(\n    .f = ~ {\n      .x %&gt;%\n        tidy_df_summary() %&gt;%\n        mutate(Method = .y, .before = 1)\n    }\n  ) %&gt;%\n  list_rbind() %&gt;%\n  mutate(\n    Method = factor(Method, \n                    levels = c(\"Observed\", \"Intercept\", \"Main effects\"))\n  ) %&gt;%\n  arrange(Column, Method) %&gt;% \n  relocate(Column, .before = Method) %&gt;% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nMethod\nMin.\n1st Qu.\nMedian\nMean\n3rd Qu.\nMax.\n\n\n\n\nn_nonzero\nObserved\n3051\n3051\n3051\n3051\n3051\n3051\n\n\nn_nonzero\nIntercept\n12.00\n25.00\n28.00\n28.61\n32.00\n50.00\n\n\nn_nonzero\nMain effects\n218.0\n255.8\n265.0\n266.1\n277.0\n312.0\n\n\nprop_zero\nObserved\n0.9492\n0.9492\n0.9492\n0.9492\n0.9492\n0.9492\n\n\nprop_zero\nIntercept\n0.9992\n0.9995\n0.9995\n0.9995\n0.9996\n0.9998\n\n\nprop_zero\nMain effects\n0.9948\n0.9954\n0.9956\n0.9956\n0.9957\n0.9964\n\n\nsample_max\nObserved\n1404186\n1404186\n1404186\n1404186\n1404186\n1404186\n\n\nsample_max\nIntercept\n265332\n918290\n1172482\n1248649\n1486689\n4658137\n\n\nsample_max\nMain effects\n165327\n348857\n479910\n633731\n693709\n4569095\n\n\nsample_total\nObserved\n7507466\n7507466\n7507466\n7507466\n7507466\n7507466\n\n\nsample_total\nIntercept\n1515008\n5966878\n7341226\n7506283\n8903769\n17569471\n\n\nsample_total\nMain effects\n4374107\n6851961\n7530698\n7630616\n8274916\n12929669\n\n\n\n\n\nThe maximum claim amounts are much smaller — mean of 633k vs 1.4 mil in the observed data and 1.24 mil on average from the intercept-only PPC model.\n\nlist(\n  \"Intercept\" = ppc_obs_exposure, \n  \"Main effects\" = ppc_glm_obs_exposure\n) %&gt;% \n  map(.f = ~ {\n    .x %&gt;%\n      mutate(sample_gt_obs_max = sample_max &lt;= 1404185.52) %&gt;% \n      pull(sample_gt_obs_max) %&gt;%\n      mean()\n  })\n\n$Intercept\n[1] 0.6975\n\n$`Main effects`\n[1] 0.94\n\n\nFor the intercept-only GLM, the observed maximum is larger than 70% of the maxima from the sampled datasets, compared to 94% of the maxima from the sampled datasets using the fitted values from the main effects GLM. In an ideal scenario, this should be about 50%.\nThe sample totals are all within about +/- 150k of each other.\nTen datasets are simulated, and only the policies with non-zero claim amounts are retained. The eCDF for each of these datasets are visually compared with the observed data. Since the model predicts a larger number of policies with non-zero claims, the distribution functions are smoother, and these functions are closer to the function for the observed sample (but still a very poor fit for the observed data).\n\n\nCode\nmap(\n  .x = 1:10,\n  .f = ~ {\n    set.seed(.x)\n    draw &lt;- (fitted_values$Exposure *\n               tweedie::rtweedie(60000,\n                                 mu = fitted_values$.fitted,\n                                 phi = phi_glm, power = 1.6))\n    tibble(sim_id = .x, y = draw, grp = \"Simulated\")\n  }) %&gt;%\n  list_rbind() %&gt;%\n  bind_rows(\n    .,\n    claims %&gt;%\n      mutate(sim_id = 100, grp = \"Observed\") %&gt;%\n      select(sim_id, y = ClaimAmount, grp)\n  ) %&gt;%\n  filter(y &gt; 0) %&gt;%\n  ggplot(aes(x = y, group = sim_id, color = grp)) +\n  stat_ecdf() +\n  xlab(\"Policies with non-zero claim amounts (log10 scale)\") +\n  ylab(\"Empirical distribution function\") +\n  scale_x_log10(\n    breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n    labels = scales::trans_format(\"log10\", scales::math_format(10^.x))\n  ) +\n  annotation_logticks(sides = \"b\") +\n  scale_y_continuous(labels = scales::percent) +\n  scale_color_manual(\n    values = c(\"Simulated\" = \"gray70\", \"Observed\" = \"Black\")\n  ) +\n  theme(\n    legend.title = element_blank(), \n    legend.position = \"inside\",\n    legend.position.inside = c(0.9, 0.2)\n  )"
  },
  {
    "objectID": "posts/fitting-tweedie-models-to-claims-data/index.html#all-the-distributions-together",
    "href": "posts/fitting-tweedie-models-to-claims-data/index.html#all-the-distributions-together",
    "title": "Sampling from the poor man’s posterior distribution of parameters from models fitted to claims data",
    "section": "All the distributions together",
    "text": "All the distributions together\nFinally, all the different estimates for the distribution of next year’s total claim amounts can be visualized together. First, the eCDFs are plotted together\n\n\nCode\ndistributions_of_total_claims &lt;- bind_rows(\n  tibble(\n    method = \"Parametric Bootstrap (Tweedie MLE)\",\n    total_claim_amount_in_millions = predicted_totals_for_unit_exposure\n  ),\n  tibble(\n    method = \"Nonparametric Bootstrap (Tweedie GLM)\",\n    total_claim_amount_in_millions = posterior_distribution_samples_for_total_claims$total\n  ),\n  tibble(\n    method = \"Closed-form Tweedie Distribution for Weighted Mean\",\n    total_claim_amount_in_millions = predicted_totals_tweedie_sampling_dist\n  ),\n  tibble(\n    method = \"Main Effects Tweedie GLM\",\n    total_claim_amount_in_millions = predicted_totals_from_glm\n  )\n) %&gt;%\n  mutate(\n    total_claim_amount_in_millions = total_claim_amount_in_millions / 1e6\n  ) %&gt;%\n  bind_rows(nonparametric_bootstrap_totals, .)\n\n# order the methods by increasing values of the ranges\nmethod_order &lt;- distributions_of_total_claims %&gt;%\n  group_by(method) %&gt;%\n  summarise(range = diff(range(total_claim_amount_in_millions))) %&gt;%\n  arrange(range) %&gt;%\n  #print() %&gt;%\n  pull(method)\n\ndistributions_of_total_claims &lt;- distributions_of_total_claims %&gt;% \n  mutate(\n    method = factor(method, levels = method_order)\n  )\n\ntotals_plot &lt;- distributions_of_total_claims %&gt;%\n  ggplot() +\n  stat_ecdf(aes(x = total_claim_amount_in_millions,\n                linetype = method, group = method), pad = FALSE) +\n  geom_vline(xintercept = (6e4 * expected_claim_amount) / 1e6,\n             color = \"orange\", linewidth = 1.2, linetype = \"dashed\") +\n  scale_y_continuous(labels = scales::label_percent()) +\n  xlab(glue::glue(\"Plausible values for the following \", \n                  \"year's total claim amount in millions, \", \n                  \"assuming unit exposure\", \n                  \"\\n(Estimate from the full sample in orange)\",\n                  \"\\n(Mean of each distribution in black)\")) +\n  ylab(\"Empirical distribution function\")+\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  )\n\ntotals_plot + \n  scale_x_continuous(breaks = seq(3, 54, 3)) + \n  guides(linetype = guide_legend(nrow = 3))\n\n\n\n\n\n\n\n\n\nand then plotted separately as well, with the expected total (217.8 x 60,000) in orange and the mean of the totals for each method overlaid as a black vertical line\n\n\nCode\ntotals_plot +\n  facet_wrap(~factor(method, levels = method_order), ncol = 1) +\n  theme(legend.position = \"none\") +\n  geom_vline(\n    data = distributions_of_total_claims %&gt;%\n      summarize(m = mean(total_claim_amount_in_millions), .by = \"method\"),\n    aes(xintercept = m, group = method, linetype = method)\n  ) +\n  scale_x_continuous(breaks = seq(3, 54, 6))\n\n\n\n\n\n\n\n\n\nIf every policy is to have the same premium, then the closed-form Tweedie distribution or the joint bootstrap distribution \\((\\hat\\mu_{\\text{int}, b}, \\hat\\phi_{\\text{int}, b})_{b = 1}^{10,000}\\) can be used for simulating plausible values for the following year’s total claim amounts.\nFor setting premiums at the policy level, plausible values for next year’s claim amount for a specific policy can be sampled using the fitted means \\(\\hat\\mu_i\\) from the main-effects GLM."
  },
  {
    "objectID": "posts/fitting-tweedie-models-to-claims-data/index.html#references",
    "href": "posts/fitting-tweedie-models-to-claims-data/index.html#references",
    "title": "Sampling from the poor man’s posterior distribution of parameters from models fitted to claims data",
    "section": "References",
    "text": "References\nI’ve skimmed (parts of some of) these references, but I’m including all the ones I encountered in case I need to come back to them in the future.\n\nAuto Claims Data\n\nCASdatasets - freMTPL\nNoll, Alexander and Salzmann, Robert and Wuthrich, Mario V., Case Study: French Motor Third-Party Liability Claims (March 4, 2020). Available at SSRN: https://ssrn.com/abstract=3164764 or http://dx.doi.org/10.2139/ssrn.3164764\n\n\n\nBootstrapping\n\nCross Validated - how to interpret multimodal distribution of bootstrapped correlation\nCross Validated - bootstrapping vs bayesian bootstrapping conceptually\nhttps://mc-stan.org/docs/stan-users-guide/posterior-predictive-checks.html\nCross Validated - difference between sampling a population vs bootstrapping\nhttps://www.sumsar.net/blog/2015/04/the-non-parametric-bootstrap-as-a-bayesian-model/\nBootstrapping - Wikipedia\n\n\n\nTweedie Models\n\nDenuit, Michel, Arthur Charpentier, and Julien Trufin. “Autocalibration and Tweedie-dominance for insurance pricing with machine learning.” Insurance: Mathematics and Economics 101 (2021): 485-497. ArXiv link - https://arxiv.org/abs/2103.03635\n\nThis paper is for calibrating boosted tree models fit by minimizing deviance instead of maximizing likelihood\n\nYang, Yi, Wei Qian, and Hui Zou. “Insurance premium prediction via gradient tree-boosted Tweedie compound Poisson models.” Journal of Business & Economic Statistics 36.3 (2018): 456-470. ArXiv link - https://arxiv.org/abs/1508.06378\nDelong, Ł., Lindholm, M. & Wüthrich, M.V. Making Tweedie’s compound Poisson model more accessible. Eur. Actuar. J. 11, 185–226 (2021). https://doi.org/10.1007/s13385-021-00264-3\nZhang, Y. Likelihood-based and Bayesian methods for Tweedie compound Poisson linear mixed models. Stat Comput 23, 743–757 (2013). https://doi.org/10.1007/s11222-012-9343-7\nThese slides comparing Tweedie vs Quasi-Poisson models\nhttps://lorentzen.ch/index.php/2024/06/03/a-tweedie-trilogy-part-i-frequency-and-aggregration-invariance/\nhttps://lorentzen.ch/index.php/2024/06/10/a-tweedie-trilogy-part-ii-offsets/\nTweedie distribution - Wikipedia\nExponential dispersion model - Wikipedia\n\n\n\nBooks\n\nDunn, Peter K., and Gordon K. Smyth. Generalized linear models with examples in R. Vol. 53. New York: Springer, 2018.\nDavison, Anthony Christopher, and David Victor Hinkley. Bootstrap methods and their application. No. 1. Cambridge university press, 1997.\nKaas, R. Modern Actuarial Risk Theory. Springer, 2008.\nOhlsson, Esbjörn, and Björn Johansson. Non-life insurance pricing with generalized linear models. Vol. 174. Berlin: Springer, 2010.\nKlugman, Stuart A., Harry H. Panjer, and Gordon E. Willmot. Loss models: from data to decisions. Vol. 715. John Wiley & Sons, 2012."
  },
  {
    "objectID": "posts/fitting-tweedie-models-to-claims-data/index.html#appendix-claims-data",
    "href": "posts/fitting-tweedie-models-to-claims-data/index.html#appendix-claims-data",
    "title": "Sampling from the poor man’s posterior distribution of parameters from models fitted to claims data",
    "section": "Appendix: Claims Data",
    "text": "Appendix: Claims Data\nThe data I’m using for most of this post — except for the last section — is a subset of 60,000 policies from the full claims dataset, which contains about 680,000 policies. The full data here is the version from OpenML (frequency, severity), and information on this dataset can be found in the documentation for the R package CASdatasets, where it’s called freMTPL2freq (frequency) and freMTPL2sev (severity). For reasons I haven’t figured out, the number of rows differ very slightly between these two datasets. The R script I used for combining the full dataset can be found here.\nAn exploratory data analysis on the full dataset can be found in this paper."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a data scientist / (bio-)statistician / autodidact currently living in the magical city of Gent, Belgium. I have degrees in mathematics and statistics, as well as more than eight years of experience working as a data scientist across multiple industries – advertising, marketing, telecommunications, epidemiology, and pharma.\nMy professional interests revolve around statistics / machine learning, programming, distributed computing, writing clean code, and using models to make real-world decisions.\nIn my free time, I like to read, cook, cycle, play tennis, and learn languages.\nThe primary reason for starting this (mostly) technical blog is to work through some (technical) concepts or challenges I encounter. Additional reasons include having my R / Python code in a clean and easy-to-navigate format, and to share recipes (food, not code) once in a while. I’ve been talking about wanting to have a blog since 2012, so I’m glad that I finally got around to having one in 2022.\nYou can reach out to me via LinkedIn or via email (first name [dot] last name [at] gmail)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Akshat Dwivedi",
    "section": "",
    "text": "Simulating data from the cause-specific hazard approach to competing risks\n\n\n\n\n\n\nR\n\n\nSimulation\n\n\nTime-to-event\n\n\n\n\n\n\n\n\n\nAug 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSampling from the poor man’s posterior distribution of parameters from models fitted to claims data\n\n\n\n\n\n\nBayesian\n\n\nBootstrap\n\n\nGLM\n\n\nInsurance\n\n\nR\n\n\nTweedie\n\n\n\n\n\n\n\n\n\nJan 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating data from a time-to-event experiment with non-proportional hazards, random censoring, and cure\n\n\n\n\n\n\nAdvertising\n\n\nExperimentation\n\n\nR\n\n\nSimulation\n\n\nTime-to-event\n\n\n\n\n\n\n\n\n\nSep 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nScrollable box for long math equations on mobile\n\n\n\n\n\n\nQuarto\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUsing an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method\n\n\n\n\n\n\nAdvertising\n\n\nCausal Inference\n\n\nExperimentation\n\n\nInstrumental Variables\n\n\nG-estimation\n\n\nG-computation\n\n\nGEE\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSetup and test Python by programming a spelling bee solver\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\nOct 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHow should the untreated in the treatment group from an experiment be analyzed?\n\n\n\n\n\n\nAdvertising\n\n\nCausal Inference\n\n\nInstrumental Variables\n\n\nR\n\n\n\n\n\n\n\n\n\nSep 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSpecifying an offset in a Tweedie model with identity link\n\n\n\n\n\n\nR\n\n\nTweedie\n\n\nOffset\n\n\n\n\n\n\n\n\n\nJul 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOne reason why a (g)lm coefficient is NA\n\n\n\n\n\n\nR\n\n\nGLM\n\n\nMulticollinearity\n\n\n\n\n\n\n\n\n\nMar 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGEE on aggregated data?\n\n\n\n\n\n\nR\n\n\nGLM\n\n\nGEE\n\n\n\n\n\n\n\n\n\nMar 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDetermining the volume of my (cocktail) jigger\n\n\n\n\n\n\nMiscellaneous\n\n\nMeasurement\n\n\nR\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHow Many Five-Star Ratings Do I Need?\n\n\n\n\n\n\nAnalysis\n\n\nMiscellaneous\n\n\nR\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHello, World!\n\n\n\n\n\n\nMiscellaneous\n\n\n\n\n\n\n\n\n\nAug 1, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/g-estimation-of-MSMM-with-an-IV/index.html",
    "href": "posts/g-estimation-of-MSMM-with-an-IV/index.html",
    "title": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method",
    "section": "",
    "text": "In a previous post, I used the ratio estimator and the method of instrumental variables (IV) to estimate the effect of showing an ad on sales from a fictitious ad campaign.\nThis post uses the lesser-known method of g-estimation1 to estimate the effect of a continuous exposure variable in a multiplicative structural mean model (MSMM) by using the randomization indicator as an instrumental variable to obtain valid treatment effect estimates in the presence of unmeasured confounding.\n1 less popular compared to IPTW and PSM, hence the clickbait-y title2 i.e., people who would not take the treatment if assigned to the treatment group. Defiers and always-takers aren’t possible as individuals in the control group cannot see an ad, because compliance in the control arm is ensured by design.As mentioned towards the end of the previous post, analyzing the treated individuals by collapsing them into a single ‘treated’ group was a simplification. The observed treatment variable would in reality be the number of impressions of the ad — naturally varying between individuals in the treated group, but zero among the individuals assigned to the control group and among the never-takers2."
  },
  {
    "objectID": "posts/g-estimation-of-MSMM-with-an-IV/index.html#introduction",
    "href": "posts/g-estimation-of-MSMM-with-an-IV/index.html#introduction",
    "title": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method",
    "section": "",
    "text": "In a previous post, I used the ratio estimator and the method of instrumental variables (IV) to estimate the effect of showing an ad on sales from a fictitious ad campaign.\nThis post uses the lesser-known method of g-estimation1 to estimate the effect of a continuous exposure variable in a multiplicative structural mean model (MSMM) by using the randomization indicator as an instrumental variable to obtain valid treatment effect estimates in the presence of unmeasured confounding.\n1 less popular compared to IPTW and PSM, hence the clickbait-y title2 i.e., people who would not take the treatment if assigned to the treatment group. Defiers and always-takers aren’t possible as individuals in the control group cannot see an ad, because compliance in the control arm is ensured by design.As mentioned towards the end of the previous post, analyzing the treated individuals by collapsing them into a single ‘treated’ group was a simplification. The observed treatment variable would in reality be the number of impressions of the ad — naturally varying between individuals in the treated group, but zero among the individuals assigned to the control group and among the never-takers2."
  },
  {
    "objectID": "posts/g-estimation-of-MSMM-with-an-IV/index.html#parameter-of-interest",
    "href": "posts/g-estimation-of-MSMM-with-an-IV/index.html#parameter-of-interest",
    "title": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method",
    "section": "Parameter of interest",
    "text": "Parameter of interest\nIf \\(U\\) indicates the (possibly multivalued) set of unmeasured confounders, \\(Z\\) the binary randomization variable indicating group assignment (treatment or control), \\(A\\) the integral exposure variable indicating observed treatment level \\(a\\) in \\(\\{0, 1, 2, … \\}\\), then an estimand or parameter of interest is the marginal causal risk ratio (CRR)\n\\[\n\\text{ATE} = \\frac{\\text{Pr}[Y^{a + 1} = 1]}{\\text{Pr}[Y^{a} = 1]}\n\\]\nThis is usually the target of inference in an experiment with perfect compliance, and corresponds to setting the treatment level to \\(A = a + 1\\) vs \\(A = a\\) for the full population (\\(Y_i^a\\) denotes the potential outcome for the \\(i\\)-th individual and \\(\\text{Pr}[Y^a = 1]\\) denotes the probability of \\(Y = 1\\) under treatment level \\(a\\)).\nAnother estimand of interest may be the average treatment effect in the treated (ATT) or the effect of the treatment on the treated (ETT). This compares the treated individuals to the counterfactual scenario had they not been treated. This corresponds to \\[\n\\text{ATT} = \\frac{\\text{Pr}[Y^a = 1 | A = a]}{\\text{Pr}[Y^{a = 0} = 1 | A = a]}\n\\]\nIn a setting with noncompliance (possibly due to unmeasured confounding), the estimand can be further conditioned on the IV \\(Z\\)\n\\[\n\\text{ATT}_{IV} = \\frac{\\text{Pr}[Y^a = 1 | A = a, Z]}{\\text{Pr}[Y^{a = 0} = 1 | A = a, Z]}\n\\]\nThe main MSMM used in this post estimates the ATE if the unmeasured confounders \\(U\\) are not effect-measure modifiers (i.e., the impact of treatment \\(A\\) on the ratio scale is the same on \\(Y\\) at all levels of \\(U\\)). If \\(U\\) is an effect modifier, but \\(Z\\) is not, then the MSMM estimate can be interpreted as the \\(\\text{ATT}_{IV}\\).\nPages 2-3 of this paper provide a concise yet illuminating discussion of the different effect measures that can be of interest.\nSince risk ratios are collapsible (see this excellent paper), the (confounder-adjusted) marginal and conditional estimands are identical3.\n3 which is only the case for risk differences and risk ratios.Before writing any code, the main packages used in this post are attached.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(broom, include.only = \"tidy\")\n\n# functions from the following packages are called via package::fun()\n# to reduce namespace conflicts\n# library(geeM)\n# library(mgcv)\n# library(ivtools)\n# library(marginaleffects)\n# library(glue)\n# library(gridExtra)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "posts/g-estimation-of-MSMM-with-an-IV/index.html#g-estimation-in-a-nutshell",
    "href": "posts/g-estimation-of-MSMM-with-an-IV/index.html#g-estimation-in-a-nutshell",
    "title": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method",
    "section": "G-estimation in a nutshell",
    "text": "G-estimation in a nutshell\nThe first time I read the chapter on g-estimation in the what-if book, it went over my head completely4. Conceptually, it seemed harder than the more popular (g-)methods of g-computation and inverse probability of treatment weighting (IPTW). However, its superiority to g-computation in terms of not being prone to bias due to extrapolation, superiority to IPTW5 in terms of having less variability (and bias), and relative ease of handling continuous exposures (and continuous instruments) piqued my curiosity6.\n4 May have had something to do with the fact that I was attempting this at 3 AM in between sleep cycles.5 I think this isn’t an issue if overlap (ATO) weights are used instead of the usual ATE weights, as g-estimation gives higher weight to strata with greater overlap, and empty strata don’t contribute to the final estimate.6 I’m basing this on the Dukes, Vansteelandt paper.Another advantage is that there’s no need to correctly model the relationship between the confounders \\(U\\) and the outcome \\(Y\\) for the correct estimation of the treatment effect. This would not be the case when using (log-)binomial or poisson regression, where misspecification of the \\(U-Y\\) relationship can bias the estimate for \\(A\\).\nThe essence of g-estimation is simple7 – once the true effect of the treatment \\(\\psi\\) is shaved off from the response variable \\(Y\\), the residuals \\(H(\\psi) = Y - \\psi A = Y^{a = 0}\\) should be uncorrelated with the treatment \\(A\\) in the confounder-adjusted propensity model, i.e., \\(\\text{Cov}(A, H(\\psi) | U) = 0\\)8. This is the same as picking the value of \\(\\psi\\) that leads to \\(\\alpha_1 = 0\\) in\n7 after having grasped it that is8 Cov denotes covariance.\\[\n\\mathbb{E}[A | Y^{a=0}, U] = \\alpha_0 + \\alpha_1 H(\\psi) + \\sum_{j=2}^k \\alpha_j U_{j-1}\n\\]\nwhere \\(\\mathbb{E}[A]\\) is replaced by \\(\\text{logit(Pr[A = 1])}\\) for a binary exposure variable, \\(\\text{logit}(p) = \\text{log}\\Big(\\frac{p}{1-p}\\Big)\\), and \\(p\\) is the probability \\(\\text{Pr}[A = 1]\\). For MSMMs, \\(H(\\psi) = Y \\text{exp}(- \\psi A)\\) is used where \\(\\psi\\) is the risk ratio on the log scale.\nThis works because of the conditional mean independence assumption — that the exposed and unexposed individuals are exchangeable within levels of \\(U\\) (i.e., \\(Y^{a = 0} \\perp\\!\\!\\!\\perp A|U\\)) as would be the case for a conditionally randomized experiment. In a marginally randomized experiment, the treatment assigned and received is independent of the potential outcomes, which would not be the case if people with (say) higher levels of \\(U\\) were more likely to take the treatment (\\(A &gt; 0\\)) and have a positive outcome \\(Y = 1\\). In this scenario, it wouldn’t be possible to say whether it was \\(A\\) or \\(U\\) causing better outcomes. This lack-of-identification is due to confounding between the effect of \\(U\\) on \\(Y\\) and the effect of \\(A\\) on \\(Y\\).\nSuppose the following dataset is available (with a true CRR value of 1.02, 70% treatment-control split, and 40% non-compliance (never-takers))\n\n\nCode\nsimulate_data &lt;- function(seed = 24, n = 1e5) {\n  \n  confounder_values &lt;- seq(0, 1e5, 50) / 1e4\n  \n  set.seed(seed)\n  tibble(\n    id = 1:n,\n    # randomization indicator / instrument, indicates group assignment\n    Z = rbinom(n, 1, prob = 0.7),\n    # simulate continuous confounder value\n    U = sample(x = confounder_values, size = n, replace = TRUE),\n    # simulate (conditional) compliance indicator\n    prob_C = plogis(2 - 0.5 * U),\n    C = rbinom(n, 1, prob_C),\n    # simulate observed treatment\n    # potential continuous exposure value is simulated \n    # for each individual on [1, 100]\n    # but for those that have Z = 0 (control) or C = 0 (never-takers), A = 0\n    A = Z * C * pmin(pmax(round(rgamma(n, shape = 5, scale = 4)), 1), 100),\n    # response variable simulated with risk ratios specified\n    # baseline log probabilities\n    logpY0 = log(0.002) + log(1.4) * U,\n    # impact of treatment\n    logpY1 = logpY0 + log(1.02) * A,\n    # simulated binary responses from log-binomial model\n    Y = rbinom(n, 1, exp(logpY1))\n  )\n}\n\ndf_large &lt;- simulate_data() %&gt;%\n  glimpse()\n\n\nRows: 100,000\nColumns: 9\n$ id     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, …\n$ Z      &lt;int&gt; 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, …\n$ U      &lt;dbl&gt; 2.795, 5.120, 4.930, 1.090, 3.675, 7.735, 8.770, 8.085, 4.780, …\n$ prob_C &lt;dbl&gt; 0.64622806, 0.36354746, 0.38580036, 0.81076675, 0.54053584, 0.1…\n$ C      &lt;int&gt; 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, …\n$ A      &lt;dbl&gt; 23, 0, 0, 21, 20, 0, 0, 0, 0, 24, 0, 24, 0, 0, 0, 0, 10, 0, 21,…\n$ logpY0 &lt;dbl&gt; -5.274168, -4.491870, -4.555800, -5.847853, -4.978073, -3.61199…\n$ logpY1 &lt;dbl&gt; -4.818708, -4.491870, -4.555800, -5.431998, -4.582020, -3.61199…\n$ Y      &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\nFor exploring g-estimation with no-unmeasured confounding (since \\(U\\) is measured) in the simple model represented by the directed acyclic graph (DAG) \\(A \\leftarrow U \\rightarrow Y\\) and \\(A \\rightarrow Y\\), one way of estimating the treatment effect \\(\\psi\\) is using grid search which visually looks like this when the estimated coefficient for \\(\\alpha_1\\) is plotted on the Y-axis\n\n\nCode\ngrid_search_coefficient &lt;- seq(-1, 1, 0.05) %&gt;%\n  # get the coefficient from the correct and incorrect propensity models\n  map_dfr(.f = ~ {\n\n    temp_df &lt;- df_large %&gt;%\n      mutate(H = Y * exp(-.x * A))\n\n    list(`No unmeasured confounding` = \"A ~ H + U\", \n         `Unmeasured confounding` = \"A ~ H\") %&gt;%\n      map_dfc(.f = ~ {\n        temp_df %&gt;%\n          lm(formula(.x), data = .) %&gt;%\n          tidy() %&gt;%\n          filter(term == \"H\") %&gt;%\n          pull(estimate)\n      }) %&gt;%\n      mutate(psi = .x, .before = `No unmeasured confounding`)\n  }) \n\ngrid_search_coefficient %&gt;%\n  pivot_longer(-psi) %&gt;%\n  ggplot(aes(x = exp(psi), y = value, color = name)) +\n  geom_point() +\n  geom_line() +\n  geom_vline(xintercept = 1.02, linetype = \"dotdash\") +\n  geom_hline(yintercept = 0, linetype = \"dotdash\") +\n  geom_hline(yintercept = -2.5, linetype = \"dotdash\") +\n  xlab(\"Risk ratio (true value: 1.02)\") +\n  ylab(\n    glue::glue(\"Estimated coefficient for H(\\U1D713)\",\n               \" \\nfrom the propensity model\")\n  ) +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  coord_cartesian(xlim = c(0, 3), ylim = c(-6, 4))\n\n\n\n\n\n\n\n\n\nor like this when the \\(p\\)-value is plotted on the Y-axis (on a narrower grid)\n\n\nCode\ngrid_search_pvalue &lt;- seq(-0.05, 0.05, 0.001) %&gt;%\n  # get the coefficient from the correct and incorrect propensity models\n  map_dfr(.f = ~ {\n\n    temp_df &lt;- df_large %&gt;%\n      mutate(H = Y * exp(-.x * A))\n\n    list(`No unmeasured confounding` = \"A ~ H + U\", \n         `Unmeasured confounding` = \"A ~ H\") %&gt;%\n      map_dfc(.f = ~ {\n        temp_df %&gt;%\n          lm(formula(.x), data = .) %&gt;%\n          tidy() %&gt;%\n          filter(term == \"H\") %&gt;%\n          pull(p.value)\n      }) %&gt;%\n      mutate(psi = .x, .before = `No unmeasured confounding`)\n  }) \n\ngrid_search_pvalue %&gt;%\n  pivot_longer(-c(psi)) %&gt;%\n  ggplot(aes(x = exp(psi), y = value, color = name)) +\n  geom_point() +\n  geom_line() +\n  geom_vline(xintercept = 1.02, linetype = \"dotdash\") +\n  geom_hline(yintercept = 0, linetype = \"dotdash\") +\n  xlab(\"Risk ratio (true value: 1.02)\") +\n  ylab(\"P-value for the test of alpha1 = 0\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nFrom both the graphs, it’s evident that the estimate from the correctly specified propensity model is much closer to the true value of 1.02 (or 2% increase when going from \\(A = a\\) to \\(A = a + 1\\)) compared to the estimate from the propensity model without \\(U\\). Recovering the true effect in the presence of unmeasured confounding would require knowing which non-zero value of \\(\\alpha_1\\) (here about -2.5) corresponds to the true value of \\(\\psi\\).\nGrid search can be kinda tedious9 and computationally inefficient when there are multiple treatment parameters10, but this paper shows a neat trick using log-link Gamma Generalized Estimating Equations (GEE) to estimate risk ratios using regression adjustment for the propensity score\n9 It was annoying to use grid-search with p-values for this setting - if the grid was too coarse so there weren’t any points close to the true value, then the p-values were very close to zero, which explains the fine grid used here. Calling optimize(gest_function, interval = c(-5, 5), maximum = TRUE) where gest_function takes \\(\\psi\\) as an input and returns the p-value as the output failed to find the correct value, until the interval was narrowed to (0, 2). The coefficient estimate function wouldn’t work too well either, since risk ratio values less than 1 lead to estimates of \\(\\alpha_1\\) to be very close to 0.10 to account for effect-measure modification or heterogeneity of treatment effect\npropensity_scores &lt;- mgcv::gam(A ~ s(U, k = 10), data = df_large) %&gt;%\n  fitted()\n\npropensity_scores %&gt;% summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.625   2.061   5.345   5.847   9.565  12.438 \n\ngest_confounder &lt;- df_large %&gt;%\n  mutate(PS = propensity_scores) %&gt;%\n  geeM::geem(Y ~ PS + A, data = ., family = Gamma(link = \"log\"),\n             corstr = \"independence\", sandwich = TRUE)\n\nsummary(gest_confounder)\n\n            Estimates Model SE Robust SE    wald     p\n(Intercept)  -3.02200 0.061950  0.041350 -73.080 0e+00\nPS           -0.25020 0.009526  0.011600 -21.580 0e+00\nA             0.01937 0.003600  0.003923   4.936 8e-07\n\n Estimated Correlation Parameter:  0 \n Correlation Structure:  independence \n Est. Scale Parameter:  117.9 \n\n Number of GEE iterations: 2 \n Number of Clusters:  100000    Maximum Cluster Size:  1 \n Number of observations with nonzero weight:  100000 \n\n\nUnfortunately there’s no tidier for objects of class geem in the broom package, but it’s easy enough to write one to extract information from the fitted model as a data frame — see the following code block\n\n\nCode\ntidy.geem &lt;- function(x, conf.int = FALSE, \n                      exponentiate = FALSE, \n                      conf.level = 0.95, \n                      robust = TRUE, ...) {\n  # works only when broom is attached\n  # as it's added as an S3 method for broom::tidy(x, ...)\n  # could also do library(broom, include.only = \"tidy\")\n  #\n  # arguments are the same as broom::tidy.geeglm()\n  stopifnot(is.logical(robust))\n  s &lt;- summary(x)\n  ret &lt;- c(\"term\" = \"coefnames\", \"estimate\" = \"beta\",\n           \"std.error\" = if (robust) \"se.robust\" else \"se.model\",\n           \"statistic\" = \"wald.test\", \"p.value\" = \"p\") %&gt;%\n    map(.x = ., .f = ~ pluck(s, .x)) %&gt;%\n    as_tibble()\n\n  if (conf.int) {\n    p &lt;- (1 + conf.level) / 2\n    ret &lt;- ret %&gt;%\n      mutate(\n        conf.low = estimate - (qnorm(p = p) * std.error),\n        conf.high = estimate + (qnorm(p = p) * std.error),\n      )\n  }\n\n  if (exponentiate) {\n    # exponentiate the point estimate and the CI endpoints (if present)\n    ret &lt;- ret %&gt;%\n      mutate(\n        across(\n          .cols = -c(term, std.error, statistic, p.value),\n          .fns = exp\n        )\n      )\n  }\n\n  return(ret)\n}\n\ngest_confounder %&gt;%\n  tidy(exponentiate = TRUE, conf.int = TRUE, robust = TRUE) %&gt;%\n  filter(term == \"A\")\n\n\n# A tibble: 1 × 7\n  term  estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 A         1.02   0.00392      4.94 0.0000008     1.01      1.03\n\n\nThis can also be compared with the usual regression estimate\n\nglm_model &lt;- glm(Y ~ U + A, data = df_large, family = binomial(link = \"log\"))\n\nglm_model %&gt;% \n  tidy(exponentiate = TRUE, conf.int = TRUE) %&gt;% \n  mutate(across(.cols = where(is.numeric), .fns = ~ round(.x, 4)))\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   0.0022    0.0833    -73.3        0   0.0019    0.0026\n2 U             1.38      0.0104     31.1        0   1.36      1.41  \n3 A             1.02      0.0025      8.52       0   1.02      1.03  \n\n\nBut what if \\(U\\) is unmeasured?"
  },
  {
    "objectID": "posts/g-estimation-of-MSMM-with-an-IV/index.html#g-estimation-with-an-iv",
    "href": "posts/g-estimation-of-MSMM-with-an-IV/index.html#g-estimation-with-an-iv",
    "title": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method",
    "section": "G-estimation with an IV",
    "text": "G-estimation with an IV\nIn a randomized trial with perfect compliance, \\(A\\) and \\(Z\\) will be identical so a simple analysis using either one of these will result in an unbiased estimate of the average treatment effect. On the other hand, in the presence of imperfect compliance due to unmeasured confounding, an unadjusted analysis of \\(A - Y\\) will lead to biased estimates of the effect of treatment, and an unadjusted analysis of \\(Z - Y\\) will lead to an estimate of assigning treatment11 instead of receiving the treatment. Analyzing only \\(A-Y\\), the estimated treatment effect concentrates around 0.98 on average instead of 1.02\n11 i.e., the intention-to-treat effect (ITT)\n\nCode\n# specify the seed\nseq(1, 100, 1) %&gt;% \n  map_dbl(.f = ~ {\n    .x %&gt;% \n      simulate_data(seed = ., n = 1e4) %&gt;% \n      glm(Y ~ A, data = ., family = binomial(link = \"log\")) %&gt;%\n      tidy(exponentiate = TRUE) %&gt;%\n      filter(term == \"A\") %&gt;% \n      pull(estimate)\n  }) %&gt;% \n  summary()\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.9578  0.9741  0.9815  0.9810  0.9864  1.0035 \n\n\nIf the DAG with relationships \\(A \\leftarrow U \\rightarrow Y\\) and \\(Z \\rightarrow A \\rightarrow Y\\) holds but only \\((Z, A, Y)\\) are observed, and \\(Z\\) is a variable that meets the IV conditions12, the effect of \\(A\\) (\\(\\text{ATE}\\) or \\(\\text{ATT}_{IV}\\)) can be estimated under some additional assumptions.\n12 Associated with \\(A\\) (relevance), not associated with any variables in the set \\(U\\) (ignorability), and no direct effect on \\(Y\\) (exclusion restriction)The key idea is captured by this (estimating) equation\n\\[\n\\sum_{i = 1}^n (z_i - \\bar{z})\\ y_i\\ \\text{exp}(-\\psi\\ a_i) = 0\n\\]\nwhere \\(\\bar{z}\\) denotes the proportion of individuals assigned to the treatment arm.\nSince \\(Z\\) is randomly assigned, it should be balanced with respect to all measured and unmeasured confounders \\(U\\). But \\(A\\) may not be. So the correct estimate of \\(\\psi\\) is the one that leads to zero covariance between the residuals \\(Y^{a = 0}\\) (or \\(H(\\psi)\\)) and \\(Z\\)13. Pretty magical, if you ask me.\n13 If \\((a_i - \\bar{a})\\) were to be used instead of \\((z_i - \\bar{z})\\), then the estimate would be the same one as from the \\(A- Y\\) analysis.This equation can be coded up as a function which visually looks like this\n\n\nCode\nMSNMM_function &lt;- function(psi, data = df_large, positive = TRUE) {\n  result &lt;- data %&gt;%\n    mutate(\n      Zbar = mean(Z),\n      s = (Z - Zbar) * Y * exp(-psi * A)\n    ) %&gt;%\n    pull(s) %&gt;%\n    sum()\n\n  if (positive) result &lt;- abs(result)\n\n  return(result)\n}\n\nIV_grid &lt;- seq(-0.05, 0.5, 0.01) %&gt;%\n  tibble(psi = .) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    `Raw value` = MSNMM_function(psi, positive = FALSE),\n    `Absolute value` = abs(`Raw value`)\n  ) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(-psi) %&gt;%\n  mutate(name = fct_rev(factor(name))) \n\nIV_grid %&gt;%\n  ggplot(aes(x = exp(psi), y = value, color = name)) +\n  geom_point() + geom_line() +\n  geom_hline(yintercept = 0, linetype = \"dotdash\") +\n  geom_vline(xintercept = 1.02, linetype = \"dotdash\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  scale_x_continuous(n.breaks = 10) +\n  xlab(\"Risk ratio (true value: 1.02)\") +\n  ylab(\"Covariance between Z and H(\\U1D713)\")\n\n\n\n\n\n\n\n\n\nInstead of using grid search, the optimize() function can be used with the absolute value of the covariance to pick the value of \\(\\psi\\) that leads to 0 covariance14\n14 if the function is flat in the specified interval, the optimization step would return incorrect results (passing `interval = c(3, 5)` returns the value of 3 and an objective function value of 83.8 which is far away from 0)\noptimize(f = MSNMM_function, interval = c(-2, 2), maximum = FALSE)\n\n$minimum\n[1] 0.01308194\n\n$objective\n[1] 0.03266255\n\n\nThis is a slight underestimate (1.013 vs 1.02) — but it seems to be a sampling artifact as the median of 100 simulated values concentrates around 1.02\n\n\nCode\nIV_estimates &lt;- map_dbl(\n  .x = 1:100,\n  .f = ~ {\n    .x %&gt;%\n      simulate_data(seed = .) %&gt;%\n      optimize(\n        f = MSNMM_function, interval = c(-2, 2), \n        maximum = FALSE, data = .\n      ) %&gt;%\n      pluck(\"minimum\")\n  })\n\nIV_estimates %&gt;% exp() %&gt;% summary()\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.9966  1.0136  1.0206  1.0219  1.0304  1.0536 \n\n\nIt might just be simpler to use the ivglm() function from the ivtools package (see this paper) to accomplish the same thing though15\n15 which has other advantages like handling multiple instruments and different effect measures\n\nCode\nIV_model &lt;- glm(Z ~ 1, family = binomial(link = \"logit\"), data = df_large)\n\nIV_gest &lt;- df_large %&gt;%\n  # need to cast tibble as a data frame to avoid cryptic errors\n  as.data.frame() %&gt;%\n  ivtools::ivglm(estmethod = \"g\",\n                 X = \"A\", Y = \"Y\",\n                 fitZ.L = IV_model,\n                 link = \"log\", data = .)\n\nIV_gest %&gt;% summary()\n\n\n\nCall:  \nivtools::ivglm(estmethod = \"g\", X = \"A\", Y = \"Y\", fitZ.L = IV_model, \n    data = ., link = \"log\")\n\nCoefficients: \n  Estimate Std. Error z value Pr(&gt;|z|)\nA  0.01310    0.01065    1.23    0.219\n\n\n\n\nCode\n# get the coefficient and CI\nglue::glue(\n  \"Mean: {round(exp(c(summary(IV_gest)$coefficients[1, 1])), 4)}\", \n  \" with 95% CI: ({round(exp(confint(IV_gest))[1], 4)},\", \n  \" {round(exp(confint(IV_gest))[2], 4)})\"\n)\n\n\nMean: 1.0132 with 95% CI: (0.9923, 1.0346)\n\n\nwhere it produces nearly the same estimate as above and additionally has the option of plotting the shape of the estimating equations in a neighborhood of the estimate\n\n\nCode\n# can use ggplot on the output from estfun()\n# ivtools::estfun(IV_gest, lower = -0.1, upper = 0.1, step = 0.01) %&gt;% \n#   pluck(\"f\") %&gt;%  \n#   pluck(\"A\") %&gt;% \n#   data.frame() %&gt;% \n#   ggplot(aes(x = exp(psi), y = Hsum)) + \n#   geom_point() +\n#   geom_line() \n\nplot(ivtools::estfun(IV_gest, lower = -0.1, upper = 0.1, step = 0.01))\n\n\n\n\n\n\n\n\n\nA caveat is that this approach can sometimes fail when this function has a weird shape in the presence of weak instruments (see this paper)."
  },
  {
    "objectID": "posts/g-estimation-of-MSMM-with-an-IV/index.html#estimating-probabilities",
    "href": "posts/g-estimation-of-MSMM-with-an-IV/index.html#estimating-probabilities",
    "title": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method",
    "section": "Estimating probabilities",
    "text": "Estimating probabilities\nIf we had the model which included \\(U\\) as a covariate, conditional probabilities \\(\\text{Pr}[Y = 1 | A = a, U = u]\\) as well as marginal probabilities \\(\\text{Pr}[Y = 1 | A = a]\\) (averaged over levels of \\(U\\)) could be generated using the amazing marginaleffects package to perform g-computation with the resulting dose-response curves visualized.\nThe following plot with conditional dose-response curves at quartiles of \\(U\\) shows that the effect of the treatment with increasing \\(A\\) is increasing across levels of \\(U\\) — which makes sense because the treatment effect is a multiplicative (percent) change relative to the previous level16\n16 the effects of \\(A\\) and \\(U\\) are linear on the log scale but not on the transformed probability scale\n\nCode\n# get predicted probabilities from the model at quartiles of U\nglm_model %&gt;%\n  marginaleffects::plot_predictions(condition = list(\"A\", \"U\" = \"fivenum\")) +\n  xlab(\"A\") +\n  ylab(\"Predicted probability of Y = 1\") + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nWhen \\(U\\) is unmeasured, only the marginal dose-response curve can be estimated, and the following plot compares g-computation and g-estimation estimates and their CIs, with the true curve plotted for comparison\n\n\nCode\n# simulate 100 datasets and take the mean of probabilities under A = 0\nbaseline_conversion_probability &lt;- map_dbl(\n  .x = 1:100,\n  .f = ~ {\n    simulate_data(seed = .x, n = 1e5) %&gt;%\n      pull(logpY0) %&gt;%\n      exp() %&gt;%\n      mean()\n    }) %&gt;%\n  mean()\n\n# true dose-response curve\ntrue_dose_response_curve &lt;- tibble(\n  A = 0:73,\n  estimate = baseline_conversion_probability * exp(log(1.02) * A)\n)\n\n# g-computation by creating counterfactual predictions\n# by setting A = a and averaging the predicted probabilities\n# doing this over a coarser grid to get faster run times\ng_computation_dose_response_extrapolated_levels &lt;- glm_model %&gt;%\n  marginaleffects::predictions(\n    by = \"A\", type = \"response\",\n    newdata = marginaleffects::datagrid(\n      A = c(0, seq(5, 70, 5), 73), grid_type = \"counterfactual\"\n    )\n  ) %&gt;%\n  select(A, estimate, conf.low, conf.high) %&gt;%\n  as_tibble() %&gt;% \n  mutate(type = \"G-computation\")\n\n# get estimates of Pr[Y = 1 | A = 0]\n# by shaving off the effect of the treatment\n# using the point estimate and CI endpoints from ivglm\ng_estimation_pY0 &lt;- c(\n  IV_gest %&gt;% pluck(\"est\") %&gt;% unname(),\n  IV_gest %&gt;% confint() %&gt;% unname()\n) %&gt;%\n  set_names(nm = c(\"estimate\", \"conf.low\", \"conf.high\")) %&gt;%\n  map_dfr(\n    .f = ~ {\n      df_large %&gt;%\n        mutate(H = Y * exp(-.x * A)) %&gt;%\n        pull(H) %&gt;%\n        mean() %&gt;%\n        tibble(logRR = .x, pY0 = .)\n    }, \n    .id = \"name\"\n  )\n\n# using logRR estimate + CI and Pr[Y = 1 | A = 0]\ng_estimation_dose_response &lt;- g_estimation_pY0 %&gt;%\n  expand_grid(., A = 0:73) %&gt;%\n  # get P[Y = 1 | A = a]\n  mutate(pY = pY0 * exp(logRR * A)) %&gt;%\n  select(-pY0, -logRR) %&gt;%\n  pivot_wider(id_cols = A, names_from = \"name\", values_from = \"pY\") %&gt;%\n  mutate(type  = \"G-estimation (using an IV)\")\n\nbind_rows(\n  g_computation_dose_response_extrapolated_levels,\n  g_estimation_dose_response\n) %&gt;%\n  ggplot(aes(x = A, y = estimate, color = type)) +\n  geom_line() +\n  # plot the true curve\n  geom_line(\n    data = true_dose_response_curve, \n    aes(x = A, y = estimate), \n    color = \"gray20\", linewidth = 1.2,, linetype = \"dotdash\",\n    inherit.aes = FALSE\n  ) + \n  geom_ribbon(\n    aes(ymin = conf.low, ymax = conf.high, fill = type), alpha = 0.2\n  ) +\n  scale_y_continuous(breaks = seq(0, 0.25, 0.05)) +\n  scale_x_continuous(breaks = seq(0, 80, 10)) +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  ylab(\"Predicted probability of Y = 1\") + \n  xlab(\"A (true curve in gray)\")\n\n\n\n\n\n\n\n\n\nAssuming no-effect modification by \\(U\\), the point estimate and the CI endpoints from ivglm are used to first estimate \\(\\text{Pr}[Y = 1 | A = 0]\\) (which is the same as \\(\\text{Pr}[Y^{a = 0} = 1]\\)17) by using the following relation\n17 assuming consistency\\[\n\\text{Pr}[Y^{a = 0} = 1] = \\text{Pr}[Y = 1 | A = 0] = \\text{E}[Y\\ \\text{exp}(\\psi A)]\n\\]\nand then using these to get the dose response curves and the following CIs\n\\[\n\\text{Pr}[Y = 1 | A] = \\text{Pr}[Y = 1 | A = 0]\\ \\text{exp}[\\psi A]\n\\]\nThe g-computation estimate is far narrower as the model has access to the information provided by \\(U\\), compared to the MSMM. The MSMM estimate for this sample underestimates the true value, which shows up as a large difference between the true and the MSMM curves at values of \\(A\\) higher than 60.\nThese probabilities can be mapped to total sales / profit given the number of individuals within each treatment level. Whether these numbers can be transported or extrapolated to future campaigns or other populations depends on how similar the distribution of \\(U\\) is across these groups. In that sense, these marginal estimates are actually conditional on the distribution of confounders and effect modifiers. I came across this paper which would be interesting to read in the future."
  },
  {
    "objectID": "posts/g-estimation-of-MSMM-with-an-IV/index.html#references",
    "href": "posts/g-estimation-of-MSMM-with-an-IV/index.html#references",
    "title": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method",
    "section": "References",
    "text": "References\nThis post is based on the following references\n\nHernán, Robins - Causal Inference: What If? - Chapters 14 (g-estimation), and 16 (instrumental variables)\nVansteelandt et al. 2011 - On Instrumental Variables Estimation of Causal Odds Ratios\nPalmer et al. 2011 - Instrumental Variable Estimation of Causal Risk Ratios and Causal Odds Ratios in Mendelian Randomization Analyses\nDukes, Vansteelandt 2018 - A Note on G-Estimation of Causal Risk Ratios\nBurgess et al. 2014 - Lack of Identification in Semiparametric Instrumental Variable Models With Binary Outcomes\nSjolander, Martinussen 2019 - Instrumental Variable Estimation with the R Package ivtools"
  },
  {
    "objectID": "posts/g-estimation-of-MSMM-with-an-IV/index.html#simulated-data",
    "href": "posts/g-estimation-of-MSMM-with-an-IV/index.html#simulated-data",
    "title": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method",
    "section": "Simulated data",
    "text": "Simulated data\nThe four figures — going clockwise from top left — show\n\nthe probability of compliance as a function of the confounder \\(U\\)\n$$\\text{Pr}[C = 1 | U] = 1 / (1 + \\text{exp}(-2 + 0.5 u)),\\ u \\in [0, 10]$$\nthe realized distribution of the treatment levels among those who received the treatment18 simulated from the truncated Gamma distribution rounded to the nearest integer \\[x_i \\sim \\text{max}(1, \\text{min}(\\Gamma(\\text{shape} = 5,\\ \\text{scale} = 4), 100))\\]\nbalance of \\(U\\) across levels of \\(A\\) (which is not balanced because of confounding by \\(U\\)), and\nbalance of \\(U\\) across levels of \\(Z\\) (which is balanced because of randomization)\n\n18 a value is sampled for each individual, but is realized only if \\(Z = 1\\) and \\(C = 1\\) (where \\(C\\) is the compliance indicator)\n\nCode\n# probability of compliance as a function of the confounder\np_compliance &lt;- df_large %&gt;%\n  ggplot(aes(x = U, y = prob_C)) +\n  geom_line() + \n  ylab(\"Compliance probability\") +\n  coord_cartesian(y = c(0, 1))\n\n# observed distribution of exposure levels\nA_dist &lt;- df_large %&gt;%\n  filter(A &gt; 0) %&gt;%\n  ggplot(aes(x = A)) +\n  geom_bar() +\n  scale_x_continuous(breaks = seq(0, 100, 10), \n                     labels = seq(0, 100, 10))\n\n# check balance of U across Z\n# balanced as expected, because Z is a randomization indicator\nUZ_plot &lt;- df_large %&gt;%\n  mutate(Z = factor(Z, levels = c(1, 0), \n                    labels = c(\"Treatment\", \"Control\"))) %&gt;%\n  ggplot(aes(x = U, color = Z)) +\n  geom_density() +\n  theme(legend.position = \"bottom\")\n\n# check balance across (dichotomized version of) A\n# not balanced because U and A are associated\nUA_plot &lt;- df_large %&gt;%\n  mutate(\n    `A (binary)` = factor(as.numeric(A &gt; 0), \n                          levels = c(1, 0), \n                          labels = c(\"Treated\", \"Control\"))) %&gt;%\n  ggplot(aes(x = U, color = `A (binary)`)) +\n  geom_density() +\n  theme(legend.position = \"bottom\")\n\ngridExtra::grid.arrange(p_compliance, A_dist, \n                        UZ_plot, UA_plot, ncol = 2)"
  },
  {
    "objectID": "posts/hello-world/index.html",
    "href": "posts/hello-world/index.html",
    "title": "Hello, World!",
    "section": "",
    "text": "This first post is to check whether the features I want for this blog work as desired.\nThese can be summarized in a non-exhaustive list as:"
  },
  {
    "objectID": "posts/hello-world/index.html#math",
    "href": "posts/hello-world/index.html#math",
    "title": "Hello, World!",
    "section": "Math",
    "text": "Math\nThe OLS estimator is given by the equation \\(\\hat\\beta_\\text{OLS} = (X^\\mathsf{T} X)^{-1} X^\\mathsf{T} y\\).\nOn the other hand, the ridge estimator is given by the following formula\n\\[\\hat\\beta_\\text{ridge} = (X^\\mathsf{T} X + \\lambda I)^{-1} X^\\mathsf{T} y\\]\nwhere \\(\\lambda \\in [0, \\infty)\\) controls the amount of shrinkage applied to the coefficients."
  },
  {
    "objectID": "posts/hello-world/index.html#r-code",
    "href": "posts/hello-world/index.html#r-code",
    "title": "Hello, World!",
    "section": "R code",
    "text": "R code\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\np &lt;- iris %&gt;% \n  ggplot(aes(x = Petal.Length, y = Petal.Width, color = Species)) + \n  geom_point() + \n  theme_classic()\n\nvanilla ggplot\n\nplot(p)\n\n\n\n\n\n\n\n\nplotly plot\n\nplotly::ggplotly(p)"
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html",
    "href": "posts/instrumental-variable-in-RCT/index.html",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "",
    "text": "This (longer-than-expected) post compares intention-to-treat, per-protocol, as-treated, and instrumental variable analyses on a simulated dataset. Along the way, it goes off on fun tangents like 1) comparing results from different estimators for risk difference (a.k.a. uplift), and 2) comparing the bootstrap distribution with the Bayesian posterior distribution and the normal approximation for the risk difference. Finally, it uses the estimated probabilities to get the posterior predictive distribution for the total profit under different scenarios. This was written largely for me to learn about IV methods in order to deal with the problem of estimating the effect of a treatment under non-compliance in randomized experiments."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#introduction",
    "href": "posts/instrumental-variable-in-RCT/index.html#introduction",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "Introduction",
    "text": "Introduction\nOne of the first problems I ever cut my teeth on as a data scientist many years ago was to analyze data from a marketing / advertising campaign to assess the impact of an advertisement (ad) on sales of a certain product.\nIt was a simple experimental dataset from a two-arm1 randomized controlled trial. The most challenging part of that analysis was how to analyze individuals – the so-called non-compliers2 – who were assigned to the treatment group eligible to view an ad, but who did not actually end up seeing a single ad.\n1 i.e., an experiment with two groups – treatment and control2 in the usual terminology used in the instrumental variable methods literatureAll details below are of a simplified version of the problem similar to the one I worked on, and have no resemblance to the real constraints / numbers / settings from the actual problem. This exact same problem shows up in pretty much every field (marketing, advertising, pharma, tech, psychology, economics, etc.) where experiments are regularly conducted, and the analyses in this post are applicable to those situations as well."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#problem-description",
    "href": "posts/instrumental-variable-in-RCT/index.html#problem-description",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "Problem description",
    "text": "Problem description\nLet’s say a company has been working on updating an existing version of a product, and wants to know whether they should invest in an advertising campaign on a specific channel (say YouTube / Instagram ads) to promote the new product. Since buying ads cost money, the campaign would have to be profitable enough in terms of increased sales to justify the ad spend3.\n3 The advertising company, on the other hand, is more interested in assessing the effectiveness of advertising, so would be happier to see a strong positive effect of advertising.The main interest is in (functions of) two quantities – \\(\\text{Pr}[Y^{a=0} = 1]\\), which is the proportion of sales in a population not shown the ad; and \\(\\text{Pr}[Y^{a=1} = 1]\\), which is the proportion of sales in a population which is shown the ad. These can be used to calculate the risk difference / uplift (\\(\\text{Pr}[Y^{a=1} = 1] - \\text{Pr}[Y^{a=0} = 1]\\)), the risk ratio / relative risk / lift (\\(\\text{Pr}[Y^{a=1} = 1] / \\text{Pr}[Y^{a=0} = 1]\\)), or any other quantity listed in the tables here. The risk difference can then be used to calculate the additional profit from showing the ad compared to doing nothing.\nThe cleanest way of estimating these probabilities is through an experiment. First, a target population – say, users of a given product – is identified, and then, this group is randomly split into a treatment and a control group. Individuals assigned to the treatment group are eligible to see an ad for the newer, updated version of the product, and the control group is ineligible to view the ad. The campaign runs for a two week period, where individuals in the treatment group see an ad multiple times. The outcome for this campaign is sales of the newer product in each of the two groups in the six-week period from the start of the campaign.\nBefore writing any code, the tidyverse packages is loaded.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nA DAG, or a directed acyclic graph for this problem can be visualized as follows\n\n\nCode\ndag &lt;- ggdag::dagify(\n  A ~ Z + U, \n  Y ~ A + U, \n  exposure = \"A\", outcome = \"Y\", \n  coords = list(x = c(Z = 1, A = 2, U = 2.5, Y = 3), \n                y = c(Z = 1, A = 1, U = 1.2, Y = 1))\n) %&gt;% \n  ggdag::ggdag() +\n  ggdag::theme_dag()\n\nplot(dag)\n\n\n\n\n\n\n\n\n\nwhere \\(Y\\) is a binary outcome (bought the newer version of the product or not during the follow-up period), \\(A\\) is a binary treatment variable (saw the ad or not), \\(Z\\) is a binary variable indicating group assignment (i.e., randomly assigned to the treatment or control group), and U is the set of (unmeasured) confounder(s), i.e., one or more variables that both impact whether an individual ends up seeing the ad, and whether they end up buying the newer product or not.\nThis DAG encodes several assumptions:\n\nGroup assignment \\(Z\\) only affects whether someone sees an ad or not (variable \\(A\\))\nThere are some unmeasured common causes (confounders) \\(U\\) that make our life difficult by potentially distorting the relationship between \\(A\\) and \\(Y\\)\n\\(Z\\) has no direct effect on Y; only an indirect effect entirely (mediated) via \\(A\\)\n\\(Z\\) has no relationship with \\(U\\), since \\(Z\\) is randomly assigned and should be balanced with respect to \\(U\\) (unless we’re unlucky and there’s some imbalance by chance)\n\nAfter the campaign is over, it is observed that a subset of the treatment group never received the treatment, i.e., didn’t see the ad at all. The main question for the analysis is then – how should these non-compliers be analyzed? In this post I look at three possible choices:\n\nShould they be analysed as part of the treatment group, despite not receiving the treatment?\nShould they be excluded from the analysis altogether?\nShould they be included in the control group since they were untreated?"
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#simulate-data",
    "href": "posts/instrumental-variable-in-RCT/index.html#simulate-data",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "Simulate data",
    "text": "Simulate data\nBefore the question in the previous section can be answered, we need to generate hypothetical data4 for this fictional ad campaign.\n4 Inspired by a comment on this post, and the Imbens, Rubin 1997 paper linked therein.What the following code does is it first partitions the population of 100,000 individuals into an 80-20% mix of compliers and non-compliers. Compliers are the individuals that would watch the ad if they were assigned to the target group. Non-compliers are the individuals who would not comply with the group they’re randomized to. More on this in the IV section below. Then, the potential outcomes for each individual under the two treatments depending on their compliance status are simulated.\nIf \\(C_i = 1\\) denotes whether individual \\(i\\) is a complier, and \\(C_i = 0\\) otherwise, then\n\\[\n\\text{Pr}[Y^{a = 0}_i = 1| C_i = 0] = \\text{Pr}[Y^{a = 1}_i = 1| C_i = 0] = 0.1\\%\n\\]where the probability of purchasing the new product is very small among the non-compliers independent of whether they’re assigned to the treatment or the control group.\nAmong the compliers, \\(\\text{Pr}[Y^{a = 0}_i = 1| C_i = 1] = 1\\%\\), i.e., 1% of the individuals would buy the new product if nobody was shown the ad. If the ad is rolled out to the full population, then \\(\\text{Pr}[Y^{a = 1}_i = 1| C_i = 1] = 11\\%\\), which leads to an average treatment effect (ATE) among the compliers of +10 percentage points.\nThese individuals are randomly assigned to the treatment (70%) or control (30%) group. Their actual treatment status (i.e., treated or untreated) is a product of the group they’re assigned to and their compliance. Their realized outcome is the outcome corresponding to the group they’re assigned to.\n\n\nCode\nsimulate_data &lt;- function(n = 100000L,\n                          pY0 = 0.01,\n                          risk_diff = 0.1, \n                          seed = 23, \n                          p_compliers = 0.8, \n                          p_treatment = 0.7,\n                          pY_non_compliers_factor = 0.1) {\n  \n  pY1 &lt;- pY0 + risk_diff\n  pY_nc &lt;- pY_non_compliers_factor * pY0\n  \n  set.seed(seed)\n  data &lt;- tibble(\n    id = 1:n,\n    # the underlying population can be stratified into \n    # never-takers and compliers\n    complier = rbinom(n, 1, prob = p_compliers), \n    # generate individual potential outcomes\n    # under control and treatment, i.e., Pr[Y^0 = 1]\n    Y0 = case_when(\n      complier == 0 ~ rbinom(n, 1, pY_nc),\n      complier == 1 ~ rbinom(n, 1, pY0),\n    ),\n    # assuming a constant effect of +10 percentage points\n    # among the compliers, and no average effect under the never-takers\n    Y1 = case_when(\n      complier == 0 ~ rbinom(n, 1, pY_nc),\n      complier == 1 ~ rbinom(n, 1, pY1)\n    ), \n    # treatment assigned at random\n    # 70-30 split into treatment / control\n    Z = rbinom(n, 1, prob = p_treatment),\n    # treatment uptake depends on \n    # being assigned to treatment (Z = 1)\n    # AND being a complier (C = 1)\n    A = Z * complier,\n    # generate observed response using the \n    # consistency equation\n    Y = (1 - Z) * Y0 + Z * Y1\n  )\n\n  return(data)\n}\n\n# creating these variables as they'll be helpful later on\n# population size\nn &lt;- 100000L\n\n# P[Y^0 = 1]\npY0 &lt;- 0.01\n# P[Y^1 = 1], ATE of +10 pct. pts.\npY1 &lt;- pY0 + 0.1\n\ndata &lt;- simulate_data(n = n, pY0 = pY0, risk_diff = 0.1)\n\nglimpse(data)\n\n\nRows: 100,000\nColumns: 7\n$ id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ complier &lt;int&gt; 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1…\n$ Y0       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Y1       &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ Z        &lt;int&gt; 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1…\n$ A        &lt;int&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1…\n$ Y        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n\n\nIn this simulated dataset, we’ve got information on compliance and the potential outcome under each treatment at the individual level. However, from a real experiment, only the \\(Z, A, Y\\) columns would be observed.\nThe exposition pipe %$% operator – similar to the pipe %&gt;% operator – is exported from the magrittr package and used with base::table() to expose the variables in the data frame to the table function to produce contingency tables.\n\nlibrary(magrittr, include.only = \"%$%\")\n\ndata %$% \n  table(complier, Z) %&gt;% \n  prop.table(margin = 2)\n\n        Z\ncomplier         0         1\n       0 0.1974162 0.2003373\n       1 0.8025838 0.7996627\n\n\nSince the treatment \\(Z\\) is randomly assigned, the proportion of compliers in each group is nearly the same as expected.\nThere are several effects that can be estimated for this problem each with its own advantages and disadvantages."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#intention-to-treat-itt",
    "href": "posts/instrumental-variable-in-RCT/index.html#intention-to-treat-itt",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "Intention-to-treat (ITT)",
    "text": "Intention-to-treat (ITT)\nThe intention-to-treat (ITT) effect is the effect of being assigned to the treatment instead of the effect of the treatment itself. These are identical when treatment compliance / adherence is perfect, i.e, when all the individuals only take the treatment they are assigned to, but not otherwise.\nFor this problem, the ITT analysis would analyse individuals based on the group they were assigned to, and not the treatment they ultimately received. Those in the treatment group who didn’t see a single ad would be analysed as part of the treatment group rather than being excluded from the analysis, or being analysed as part of the control group.\nAn advantage of an ITT analysis is that randomization preserves the balance of confounders in both the treatment and control groups, so the \\(Z-Y\\) association remains unconfounded and a valid, albeit conservative5 effect of assigning the treatment at the population level. However, this validity would be affected6 if this ad is rolled out to another target population – a different period in time, or another geographical location – with a different proportion of (non-)compliers7.\n5 Section 22.1 of the what-if book mentions that while the ITT estimate is usually conservative, it’s not guaranteed to be attenuated compared to the per-protocol effect (described in the next section) for 1) non-inferiority trials, or 2) if the per-protocol effect is not monotonic for all individuals and non-compliance is high. It also lists other counterarguments (e.g., lack of transportability) against the ITT effect.6 Rerunning the data simulation chunk above with simulate_data(p_compliers = 0.4) %&gt;% mutate(diff = Y1 - Y0) %&gt;% pull(diff) %&gt;% mean() instead of 0.8 leads to an ITT estimate of +4 instead of +8 percentage points, even though the effect of the treatment itself is still +10 percentage points.7 For a garden-variety ad campaign, the proportion of compliers could be effectively random, unless the ad in question is visually appealing, or contentious. On the contrary, for something like a vaccine in a global pandemic, compliance could vary wildly between the trial and rollout at the population level. Source: reading internet discussions between 2021-2022.\nEstimators / models\nSince both \\(Y\\) and \\(Z\\) are binary variables, an estimate of the ITT effect can be obtained by fitting a simple logistic regression model, and using the marginaleffects package to perform g-computation / (marginal / model-based) standardization to get the risk difference (and the associated CIs)\n\ndata %&gt;% \n  glm(Y ~ Z, family = binomial(link = \"logit\"), data = .) %&gt;%\n  marginaleffects::avg_comparisons(variables = \"Z\")\n\n\n Estimate Std. Error  z Pr(&gt;|z|)   S  2.5 % 97.5 %\n   0.0807     0.0012 67   &lt;0.001 Inf 0.0783  0.083\n\nTerm: Z\nType:  response \nComparison: mean(1) - mean(0)\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n\n\nor by fitting an identity-link logistic regression where the estimated coefficient for \\(Z\\) can be directly interpreted as the risk difference\n\ndata %&gt;% \n  glm(Y ~ Z, family = binomial(link = \"identity\"), data = .) %&gt;% \n  broom::tidy(conf.int = TRUE) %&gt;% \n  mutate(across(.cols = where(is.numeric), .fns = ~ round(.x, 4)))\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   0.0087    0.0005      16.2       0   0.0077    0.0098\n2 Z             0.0807    0.0012      67.0       0   0.0783    0.083 \n\n\nThe use of glm + g-computation here is a bit overkill, as the same estimate can be obtained by looking at the 2x2 table of proportions scaled to sum to 1 within each column\n\ndata %$%\n  table(Y, Z) %&gt;% \n  print() %&gt;% \n  prop.table(margin = 2)\n\n   Z\nY       0     1\n  0 29772 63715\n  1   261  6252\n\n\n   Z\nY             0           1\n  0 0.991309559 0.910643589\n  1 0.008690441 0.089356411\n\n\nand taking the difference of \\(P[Y = 1 | Z]\\) in the two groups to give the risk difference \\(\\hat{p}_1 - \\hat{p}_0\\), where \\(\\hat{p}_0 = \\text{Pr}[Y = 1 | Z = 0]\\) and \\(\\hat{p}_1 = \\text{Pr}[Y = 1 | Z = 1]\\)\n\nround(0.089356411 - 0.008690441, 4)\n\n[1] 0.0807\n\n\nAdditionally, a wald-type CI can be obtained using the formula for variance mentioned (using counts) here or (using proportions) here\n\\[\nSE(\\hat{p}_1 - \\hat{p}_0) = \\sqrt{\\frac{\\hat{p}_0 (1 - \\hat{p}_0)}{n_0} + \\frac{\\hat{p}_1 (1 - \\hat{p}_1)}{n_1}}\n\\]\nwhich results in the same standard error\n\n\nCode\nvar_p0 &lt;- (0.008690441 * (1 - 0.008690441)) / (261 + 29772)\nvar_p1 &lt;- (0.089356411 * (1 - 0.089356411)) / (6252 + 63715)\n\nround(sqrt(sum(var_p0, var_p1)), digits = 4)\n\n\n[1] 0.0012\n\n\nThis paper mentions other ways of estimating the risk difference from different models, e.g. linear regression + sandwich estimator for the standard errors, which are again identical to the ones from the other methods above\n\nlm_mod &lt;- lm(Y ~ Z, data = data) \n\nlm_mod %&gt;% \n  broom::tidy() %&gt;% \n  select(term, estimate, model_SE = std.error) %&gt;% \n  mutate(robust_SE = sqrt(diag(sandwich::vcovHC(lm_mod))))\n\n# A tibble: 2 × 4\n  term        estimate model_SE robust_SE\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  0.00869  0.00141  0.000536\n2 Z            0.0807   0.00168  0.00120 \n\n\nHowever, getting a distribution of this effect as opposed to just a 95% CI can be more informative from a decision making point of view. This distribution can be obtained three ways – 1) plugging the estimate and its standard error into a normal distribution and simulating effect sizes from that; 2) using the (nonparametric) bootstrap to resample the data and getting the bootstrap distribution of effect sizes; 3) using a Bayesian model to get the posterior distributions of the parameters from the model.\nFor the rest of the analyses, I pick option 3, where the simple (Bayesian) Beta-Binomial model is fit to this data separately within the treatment and the control groups. Taking the difference of these two (Beta) posterior distributions produces the posterior distribution of the risk difference. The bayesAB package in R can quickly fit Beta-Binomial models, or the brms package can be used for more general models.\n\n\nBack to ITT analysis\nA \\(\\text{Beta}(\\alpha = 1, \\beta = 9)\\) prior is specified for the probability of success in each arm reflecting the knowledge that the we can expect the conversion in each group to be somewhere between 0%-70%. Normally this depends on the product in question – expensive and / or infrequently bought products are not expected to lead to very large conversion rates.\nThe following shows a summary of 10,000 draws from the prior distribution for the parameters\n\nsummary(rbeta(1e5, 1, 9))\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0000017 0.0313705 0.0740948 0.1002186 0.1430362 0.7405391 \n\n\nwhich implies the following prior distribution for the risk difference\n\nsummary(rbeta(1e5, 1, 9) - rbeta(1e5, 1, 9))\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.6858270 -0.0709874 -0.0000165  0.0000020  0.0699090  0.6885754 \n\n\nA priori, we expect the difference to be pretty small with mean of close to 0, but allowing for large differences of +/- 60-70%. We can also visualize these distributions\n\n\nCode\nbind_rows(\n  tibble(x = rbeta(1e5, 1, 9), cat = \"Prior conversion probability\"), \n  tibble(x = rbeta(1e5, 1, 9) - rbeta(1e5, 1, 9), \n         cat = \"Prior risk difference\")\n) %&gt;% \n  ggplot(aes(x = x, group = cat, color = cat)) + \n  geom_density() + \n  theme_classic() + \n  xlab(\"\") + \n  scale_x_continuous(labels = scales::percent) +\n  facet_wrap(~ cat, scales = \"free\") + \n  guides(color = \"none\")\n\n\n\n\n\n\n\n\n\nTo get the posterior distribution for conversion in each arm, the counts from the 2x2 contingency table can be plugged into the formula for Beta posterior distribution \\(\\text{Beta}(y + \\alpha, n - y + \\beta)\\), where \\(y\\) corresponds to the count for \\(Y = 1\\), and \\(n-y\\) for \\(Y = 0\\).\n\ndata %$% table(Y, Z)\n\n   Z\nY       0     1\n  0 29772 63715\n  1   261  6252\n\n\nThis gives\n\\[\np (\\alpha_0, \\beta_0|y) \\sim \\text{Beta}(261 + 1, 29772 + 9) = \\text{Beta}(262, 29781)\n\\]\nfor the control group, and\n\\[\np (\\alpha_1, \\beta_1|y) \\sim \\text{Beta}(6252 + 1, 63715 + 9) = \\text{Beta}(6253, 63724)\n\\]\nfor the treatment group, visualized as follows\n\n\nCode\nposterior_categories &lt;- c(\n  \"Conversion probability \\n(control group)\", \n  \"Conversion probability \\n(treatment group)\",\n  \"Risk difference\"\n)\n\nsample_from_beta_posterior &lt;- function(theta_0, theta_1, \n                                       seed = 23, n_samples = 1e5) {\n  set.seed(seed)\n  posterior_control &lt;- rbeta(n_samples, theta_0[1], theta_0[2])\n  posterior_treatment &lt;- rbeta(n_samples, theta_1[1], theta_1[2])\n  risk_difference &lt;- posterior_treatment - posterior_control\n  \n  tibble(\n    id = rep(1:n_samples, times = 3),\n    cat = rep(posterior_categories, each = n_samples),\n    x = c(posterior_control, posterior_treatment, risk_difference)\n  )\n}\n\ntrue_effect &lt;- tibble(\n  x = c(NA_real_, NA_real_, pY1 - pY0), \n  cat = posterior_categories\n)\n\nplot_posteriors &lt;- function(posterior, effect = true_effect) {\n  posterior %&gt;% \n    ggplot(aes(x = x, group = cat, color = cat)) + \n    geom_density(trim = TRUE) + \n    geom_vline(data = effect, \n               aes(xintercept = x), \n               color = \"gold3\", \n               linetype = \"dashed\",\n               linewidth = 1.3,\n               na.rm = TRUE) + \n    theme_classic() + \n    theme(plot.margin = margin(0.5, 0.5, 0.5, 0.5, \"cm\")) + \n    xlab(\"Posterior distribution\") + \n    scale_x_continuous(labels = scales::label_percent(accuracy = 0.1)) +\n    facet_wrap(~ cat, scales = \"free\", ncol = 3) + \n    guides(color = \"none\")\n}\n\nITT_posterior &lt;- sample_from_beta_posterior(\n  theta_0 = c(262, 29781), theta_1 = c(6253, 63724)\n)\n\nplot_posteriors(ITT_posterior)\n\n\n\n\n\n\n\n\n\nThe posterior for the risk difference completely fails to capture the underlying true effect size of +10 percentage points. However, since the dataset is simulated with the potential outcomes \\(\\{Y_{i}^{z = 0},\\ Y_{i}^{z = 1}\\}\\) at the individual level, we can see that the estimate of +8 percentage points is correct for this population.\n\ntrue_ITT_effect &lt;- data %&gt;% \n  mutate(diff = Y1 - Y0) %&gt;% \n  pull(diff) %&gt;% \n  mean() %&gt;% \n  print()\n\n[1] 0.08069\n\n\nThe posterior distribution is summarized via the posterior mean, mode, and 95% (equal-tailed) credible intervals, which are nearly identical to the ones above.\n\n\nCode\nITT_risk_difference &lt;- ITT_posterior %&gt;% \n  filter(cat == \"Risk difference\") \n\n# get the posterior density to find the mode\n# based on this SE answer: \n# https://stackoverflow.com/a/13874750\nITT_risk_difference_density &lt;- density(ITT_risk_difference$x)\nITT_risk_difference_mode &lt;- ITT_risk_difference_density$x[which.max(ITT_risk_difference_density$y)]\n\nITT_risk_difference %&gt;% \n  summarize(\n    mean = mean(x), \n    lower = quantile(x, probs = 0.025),\n    upper = quantile(x, probs = 0.975)\n  ) %&gt;% \n  mutate(mode = ITT_risk_difference_mode, .after = mean)\n\n\n# A tibble: 1 × 4\n    mean   mode  lower  upper\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 0.0806 0.0808 0.0783 0.0830\n\n\nJust for fun, the Bayesian posterior distribution is visually compared to the normal approximation and the bootstrap distribution for the risk difference\n\n\nCode\nboot_ITT_fun &lt;- function(data, indx, ...) {\n  data &lt;- data[indx, ]\n  data %&gt;% \n    count(Z, Y) %&gt;% \n    # get P[Y = 1] within levels of Z\n    group_by(Z) %&gt;% \n    mutate(p = n / sum(n)) %&gt;% \n    ungroup() %&gt;% \n    # only keep P[Y = 1 | Z]\n    filter(Y == 1) %&gt;% \n    pull(p) %&gt;% \n    # get |p1 - p0|\n    reduce(.f = `-`) %&gt;% \n    abs()\n} \n\nITT_boot &lt;- boot::boot(data = data, \n                       statistic = boot_ITT_fun, \n                       R = 999, \n                       # multicore only works on linux\n                       parallel = \"multicore\")\n\n# plot the three distributions\nset.seed(23)\nbind_rows(\n  tibble(x = rnorm(999, 0.0807, 0.0012), y = \"Normal approximation\"), \n  tibble(x = ITT_boot$t[, 1], y = \"Bootstrap distribution\"), \n  ITT_posterior %&gt;% \n    filter(cat == \"Risk difference\") %&gt;% \n    rename(y = cat) %&gt;% \n    slice_sample(n = 999) %&gt;% \n    mutate(y = \"Beta posterior\")\n) %&gt;% \n  ggplot(aes(x = x, y = y, group = y, fill = y)) + \n  ggdist::stat_halfeye() +\n  theme_classic() + \n  theme(legend.position = \"none\", \n        plot.margin = margin(0.5, 0.5, 0.5, 0.5, \"cm\")) + \n  xlab(\"Distribution of risk difference / uplift (999 samples)\") + \n  ylab(\"\") + \n  scale_x_continuous(labels = scales::label_percent(accuracy = 0.1))\n\n\n\n\n\n\n\n\n\nAs expected, despite being philosophically different, all of these methods give nearly identical results (increasing the samples from 999 to 100,0008 would lead to them being virtually identical), but all of these underestimate the correct treatment effect of +10 percentage points.\n8 Using the boot package here complains about memory issues when trying to set R = 100,000, and I’m too lazy to write a (parallel) map here myself."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#per-protocol-pp",
    "href": "posts/instrumental-variable-in-RCT/index.html#per-protocol-pp",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "Per-protocol (PP)",
    "text": "Per-protocol (PP)\nThe per-protocol (PP) effect is generated by including all the people who adhere to the protocol, i.e., those with treatment status \\(A\\) identical to the assignment treatment \\(Z\\). Contrary to the ITT effect from the previous section, this effect is a measure of the effectiveness of the actual treatment itself, although this effect estimate is prone to bias.\nSince it’s not possible for us to measure9 an individual seeing an ad if they were assigned to the control group, i.e, individuals with \\(Z = 0, A = 1\\), this would only exclude all the individuals with \\(Z = 1\\) and \\(A = 0\\), i.e., about 14,000 individuals from the treatment group who didn’t see an ad.\n9 An individual in the control group cannot see an ad, if the ad is not shown to the individual.\ndata %$% table(A, Z)\n\n   Z\nA       0     1\n  0 30033 14017\n  1     0 55950\n\n\n\ndata %&gt;% filter(A == Z) %$% table(A, Z)\n\n   Z\nA       0     1\n  0 30033     0\n  1     0 55950\n\n\n\ndata %&gt;% \n  # only restrict analysis to those with perfect adherence / compliance\n  filter(A == Z) %$% \n  table(Y, A) %&gt;% \n  print() %&gt;% \n  sum()\n\n   A\nY       0     1\n  0 29772 49716\n  1   261  6234\n\n\n[1] 85983\n\n\nSImilar to the previous section, the posterior probability of success / conversion and the risk difference for the PP analysis can be obtained by using the same beta-binomial model from the previous section and visually assessed\n\nPP_posterior &lt;- sample_from_beta_posterior(\n  theta_0 = c(261 + 1, 29772 + 9), \n  theta_1 = c(6234 + 1, 49716 + 9)\n)\n\nplot_posteriors(PP_posterior)\n\n\n\n\n\n\n\n\nThe PP effect is exaggerated – albeit by a relatively small margin – compared to both the ITT effect and the true treatment effect of +10 percentage points."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#as-treated-at",
    "href": "posts/instrumental-variable-in-RCT/index.html#as-treated-at",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "As-treated (AT)",
    "text": "As-treated (AT)\nThe as-treated effect (AT) estimates the effect of analyzing individuals by treatment received. This analysis includes individuals in the group corresponding to the treatment actually received. So the individuals randomized to the treatment group who did not see an ad are included in the control group.\n\ndata %$% table(Y, A)\n\n   A\nY       0     1\n  0 43771 49716\n  1   279  6234\n\n\n\nAT_posterior &lt;- sample_from_beta_posterior(\n  theta_0 = c(279 + 1, 43771 + 9), \n  theta_1 = c(6234 + 1, 49716 + 9)\n)\n\nplot_posteriors(AT_posterior)\n\n\n\n\n\n\n\n\nLike the PP effect, this effect is a measure of the effectiveness of the actual treatment itself. By including the untreated from the treatment group – who had a lower probability of conversion – into the control group, the overall conversion probability in the control group is attenuated thereby leading to an exaggeration of the ATE. Thus, it overestimates the true effect and is likewise exaggerated compared to the ITT effect estimate. It is similarly prone to bias.\nWe’ve produced three estimates so far, and since we know the true effect of treatment here, we can see that they’re all biased. Great! The analysis could stop here and all the three estimates could be presented with their respective caveats.\nBut can we do better (in this experiment)?"
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#instrumental-variable-analysis-iv",
    "href": "posts/instrumental-variable-in-RCT/index.html#instrumental-variable-analysis-iv",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "Instrumental variable analysis (IV)",
    "text": "Instrumental variable analysis (IV)\nThe fourth and final method of analysis looks at the method of instrumental variables. Causal estimates for the treatment effect can be obtained by using just the three variables \\(\\{Z, A, Y\\}\\) if the assumptions behind this method are plausible for our (fictional) ad campaign.\nFor an instrumental variable analysis to be valid, we need an instrument (say \\(Z\\)) that needs to satisfy the first three conditions and at least the fourth or the fifth condition\n\nRelevance condition – Be correlated (hopefully strongly) with the treatment variable (in our case \\(A\\))\nIgnorability - Be uncorrelated with all measured and unmeasured confounders (\\(U\\)) of the \\(A \\rightarrow Y\\) relationship and share no common causes with \\(Y\\)\nExclusion restriction – have no direct impact on the outcome \\(Y\\), only an indirect effect entirely mediated via the treatment \\(A\\)\n(Weak) Homogeneity – for a binary \\(Z-A\\) system, if \\(Z\\) is not an effect modifier of the \\(A-Y\\) relationship, i.e.,\n\\[\\mathbb{E}[Y^{a = 1} - Y^{a = 0} | Z = 1, A = a] = \\mathbb{E}[Y^{a = 1} - Y^{a = 0} | Z = 0, A = a]\\]\nthen the IV estimate can be interpreted as the average treatment effect among the treated (ATT) in the population. If the ATT is identical to the ATU(ntreated), then the IV estimate can be interpreted as the ATE in the population. Other homogeneity conditions are described in section 16.3 of the what-if book. Appendix 2.1 of the Hernán, Robins 2006 paper has some stuff on this too.\nMonotonicity - The trial population can be partitioned into four-latent subgroups / prinicipal strata defined by a combination of \\(A, Z\\) - always takers, compliers, never-takers, and defiers. Always (or never) takers are people who would always (or never) take the treatment independent of group assignment. Compliers would comply with the treatment corresponding to the group they are assigned to. Defiers would do the opposite to the group they’re assigned to. In the absence of defiers and always takers, the IV estimand is the complier average causal effect (CACE) or the local average treatment effect (LATE). This can be written as\n\\[\n\\mathbb{E}[Y^{a = 1} - Y^{a = 0} | A^{z = 1} \\gt A^{z = 0}] = \\frac{\\mathbb{E}[Y = 1 | Z = 1] - \\mathbb{E}[Y = 1 | Z = 0]}{\\mathbb{E}[A = 1 | Z = 1] - \\mathbb{E}[A = 1 | Z = 0]}\n\\]\n\nThe variable \\(Z\\) from the DAG for this problem\n\n\nCode\nplot(dag)\n\n\n\n\n\n\n\n\n\nseems to fulfill the first three conditions, since individuals are randomly assigned into treatment and control groups. This makes it a causal instrument, as an individual cannot choose to see an ad unless they’re assigned to the treatment group.\nRegarding homogeneity, it can be argued that the assignment of an individual10 to the group cannot be an effect modifier, since this is done by a random number generator and so should not affect the potential outcomes.\n10 In the presence of interference, e.g. individuals in the same household being assigned to different groups but affecting each others’ potential outcomes, the unit of assignment and analysis would be at the household level.11 Not that anyone would choose to see an ad.Since the ad is randomized at the individual level, there can never be any defiers (i.e., individuals assigned to the control group can never deliberately switch over to the treatment group). Always takers are also not an option, since people assigned to the control group cannot choose to take the treatment11. This essentially leaves the population to be a mix of never-takers and compliers.\n\nIV in action\nSince the instrument, treatment, and outcome are all binary, we can use the following ratio / Wald estimator for the IV estimand\n\\[\n\\text{IV} = \\frac{\\text{Pr}[Y = 1 | Z = 1] - \\text{Pr}[Y = 1 | Z = 0]}{\\text{Pr}[A = 1 | Z = 1] - \\text{Pr}[A = 1 | Z = 0]}\n\\]\nwhere the numerator is the ITT effect estimate defined a few sections above, and the denominator is a measure of compliance on the risk difference scale with regards to the assigned treatment, i.e., an association between treatment assignment \\(Z\\) and treatment uptake \\(A\\).\nDepending on whether we invoke one of the homogeneity assumptions or the monotonicity assumption, we end up with either the average causal effect in the treated, or in the population, or the average causal effect among the compliers. All the different assumptions for this are described in the Swanson et al 2018 article.\nTo fit this Beta-binomial model to the data, we can use the posterior distribution of the risk difference from the ITT effect for the numerator. For the denominator, we need to get a similar distribution of treatment compliance. Looking at the 2x2 \\(Z-A\\) table,\n\ndata %$% table(A, Z)\n\n   Z\nA       0     1\n  0 30033 14017\n  1     0 55950\n\n\nthe group of \\(\\text{Pr}[A = 1 | Z = 0]\\) is (expected to be) 0, since those assigned to the control group should not be able to see the ad. We can pick a degenerate (Beta) prior distribution12 with \\(\\alpha = 0,\\ \\beta &gt; 0\\), e.g., \\(\\text{Beta}(0, 1)\\), which results in a Beta posterior with shape parameters \\(\\alpha = 0 + 0 = 0\\) and \\(\\beta = 30033 + 1 = 30034\\). Since \\(\\alpha = 0\\), all draws from this posterior distribution are exactly zero.\n12 Since I’m not using Stan to fit the model here, I can get away with this. Using Stan, I’d pick something like a \\(\\text{Beta}(1, 10000)\\) distribution that puts a very small non-zero probability for \\(\\text{Pr}[A = 1 | Z = 0]\\). I don’t know for sure whether the degenerate prior would cause issues for Stan to be honest.\nsummary(rbeta(1e2, 0, 30034))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\n\nFor \\(\\text{Pr}[A = 1 | Z = 1]\\), we can expect the compliance to be reasonably high, so the \\(\\text{Beta}(7, 3)\\) with a mean compliance of 70%, and a range of 10%-100% might be a good choice. With the large sample sizes in the dataset, the prior is going to be dominated by the likelihood anyway.\n\nsummary(rbeta(1e4, 7, 3))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.09285 0.60786 0.71188 0.69917 0.80307 0.98952 \n\n\nUsing the counts from the \\(A-Z\\) table above, we get the \\(\\text{Beta}(55957, 14020)\\) posterior with \\(\\alpha = 7 + 55950\\) and \\(\\beta = 3 + 14017\\). Taking a difference of these two posteriors would leave this posterior unchanged, so we can produce the corresponding IV posterior distribution\n\n\nCode\nset.seed(23)\ncompliance_posterior &lt;- rbeta(nrow(ITT_risk_difference), 55957, 14020)\n\nIV_labels &lt;- c(\"Intention-to-treat effect\", \n               \"Proportion of compliers\", \n               \"Instrumental variable estimate\")\n\nIV_posterior &lt;- tibble(\n  id = rep(1:nrow(ITT_risk_difference), times = 3),\n  cat = factor(rep(IV_labels, each = nrow(ITT_risk_difference)), \n               levels = IV_labels),\n  x = c(ITT_risk_difference$x, compliance_posterior, \n        ITT_risk_difference$x / compliance_posterior)\n)\n\ntrue_effect_IV &lt;- true_effect %&gt;% \n  mutate(cat = factor(IV_labels, levels = IV_labels))\n\nplot_posteriors(IV_posterior, true_effect_IV)\n\n\n\n\n\n\n\n\n\n\nIV_posterior %&gt;% \n  filter(cat == IV_labels[3]) %&gt;% \n  summarize(mean = mean(x), \n            lower = quantile(x, probs = 0.025), \n            upper = quantile(x, 0.975))\n\n# A tibble: 1 × 3\n   mean  lower upper\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 0.101 0.0979 0.104\n\n\nCompared to the ITT, PP, and AT estimates, the IV estimate is the closest to the true treatment effect of +10 percentage points, which makes sense as all the IV conditions are valid here due to the use of a randomized experiment with the causal instrument \\(Z\\). However, if any of the conditions (monotonicity, relevance, etc.) were to weaken, this would lead to bias in the IV estimate.\nSome recommend producing bounds13 for the IV estimate since it’s usually not point identified. For the case of binary \\(Z, A, Y\\), we can use ivtools::ivbounds() to produce the natural bounds – which are between 8%-28% for the ATE (CRD in the result)\n13 Note to (future) self: look at the Balke, Pearl 1997 and Swanson et al 2018 papers, and the causaloptim R package.\ndata %&gt;% \n  ivtools::ivbounds(data = ., \n                    Z = \"Z\", X = \"A\", Y = \"Y\", \n                    monotonicity = TRUE) %&gt;% \n  summary()\n\n\nCall:  \nivtools::ivbounds(data = ., Z = \"Z\", X = \"A\", Y = \"Y\", monotonicity = TRUE)\n\nThe IV inequality is not violated\n\nSymbolic bounds:\n   lower upper  \np0 p10.0 1-p00.0\np1 p11.1 1-p01.1\n\nNumeric bounds:\n       lower    upper\np0   0.00869  0.00869\np1   0.08910  0.28944\nCRD  0.08041  0.28075\nCRR 10.25255 33.30515\nCOR 11.15758 46.46413\n\n\nThe width of the bounds is 20 percentage points, which matches the formula for the width14 – \\(\\text{Pr}[A = 1 | Z = 0] + \\text{Pr}[A = 0 | Z = 1]\\) – where the former is 0, since we have no defiers, and 20% for the second quantity, which is the proportion of never-takers.\n14 given in some of the references belowThe slight difference between the true value and the estimate is due to sampling variability, which can be assessed by simulating multiple datasets with different seeds to give a mean ATE very close to +10 percentage points\n\n\nCode\nmap_dbl(\n  .x = 1:100, \n  .f = ~ {\n    .x %&gt;% \n      simulate_data(seed = .) %&gt;% \n      mutate(diff = (Y1 - Y0) / 0.8) %&gt;%\n      pull(diff) %&gt;% \n      mean()\n    }) %&gt;% mean()\n\n\n[1] 0.0999695"
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#all-the-estimates-in-a-single-plot",
    "href": "posts/instrumental-variable-in-RCT/index.html#all-the-estimates-in-a-single-plot",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "All the estimates in a single plot",
    "text": "All the estimates in a single plot\nWe can look at all four estimates in a single plot\n\n\nCode\nlist(\n  \"Instrumental variable\" = IV_posterior, \n  \"As-treated\" = AT_posterior, \n  \"Per-protocol\" = PP_posterior, \n  \"Intention-to-treat\" = ITT_posterior\n) %&gt;% \n  map2_dfr(.x = ., .y = names(.), .f = ~ {\n    .x %&gt;% \n      filter(str_detect(cat, \"Risk|Instrument\")) %&gt;% \n      mutate(cat = {{ .y }})\n    }) %&gt;% \n  mutate(\n    cat = factor(cat, levels = c(\n      \"Instrumental variable\", \"As-treated\",\n      \"Per-protocol\", \"Intention-to-treat\"\n  ))) %&gt;% \n  ggplot(aes(x = x, y = cat, group = cat, fill = rev(cat))) + \n  ggdist::stat_halfeye(\n    show.legend = FALSE\n  ) +\n  geom_vline(\n    xintercept = pY1 - pY0, \n    color = \"gray50\", \n    linetype = \"dashed\",\n    linewidth = 1.3\n  ) +\n  geom_vline(\n    xintercept = true_ITT_effect, \n    color = \"gray20\", \n    linetype = \"dotted\",\n    linewidth = 1.3\n  ) +\n  theme_classic() + \n  xlab(paste0(\n    \"Posterior distribution of risk difference / uplift\\n\", \n    \"(Dotted line: ATE in this population; dashed line: true ATE)\"\n  )) + \n  ylab(\"\") + \n  scale_x_continuous(labels = scales::label_percent(accuracy = 0.1))\n\n\n\n\n\n\n\n\n\nThe IV estimate is the closest to the true treatment effectiveness of +10 percentage points (in gray), with the AT and the PP effects both showing some bias.\nThe simulated data analyzed here are from a relatively clean setting where the treatment effectiveness, sample sizes, and strength of association between A-Z are all very high. If any of these conditions weaken, the analysis may become more challenging."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#profit-distribution",
    "href": "posts/instrumental-variable-in-RCT/index.html#profit-distribution",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "Profit distribution",
    "text": "Profit distribution\nSo we have two valid estimates – the ITT and the IV estimate. Can we use these estimates to calculate the total profit from this campaign, as well as other hypothetical campaigns with varying treatment / control splits and compliance?\nThis assumes that\n\nthe effect of treatment among the compliers and the never-takers would be the same\nthe conversion probabilities specified below would apply to these other populations\ncompliance varies randomly between populations\n\nData from other experiments, if available, can be used to get more realistic estimates by capturing the heterogeneity in the estimated probabilities (e.g., by meta-analysis).\nTo estimate the total profit, we need the following probability of conversion\n\\[\n\\text{Pr}[Y = 1] = \\sum_z \\text{Pr}[Y = 1 | Z = z] \\text{Pr}[Z = z]\n\\]\nwhere \\(\\text{Pr}[Z = z]\\) is the proportion of people assigned to treatment (\\(Z = 1\\)) and control (\\(Z = 0\\)), and the \\(\\text{Pr}[Y = 1 | Z = z]\\) is the ITT estimate15 in each arm. The ITT estimate under the monotonicity assumption can be further decomposed – using the law of total probability – into a weighted sum of the conversion probabilities among the compliers and the never-takers16\n15 Technically an estimand, but feels odd to type out ‘estimand’ everywhere in this and following paragraphs instead of ‘estimate’.16 The \\(\\text{Pr}[Z = z]\\) term in the numerator on the right side of the equation is constant with respect to the summation (over \\(c\\)), so can be pulled out and cancels out the same term in the denominator.\\[\n\\text{Pr}[Y = 1 | Z = z] = \\sum_c \\text{Pr}[Y = 1 | Z = z, C = c] \\text{Pr}[C = c | Z = z]\n\\]\nwhere \\(\\text{Pr}[C = c | Z = z] = \\text{Pr}[C = c]\\) because \\(Z\\) is randomly assigned, so \\(Z \\perp\\!\\!\\!\\perp C\\), i.e., \\(Z\\) and \\(C\\) are independent.\nOur best estimate of compliance is given by\n\\[\n\\text{Pr}[C = 1] = \\text{Pr}[A = 1 | Z = 1] - \\text{Pr}[A = 1 | Z = 0]\n\\]\nand \\(\\text{Pr}[C = 0] = 1 - \\text{Pr}[C = 1]\\) is the proportion of never-takers. To get the conditional probability in the first term in the sum, we can rewrite the previous equation – shown only for \\(Z = 1\\) but is the same for \\(Z = 0\\) – after diving each term by \\(\\text{Pr}[C = 1]\\)\n\\[\n\\begin{split}\n\\frac{\\text{Pr}[Y = 1 | Z = 1]}{\\text{Pr}[C = 1]} & = \\text{Pr}[Y = 1 | Z = 1, C = 1] \\\\ & + \\text{Pr}[Y = 1 | Z = 1, C = 0] \\frac{\\text{Pr}[C = 0]}{\\text{Pr}[C = 1]}\n\\end{split}\n\\]\nThe term on the left is the ITT estimate rescaled by the proportion of compliers. The first term on the right is the conversion probability among the compliers who received the treatment \\(\\text{Pr}[Y = 1 | Z = 1, C = 1]\\), and the second term on the right is the probability of conversion among the never-takers who were assigned to the treatment, weighted by the ratio of never-takers to compliers.\nThis also shows why subtracting the two rescaled ITT estimates (i.e., the IV estimate) is the effect of treatment among the compliers – the second term on the right cancels out (under the assumption of exclusion restriction) since \\(Z\\) is randomly assigned, so the never-takers in each arm should have the same probability of conversion\n\\[\n\\text{Pr}[Y = 1 | Z = 1, C = 0] = \\text{Pr}[Y = 1 | Z = 0, C = 0]\n\\]\nand proportion of compliers \\(\\text{Pr}[C = 1 | Z = 1] = \\text{Pr}[C = 1 | Z = 0]\\), and we’re left with\n\\[\n\\text{Pr}[Y = 1 | Z = 1, C = 1] - \\text{Pr}[Y = 1 | Z = 0, C = 1]\n\\]\nwhich is the CACE from the IV analysis.\nWhile compliance (\\(C\\)) is unobserved, we can estimate all these probabilites using the combined information present in \\(Z, A, Y\\) along with the assumptions in the previous paragraphs.\nFor the proportion of compliers (\\(\\text{Pr}[C = 1]\\)), we can use the posterior distribution we obtained for the IV analysis.\n\nprop_compliers &lt;- IV_posterior %&gt;% \n  filter(cat == \"Proportion of compliers\") %&gt;% \n  select(id, p_compliers = x)\n\nprop_compliers %&gt;%\n  mutate(p_never_takers = 1 - p_compliers) %&gt;% \n  summary()\n\n       id          p_compliers     p_never_takers  \n Min.   :     1   Min.   :0.7913   Min.   :0.1933  \n 1st Qu.: 25001   1st Qu.:0.7986   1st Qu.:0.1993  \n Median : 50000   Median :0.7997   Median :0.2003  \n Mean   : 50000   Mean   :0.7996   Mean   :0.2004  \n 3rd Qu.: 75000   3rd Qu.:0.8007   3rd Qu.:0.2014  \n Max.   :100000   Max.   :0.8067   Max.   :0.2087  \n\n\nFor the probability of conversion among the never takers, we can calculate this from information among the untreated assigned to the treatment group. A \\(\\text{Beta}(1, 1)\\) prior can be assumed here to get the following posterior\n\\[\n\\text{Beta}(18 + 1, 13999 + 1) = \\text{Beta}(19, 14000)\n\\]\n\ndata %&gt;% filter(A != Z) %$% table(Y)\n\nY\n    0     1 \n13999    18 \n\npY_never_takers &lt;- tibble(\n  id = 1:1e5, \n  pY_nt = rbeta(1e5, 19, 14000)\n)\n\npY_never_takers %&gt;% summary()\n\n       id             pY_nt          \n Min.   :     1   Min.   :0.0004274  \n 1st Qu.: 25001   1st Qu.:0.0011353  \n Median : 50000   Median :0.0013325  \n Mean   : 50000   Mean   :0.0013561  \n 3rd Qu.: 75000   3rd Qu.:0.0015507  \n Max.   :100000   Max.   :0.0031606  \n\n\nFor the ITT estimates, the posterior probabilities from the ITT analysis above are used\n\nITT_posterior_subset &lt;- ITT_posterior %&gt;% \n  filter(cat != \"Risk difference\") %&gt;% \n  pivot_wider(id_cols = id, names_from = cat, values_from = x) %&gt;% \n  rename(\n        \"p_control\" = \"Conversion probability \\n(control group)\", \n        \"p_treated\" = \"Conversion probability \\n(treatment group)\"\n    ) %&gt;% \n  glimpse()\n\nRows: 100,000\nColumns: 3\n$ id        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ p_control &lt;dbl&gt; 0.008838875, 0.008459186, 0.009314263, 0.009458440, 0.008553…\n$ p_treated &lt;dbl&gt; 0.09029532, 0.08987783, 0.09138095, 0.08802061, 0.09105225, …\n\n\nPutting these together gives\n\nposterior_draws &lt;- list(\n  prop_compliers, pY_never_takers, ITT_posterior_subset\n  ) %&gt;% \n  reduce(.f = ~ full_join(.x, .y, by = \"id\")) %&gt;% \n  glimpse()\n\nRows: 100,000\nColumns: 5\n$ id          &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ p_compliers &lt;dbl&gt; 0.7993178, 0.8003960, 0.7980252, 0.7976448, 0.8001245, 0.7…\n$ pY_nt       &lt;dbl&gt; 0.0010972405, 0.0010973038, 0.0017398618, 0.0014166189, 0.…\n$ p_control   &lt;dbl&gt; 0.008838875, 0.008459186, 0.009314263, 0.009458440, 0.0085…\n$ p_treated   &lt;dbl&gt; 0.09029532, 0.08987783, 0.09138095, 0.08802061, 0.09105225…\n\n\nThe conversion probabilities among the compliers are calculated, which results in the data frame with all the posterior draws for the probabilities we need.\n\n# set seed in case a subset of the posterior draws are used\n# set.seed(23)\nposterior_draws &lt;- posterior_draws %&gt;% \n  mutate(\n    across(\n      .cols = c(p_treated, p_control), \n      # rescale the ITT probabilities by p(compliance) and\n      # shave off the effect of never takers on this probability\n      # to recover the p(Y = 1 | Z = z, C = 1)\n      .fns = ~ ((.x / p_compliers) - \n                  (pY_nt * ((1 - p_compliers) / p_compliers))), \n      .names = \"{.col}_compliers\"\n    )\n  ) %&gt;% \n  # slice_sample(n = 5000) %&gt;% \n  # select(-id)  %&gt;% \n  glimpse()\n\nRows: 100,000\nColumns: 7\n$ id                  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ p_compliers         &lt;dbl&gt; 0.7993178, 0.8003960, 0.7980252, 0.7976448, 0.8001…\n$ pY_nt               &lt;dbl&gt; 0.0010972405, 0.0010973038, 0.0017398618, 0.001416…\n$ p_control           &lt;dbl&gt; 0.008838875, 0.008459186, 0.009314263, 0.009458440…\n$ p_treated           &lt;dbl&gt; 0.09029532, 0.08987783, 0.09138095, 0.08802061, 0.…\n$ p_treated_compliers &lt;dbl&gt; 0.1126900, 0.1120181, 0.1140685, 0.1099912, 0.1135…\n$ p_control_compliers &lt;dbl&gt; 0.010782542, 0.010295105, 0.011231293, 0.011498576…\n\n\nIn principle, any of these probabilities can be replaced with other priors17 when attempting to transport these estimates to the other scenarios.\n17 rather than using the observed posteriors as priors.For the plot below, there are two dimensions\n\ncompliance with levels set to 50%, 80%, 100%\ntreatment / control split\n\n70/30% as in this experiment\n0/100% - don’t treat the population\n100/0% - treat the full population\n\n\nIt feels a bit odd to look at the combination of compliance with 0% of the population assigned to treatment, as the (non-)compliers are only identified – and the respective conditional probabilities estimated – when at least some individuals are assigned to treatment. In this case, it’s more of a hypothetical exercise using data from an experiment where &gt;0% of the population was assigned to the treatment arm. Page 41 of the Greenland et al 1999 paper mentions\n\n[…] noncompliance represents movement into a third untreated state that does not correspond to any assigned treatment (Robins, 1998).\n\nI haven’t managed to go through the Robins 1998 paper yet but it would be interesting to dig into this further.\n\nparameter_grid &lt;- expand_grid(\n  compliance = c(0.5, 0.8, 1.0),\n  split = c(0.7, 1.0, 0.0), \n  n = n\n)\n\nTotal profit distributions for each of these hypothetical experiments is calculated using the formulas specified above. To get the variability in the profit distribution, total sales \\(S \\sim \\text{BetaBin}(n, \\alpha, \\beta)\\) can be sampled from the (beta-binomial) posterior predictive distribution, where the probability of conversion \\(p \\sim \\text{Beta}(\\alpha, \\beta)\\) and \\(S \\sim \\text{Binomial}(n, p)\\) under the different interventions in different settings. The total profit can be obtained from multiplying \\(S\\) with the profit per unit, which can be fixed at (say) +100 euros.\n\nprofit_distributions &lt;- expand_grid(parameter_grid, posterior_draws) %&gt;% \n  mutate(\n    # get the ITT estimates for the hypothetical campaigns\n    # under the different parameters\n    # pY_nt is scaled by proportion of compliers\n    # as this wasn't done above\n    p_treated_hyp = ((compliance * p_treated_compliers) + \n      ((1 - compliance) * (pY_nt / p_compliers))),\n    p_control_hyp = ((compliance * p_control_compliers) + \n      ((1 - compliance) * (pY_nt / p_compliers))),\n    n_treated = round(split * n), \n    n_control = n - n_treated, \n    s_treated = rbinom(n(), n_treated, p_treated_hyp), \n    s_control = rbinom(n(), n_control, p_control_hyp), \n    total_profit = 100 * (s_treated + s_control)\n  ) %&gt;% \n  glimpse()\n\nRows: 900,000\nColumns: 17\n$ compliance          &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, …\n$ split               &lt;dbl&gt; 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, …\n$ n                   &lt;int&gt; 100000, 100000, 100000, 100000, 100000, 100000, 10…\n$ id                  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ p_compliers         &lt;dbl&gt; 0.7993178, 0.8003960, 0.7980252, 0.7976448, 0.8001…\n$ pY_nt               &lt;dbl&gt; 0.0010972405, 0.0010973038, 0.0017398618, 0.001416…\n$ p_control           &lt;dbl&gt; 0.008838875, 0.008459186, 0.009314263, 0.009458440…\n$ p_treated           &lt;dbl&gt; 0.09029532, 0.08987783, 0.09138095, 0.08802061, 0.…\n$ p_treated_compliers &lt;dbl&gt; 0.1126900, 0.1120181, 0.1140685, 0.1099912, 0.1135…\n$ p_control_compliers &lt;dbl&gt; 0.010782542, 0.010295105, 0.011231293, 0.011498576…\n$ p_treated_hyp       &lt;dbl&gt; 0.05703136, 0.05669450, 0.05812436, 0.05588362, 0.…\n$ p_control_hyp       &lt;dbl&gt; 0.006077632, 0.005833028, 0.006705751, 0.006637289…\n$ n_treated           &lt;dbl&gt; 70000, 70000, 70000, 70000, 70000, 70000, 70000, 7…\n$ n_control           &lt;dbl&gt; 30000, 30000, 30000, 30000, 30000, 30000, 30000, 3…\n$ s_treated           &lt;int&gt; 3981, 3988, 4160, 3843, 3968, 3943, 3813, 3905, 40…\n$ s_control           &lt;int&gt; 186, 172, 209, 214, 199, 189, 204, 209, 183, 159, …\n$ total_profit        &lt;dbl&gt; 416700, 416000, 436900, 405700, 416700, 413200, 40…\n\n\nThe posterior predictive distribution for the total profit is visually compared with simulated draws from the data generating process\n\n\nCode\nsimulated_profit &lt;- parameter_grid %&gt;% \n  expand_grid(id = seq(from = 20, by = 1, length.out = 20)) %&gt;% \n  pmap_dfr(\n    .f = function(compliance, split, id, n) {\n      simulate_data(n = n,\n                    p_compliers = compliance, \n                    p_treatment = split,\n                    seed = id) %&gt;%\n        summarise(total_profit = 100 * sum(Y)) %&gt;% \n        tibble(compliance, split, .)\n    }\n  )\n\nprofit_distributions_plot_data &lt;- list(\n  \"extrapolated\" = profit_distributions %&gt;% \n    select(compliance, split, total_profit),\n  \"simulated\" = simulated_profit\n  ) %&gt;% \n  map(\n    .f = ~ {\n      .x %&gt;% \n        mutate(\n          total_profit = total_profit / 1e5, \n          compliance = paste0(\"Compliers: \", compliance * 100, \"%\"), \n          compliance = factor(compliance, \n                        levels = c(\"Compliers: 50%\",\n                                   \"Compliers: 80%\",\n                                   \"Compliers: 100%\")),\n          split = paste0(\"Treatment: \", split * 100, \"%\"), \n          split = factor(split, \n                         levels = c(\"Treatment: 0%\", \n                                    \"Treatment: 70%\", \n                                    \"Treatment: 100%\"))\n        )\n    }\n  )\n\nprofit_distributions_plot_data %&gt;% \n  pluck(\"extrapolated\") %&gt;% \n  ggplot(aes(x = total_profit)) + \n  geom_density(trim = TRUE) + \n  geom_vline(\n    data = profit_distributions_plot_data %&gt;% pluck(\"simulated\"), \n    aes(xintercept = total_profit), color = \"maroon\"\n  ) + \n  theme_bw() + \n  facet_wrap(vars(split, compliance), scales = \"free\") + \n  #facet_grid(compliance ~ split, scales = \"fixed\") +\n  xlab(\"Total profit (multiples of 100,000 EUR, per 100,000 individuals)\")\n\n\n\n\n\n\n\n\n\nThe middle column shows the total profit distribution for this experiment with a 70-30% split and 80% compliance. In this fictional scenario, the ad campaign has a strong positive impact on the total profits (90,000 under 0% treated and 80% compliance vs 900,000 under 100% treated) so with the gift of hindsight, it could have been rolled out to the full population.\nIf you’re wondering why the posterior predictive distribution (black line) in each panel is not centered at the mean of the vertical lines (in maroon), it’s from using the estimated probabilities from the original dataset which differ from the true probabilities because of sampling variability18.\n18 This can be checked by using the true probabilities, i.e., p_compliers = 0.8, pY_nt = 0.001, p_treated_compliers = 0.1, p_control_compliers = 0.01 instead of the posterior distributions for these probabilities."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#ignored-complexities",
    "href": "posts/instrumental-variable-in-RCT/index.html#ignored-complexities",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "Ignored complexities",
    "text": "Ignored complexities\nIn this post, I ignored the complexity where the exposure among the treated – i.e., number of impressions of the ad – would be continuous, possibly something like this\n\n\nCode\nset.seed(23)\ntibble(impressions = pmax(round(rgamma(5e4, shape = 4, scale = 3)), 1)) %&gt;% \n  ggplot(aes(x = impressions)) + \n  geom_bar() +\n  #stat_ecdf() + \n  theme_classic() + \n  xlab(\"Impressions\") + \n  scale_x_continuous(breaks = seq(0, 50, 5), labels = seq(0, 50, 5))\n\n\n\n\n\n\n\n\n\nIgnoring the actual impressions effectively assumes treatment variation irrelevance, i.e, seeing 1 impression is as effective as seeing 20 impressions with regards to conversion. This might be fun to explore in a future post on binary instruments, with a continuous exposure19. Baiocchi et al. (section 12) describes an analysis with continuous treatments.\n19 although all references mention that this complicates the analysisIssue with handling partial exposures, i.e., people with one or more partial impressions are also not tackled here.\nFor building more complex / full Bayesian IV models using brms / Stan, see this, this, this, or this."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#references",
    "href": "posts/instrumental-variable-in-RCT/index.html#references",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "References",
    "text": "References\nThese are the main references I’m using for this post20.\n20 although I can’t claim to have read and mastered all these.\nSimulating data for instrumental variables - Andrew Gelman blog post link, or this link to statistical rethinking models implemented using brms\nHernán, Robins - Causal Inference - What if? - Chapters 16 (instrumental variables), and 22 (sections on intention-to-treat and per-protocol analyses)\nGelman, Hill, Vehtari - Regression and other stories, Sections 21.1-21.2\nHernán, Robins 2006 - Instruments for causal inference: an epidemiologist’s dream?\nBurgess, Small, Thompson 2017 - A review of instrumental variable estimators for Mendelian randomization\nBaiocchi, Cheng, Small 2014 - Instrumental variable methods for causal inference\n\nGood review paper for different IV situations\n\nSussman, Hayward 2010 - An IV for the RCT: using instrumental variables to adjust for treatment contamination in randomised controlled trials\nNaimi, Whitcomb 2020 - Calculating risk differences and risk ratios from regression models\nImbens, Rubin 1997 - Bayesian inference for causal effects in randomized experiments with noncompliance\nBalke, Pearl 1997 - Counterfactual Probabilities: Computational Methods, Bounds and Applications\nSwanson et al 2018 - Partial Identification of the Average Treatment Effect Using Instrumental Variables: Review of Methods for Binary Instruments, Treatments, and Outcomes\n\nNote to future self: good rabbit hole to go down to calculate bounds on the ATE under different assumption sets\n\nGreenland, Pearl, Robins 1999 - Confounding and Collapsibility in Causal Inference\nRobins 1998 - Correction for non-compliance in equivalence trials (free link)"
  },
  {
    "objectID": "posts/mathjax-scrollable-box-display-mode/index.html",
    "href": "posts/mathjax-scrollable-box-display-mode/index.html",
    "title": "Scrollable box for long math equations on mobile",
    "section": "",
    "text": "While writing new posts, I preview them on (Google) Chrome on my laptop before publishing. The three people who read my blog posts usually read them on mobile. So one fine day I decided to check what the posts look like on mobile, and I found something mildly annoying\n\n\n\n\n\nThe long math equation makes the page wider on mobile and adds whitespace to the rest of the page. This is not the case with code and output blocks, which are automatically put in scrollable containers if their width is larger than the screen width.\nWhich made me wonder — how can I do this for math equations as well? It definitely seemed possible from looking at Wikipedia. Thus began a three-day journey into the dark art of tweaking CSS files.\nThe most helpful thing I learned in this process is how to (pre)view posts — as they would show on mobile screens — on a laptop browser1\n1 What’s cool is that it’s possible to see how it would render on different handsets.\n\n\n\n\nUntil quarto 1.4, there seems to be no parameter that can put a scrollable box around the math equations (for code it’s as simple as specifying code-overflow: scroll in the YAML post header, or more generally, the _metadata.yml file).\nAfter some googling, I came across this solution\n:::{style=\"overflow-x:auto;overflow-y:hidden;\"}\n$$\n... long equation goes here ...\n$$\n:::\nbut I wanted something that would automatically apply to all posts.\nThe styles.css file in the root directory seemed to be the place to specify that MathJax should render long equations in scrollable containers. Based on this stackoverflow answer, I added the following code to the styles.css file (mjx-container is recommended for MathJax version 3, which is what Quarto seems to be using)\nmjx-container {\n  display: inline-flex;\n  overflow-x: auto;\n  overflow-y: hidden;\n  max-width: 100%;\n}\nThis fixed the equation overflow issue on Firefox2 (version 123.0)\n2 which has ‘mobile mode’ as well in developer tools\n\n\n\n\nbut left ugly scroll bars in inline math in Chrome both on mobile (see the scroll bar under \\(\\beta_0\\))\n\n\n\n\n\nand on desktop3\n3 (version 122.0.6261.94)\n\n\n\n\nwhich looks horrible. Changing to display: inline-grid didn’t lead to any visible changes, and to display: block led to all inline math getting rendered in separate blocks.\nAfter some more googling, I came across this suggestion\n.MathJax {\n  overflow-x: auto;\n  overflow-y: hidden;\n  max-width: 100%;\n}\nwhich fixed the unnecessary inline scroll bar issue (on Chrome)\n\n\n\n\n\nand continued to have scroll bars for long display mode math equations (on Chrome)\n\n\n\n\n\nHowever, this was not an ideal solution, as any long inline math equation on mobile would now overflow the page\n\n\n\n\n\nwhereas scroll-bar-on-inline-overflow would’ve been ideal, if it didn’t put unnecessary scroll bars elsewhere\n\n\n\n\n\nSince I was a bit sick of this exercise, it was quite simple to track down the few long inline equations in some of the already published posts and change them from inline to display mode so they wouldn’t overflow the page4, and add a note to self to not use long inline equations for future posts.\n4 which may still overflow on very small (mobile) screensTo recap, the ideal solution would’ve been one that puts scroll bars on long inline and display math equations if they overflow the screen (depending on the device type), but not put scroll bars where the text clearly fits on screen."
  },
  {
    "objectID": "posts/simulating-and-modelling-competing-risk-data/index.html",
    "href": "posts/simulating-and-modelling-competing-risk-data/index.html",
    "title": "Simulating data from the cause-specific hazard approach to competing risks",
    "section": "",
    "text": "In a previous post, I used the cumulative hazard inversion method to simulate time-to-event (TTE) data from a log-logistic accelerated-failure time (AFT) model with statistical cure. The estimand of interest was the cumulative incidence function (CIF), which is the probability of experiencing the event of interest before time \\(t\\) (i.e., \\(\\text{Pr}[T \\leq t]\\), also known as the cumulative distribution function (CDF) in the absence of censoring).\nI came across two things while reading up on material for that post, that I’d mentally filed away for looking into in the future. The first was on simulating TTE data using numerical methods. The second was on estimating the CIF in the competing risk (CR) setting where it wasn’t correct to invert the survivor function (\\(S(t) = \\text{Pr}[T &gt; t] = 1 - \\text{Pr}[T \\le t]\\)) to get to the CIF.\nSo this post is on simulating data from the cause-specific hazards (CSH) approach to CR based on this paper (Beyersmann et al. 2009); other key papers I use material from are listed in the references below. To check whether my implementation (and understanding) is correct, I run a mini simulation to see if I can recover the true parameters used for generating the data. I also compare the estimated cause-specific CIFs with the true CIFs.\nI initially tried to use the survsim package to simulate CSH CR data, but I found it quite slow. Figuring out what I was doing suboptimally was only clear to me after digging deeply into it and reimplementing the code myself by studying the implementation of survsim::crisk.ncens.sim and the Beyersmann paper. My implementation has the advantage that it\nI mean I’d most likely still use survsim, unless I need to simulate a lot of not very small datasets, or want the additional flexibility around simulation-model specification.\nI think adjustedCurves::sim_confounded_crisk() can simulate CR data from the CSH approach as well, but I haven’t tried it out.\nI’m using the following packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(survival)\nlibrary(flexsurv)\n# functions called via namespace::fun() to avoid conflicts\n# library(glue)  # string interpolation\n# library(mirai)  # for setting daemons() for parallelization\n# library(survsim)\n# library(ggh4x)  # facet_grid2 function\n# library(bench)  # for benchmarking\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "posts/simulating-and-modelling-competing-risk-data/index.html#data-generating-process",
    "href": "posts/simulating-and-modelling-competing-risk-data/index.html#data-generating-process",
    "title": "Simulating data from the cause-specific hazard approach to competing risks",
    "section": "Data generating process",
    "text": "Data generating process\nIn the usual TTE setting, interest is on a single event. An example from marketing is that of customer churn, where a customer cancels their subscription. The competing risk approach to TTE analysis extends this to cases where more than one event can occur.\nSticking with the churn example, this could be (say) three possible event types (using the event indicator \\(D\\) with values \\(k = 1, 2, 3 (= K)\\)) — churn due to customer dissatisfaction with some aspect of the product / service which is the main event of interest that a company would like to reduce (\\(D = 1\\)), churn due to financial reasons (\\(D = 2\\)), and churn due to unavoidable reasons (\\(D = 3\\)). \\(D = 0\\) indicates not having churned (yet). I’m using an example just to make things a bit more concrete; these hazard functions are unlikely to be representative of any actual churn process.\nThis single vs multiple TTE analysis is analogous to the distinction between binary (binomial) vs categorical (multinomial) outcomes, with the former taking on values 0 / 1, and the latter taking on more than 2 values e.g., 0 / 1 / 2 / 3.\n\nDistributional parameters\nThe three churn event types are assumed to have the hazard functions corresponding to these distributions\n\\[\n\\begin{align*}\nT^1 | x_1 = 0 &\\sim \\text{Weibull}(\\text{shape} = 1.6, \\text{scale} = 5) \\\\\nT^2 | x_1 = 0 &\\sim \\text{Log-logistic}(\\text{shape} = 5, \\text{scale} = 5) \\\\\nT^3 | x_1 = 0 &\\sim \\text{Exponential}(\\text{rate} = 5)\n\\end{align*}\n\\]\nfor the control arm (\\(x_1 = 0\\)) assuming some experiment is carried out to assess the effectiveness of some action intended to reduce churn. Treatment arm can be coded as \\(x_1 = 1\\) with treatment assumed to reduce the rate of churn by a time acceleration factor of 1.4 or 40%. So we have\n\\[\n\\begin{align*}\nT^1 | x_1 = 0 &\\sim \\text{Weibull}(\\text{shape} = 1.6, \\text{scale} = 5) \\\\\nT^1 | x_1 = 1 &\\sim \\text{Weibull}(\\text{shape} = 1.6, \\text{scale} = 7)\n\\end{align*}\n\\]\nand \\(T^2 |x_1\\) and \\(T^3 | x_1\\) are the same for \\(x_1 = 0\\) and \\(x_1 = 1\\).\nChoosing \\(x_1\\) to have a non-zero effect only on \\(T^1\\) means I’m avoiding some of the messiness that comes with interpreting treatment effects on the CIFs.\nI’m also assuming a 50-50 (%) split of \\(x_1\\) into the two groups.\nThese values for the shape / scale / rate parameters were chosen to have decent balance across the different event types and to not have some categories with very low event counts.\n\n\nLatent failure time approach\nThe use of \\(T^{\\bullet}\\) makes it seem like I’m invoking the latent failure time approach to CR, but I’m only using it as a convenient way of indicating the chosen CSH functions. As pretty much all papers on CR warn, the latent variable approach — which retains \\(T_i = min(T^1_i, \\dots, T^K_i)\\) for each individual \\(i\\) — assumes unverifiable dependence structures between the latent event time variable \\(T^k\\) which are generally not identifiable from observed data.\nThis is sad because the latent variable approach seems a lot more intuitive and is very simple to generate data from when the latent event times are assumed to be independent\n\nn &lt;- 7\nset.seed(25)\ntibble(\n  # generate group membership\n  x1 = rbinom(n, 1, prob = 0.5),\n  # generate counterfactual for x1 = 0\n  t1_0 = rweibull(n, shape = 1.6, scale = 5),\n  # and counterfactual for x1 = 1 (using scale = 7)\n  t1_1 = rweibull(n, shape = 1.6, scale = exp(log(5) + log(1.4))),\n  # use the consistency equation to realize the latent t1\n  t1 =  (x1 * t1_1) + ((1 - x1) * t1_0),\n  t2 = rllogis(n, shape = 5, scale = 5),\n  # exponential distribution, same as rexp(n, 1 / 5)\n  t3 = rweibull(n, shape = 1, scale = 5),\n  # administrative censoring\n  cens_time = 4,\n  # take the minimum of the latent event and censoring times\n  time = pmin(t1, t2, t3, cens_time), \n  # record which event / censor time is the smallest\n  event = case_when(\n    cens_time &lt; t1 & cens_time &lt; t2 & cens_time &lt; t3 ~ 0,\n    t1 &lt; t2 & t2 &lt; t3 ~ 1,\n    t2 &lt; t1 & t2 &lt; t3 ~ 2,\n    t3 &lt; t1 & t3 &lt; t2 ~ 3\n  )\n) %&gt;% \n  mutate(across(.cols = everything(), .fns = ~ round(.x, 2)))\n\n# A tibble: 7 × 9\n     x1  t1_0  t1_1    t1    t2    t3 cens_time  time event\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0  5.26  3.72  5.26  4.81  3.15         4  3.15     3\n2     1  9.32 10.5  10.5   3.17  7.18         4  3.17     2\n3     0  5.79  5.18  5.79  3.56  0.97         4  0.97     3\n4     1  5.35  3.42  3.42  4.19  5.26         4  3.42     1\n5     0  5.04  5.8   5.04  4.22  7.2          4  4        0\n6     1  0.69  3.4   3.4   3    11.4          4  3        2\n7     1  3.36 14.5  14.5   5.39  0.92         4  0.92     3\n\n\nI’m quite sure that if I used this for my simulation I’d still end up with unbiased parameter estimates, but the assumption of independence may not be very realistic depending on the application.\nA future rabbit hole to go down would be to look into the literature on using copulas for modelling dependence between the latent event times.\nThe event time for each individual will be referred to using a single random variable \\(T\\) since no latent variables are assumed.\n\n\nCause-specific hazards (CSH)\nWe can plot the CSH functions defined as\n\\[\nh_k(t | x_1) = \\lim_{\\Delta t \\rightarrow 0} \\frac{\\text{Pr}[t \\le T &lt; t + \\Delta t, D = k | T \\ge t, x_1]}{\\Delta t}\n\\]\nfor the \\(k^\\text{th}\\) event showing the instantaneous rate of occurrence of that event\n\n\nCode\ntime_seq &lt;- seq(0.01, 50, 0.01)\nhazards &lt;- bind_rows(\n  tibble(\n    Distribution = \"Event 1: Weibull(shape = 1.6, scale = 5)\",\n    time = time_seq,\n    Hazard = hweibull(time_seq, shape = 1.6, scale = 5)\n  ),\n  tibble(\n    Distribution = \"Event 1: Weibull(shape = 1.6, scale = 7)\",\n    time = time_seq,\n    Hazard = hweibull(time_seq, shape = 1.6, scale = 7)\n  ),\n  tibble(\n    Distribution = \"Event 2: Log-logistic(shape = 5, scale = 5)\",\n    time = time_seq,\n    Hazard = hllogis(time_seq, shape = 5, scale = 5)\n  ),\n  tibble(\n    Distribution = \"Event 3: Exponential(rate = 5)\",\n    time = time_seq,\n    Hazard = hweibull(time_seq, shape = 1, scale = 5)\n  )\n)\n\nhazards %&gt;%\n  ggplot(aes(x = time, y = Hazard, group = Distribution, \n             linetype = Distribution)) +\n  geom_line() +\n  xlab(\"Time (t)\") +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  guides(linetype = guide_legend(nrow = 2))\n\n\n\n\n\n\n\n\n\nI’m using the hazard functions implemented within flexsurv (e.g., flexsurv::hweibull()). The exponential hazard is constant over time. The log-logistic hazard first rises and then falls with time. The Weibull hazards for the chosen parameters are monotonically increasing functions of time. The hazard for each event is independent of the hazard for all other events.\n\n\nRelative contribution to the overall hazard\nAs mentioned in the Hinchliffe and Lambert (2013) paper, it can be useful to look at the relative contribution of the hazard of each event to the overall hazard within each group at each \\(t\\)\n\\[\n\\text{Pr}[D = k | t \\le T &lt; t + \\Delta t, T \\ge t, x_1] = \\frac{h_k(t | x_1)}{\\sum_{k = 1}^K h_k(t | x_1)}\n\\]\nThis can be interpreted as the probability of an event of type \\(k\\) among those who experience an event at time \\(t\\) and haven’t experienced any of the events before \\(t\\).\n\n\nCode\n# prob(cause | event = 1)\ncause_specific_hazards_by_group &lt;- bind_rows(\n  hazards %&gt;%\n    # for x = 0, drop the cause 1 for x = 1\n    filter(Distribution != \"Event 1: Weibull(shape = 1.6, scale = 7)\") %&gt;%\n    mutate(Group = \"x1 = 0\"),\n  hazards %&gt;%\n    # for x = 1, drop the cause 1 for x = 0\n    filter(Distribution != \"Event 1: Weibull(shape = 1.6, scale = 5)\") %&gt;%\n    mutate(Group = \"x1 = 1\")\n)\n\nrelative_contribution_to_overall_hazard &lt;-cause_specific_hazards_by_group %&gt;%\n  arrange(Group, time, Distribution) %&gt;%\n  group_by(Group, time) %&gt;%\n  mutate(overall_hazard = sum(Hazard), p = Hazard / overall_hazard) %&gt;%\n  ungroup()\n\nrelative_contribution_to_overall_hazard %&gt;%\n  ggplot(aes(x = time, y = p, \n             group = interaction(Group, Distribution), \n             linetype = Distribution)) +\n  geom_line() +\n  xlab(\"Time (t)\") +\n  ylab(\"Pr [event type k | any event]\") +\n  facet_wrap(~ Group) +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  guides(linetype = guide_legend(nrow = 2))\n\n\n\n\n\n\n\n\n\nFor individuals that churn very soon after \\(t = 0\\), they’re more likely to have had \\(D = 3\\), followed by \\(D = 1\\), followed by \\(D = 2\\). Events that occur around \\(t = 5\\) are most likely to be \\(D = 2\\) as shown by the long-dash line peaking after that time.\n\n\nCause-specific cumulative incidence function (CIF)\nA key quantity is the cumulative incidence function (CIF) for each of the \\(K = 3\\) events, which is the probability that an individual will experience event \\(k\\) before time \\(t\\) in the absence of the occurrence of any of the other events\n\\[\n\\text{CIF}_k(t|x_1) = \\text{Pr}[T \\le t, D = k | x_1] = \\int_{u=0}^{u=t} h_k(u | x_1) S(u | x_1) du\n\\]\n\\(h_k(t|x_1)\\) is the CSH function, and \\(S(t|x_1)\\) is the overall survivor function in each group. The latter is defined as the probability of being free of any event up to time \\(t\\)\n\\[\nS(t | x_1) = \\text{Pr}[T &gt; t | x_1] =  \\text{exp}\\Bigg(- \\sum_{k = 1}^K H_k(t | x_1)\\Bigg)\n\\]\n\\(H_k(t | x_1)\\) is the cumulative hazard function for the \\(k^{\\text{th}}\\) event in each group\n\\[\nH_k(t | x_1) = \\int_{u = 0}^{u = t} h_k(u | x_1) du\n\\]\nThe cumulative hazard functions are used as implemented in flexsurv (e.g., flexsurv::Hweibull()). In discrete-time, the integral for the CIF is replaced by summation, and \\(S(u|\\cdot)\\) in the integrand / summand is replaced with \\(S(-u | \\cdot)\\), which is the survival probability at a time point right before \\(u\\). The CIFs and \\(S(t)\\) are probabilities of being in the different states by time \\(t\\)\n\n\nCode\n# calculate the overall survival from the overall hazard\noverall_survival &lt;- time_seq %&gt;% \n  expand_grid(\n    x1 = c(0, 1), time = .\n  ) %&gt;%\n  mutate(\n    surv = exp(-(\n      Hweibull(x = time, shape = 1.6, scale = exp(log(5) + log(1.4) * x1), \n               log = FALSE) +\n      Hllogis(x = time, shape = 5, scale = 5, log = FALSE) +\n      Hweibull(x = time, shape = 1, scale = 5, log = FALSE)\n    )),\n    Group = paste0(\"x1 = \", x1)\n  ) %&gt;%\n  select(-x1)\n\ntime_step &lt;- time_seq[2] - time_seq[1]\n\n# calculate CIFs\ncumulative_incidence_curves &lt;- cause_specific_hazards_by_group %&gt;%\n  inner_join(y = overall_survival, by = c(\"Group\", \"time\")) %&gt;%\n  group_by(Group, Distribution) %&gt;%\n  mutate(CIF = order_by(time, cumsum(Hazard * surv * time_step))) %&gt;%\n  ungroup()\n\n# plot the CIFs\n# P[T &lt;= t, cause = k | Event]\ncumulative_incidence_curves_plot_data &lt;- cumulative_incidence_curves %&gt;%\n  select(-Hazard) %&gt;%\n  pivot_wider(id_cols = c(time, Group, surv), \n              names_from = Distribution, values_from = CIF) %&gt;%\n  rename(`Event-free` = surv) %&gt;%\n  pivot_longer(cols = -c(time, Group), \n               names_to = \"Event\", values_to = \"CIF\") %&gt;%\n  filter(complete.cases(.))\n\ncumulative_incidence_curves_plot &lt;- cumulative_incidence_curves_plot_data %&gt;%\n  ggplot(aes(x = time, y = CIF, linetype = Event)) +\n  geom_line() +\n  xlab(\"Time (t)\") +\n  ylab(\"Probability of being in state\") +\n  facet_wrap(~ Group) +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  guides(linetype = guide_legend(nrow = 3))\n\ncumulative_incidence_curves_plot\n\n\n\n\n\n\n\n\n\nThe CIFs are called subdistribution functions because they never reach 1 (at \\(t = \\infty\\)) due to the presence of competing events so are not proper probability distributions. In the single event case, the CIF eventually reaches 1 given enough time.\nWithin each panel, the curves sum to 1 at every time point, since each individual is in one state at each time point\n\\[\n\\sum_{k = 1}^K\\text{CIF}_k(t | x_1) + S(t | x_1) = 1 = \\text{Pr}[T \\le t, D = 1 | x_1] + \\text{Pr}[T \\le t, D = 2 | x_1] + \\text{Pr}[T \\le t, D = 3 | x_1] + \\text{Pr}[T &gt; t | x_1]\n\\]\nAll individuals start with being event free at \\(t = 0\\), but as as time progresses, they move into any one of the \\(k\\) states, so the probability of being event-free, i.e., staying in state \\(D = 0\\), decreases with time and goes to 0 eventually (here around \\(t = 7\\)).\nOne difference between this plot and the unnormalized hazard plot is that here the hazard of each event contributes to the CIF of all other events via it’s dependence on the overall survival. So \\(\\text{CIF}_2(t | x1 = 0) \\neq \\text{CIF}_2(t | x1 = 1)\\) and \\(\\text{CIF}_3(t | x1 = 0) \\neq \\text{CIF}_3(t | x1 = 1)\\) because \\(H_1(t | x_1 = 0) \\ne H_1(t | x_1 = 1)\\), even though \\(H_2(t | x_1 = 0) = H_2(t | x_1 = 1)\\) and \\(H_3(t | x_1 = 0) = H_3(t | x_1 = 1)\\).\n\n\nRelative contribution to overall churn\nSimilar to the relative contribution of the CSHs to overall hazard I came across in the Hinchliffe and Lambert paper, they also mention the rescaled cause-specific CIFs (disregarding \\(S(t | x_1)\\))\n\\[\n\\text{Pr}[D = k | T \\le t, x_1] = \\frac{\\text{CIF}_k(t | x_1)}{\\sum_{k = 1}^K \\text{CIF}_k(t | x_1)}\n\\]\nwhich is the probability of having had event \\(k\\) among those who have churned by time \\(t\\) within levels of \\(x_1\\)\n\n\nCode\n# plot the rescaled CIFs conditional on event\n# so ignores the no even state\n# P[T &lt;= t, cause = k | Event]\ncumulative_incidence_curves %&gt;%\n  mutate(rescaled_CIF = CIF / sum(CIF), .by = c(Group, time)) %&gt;%\n  # arrange(Group, time) %&gt;%\n  # print()\n  ggplot(aes(x = time, y = rescaled_CIF, linetype = Distribution)) +\n  geom_line() +\n  xlab(\"Time (t)\") +\n  ylab(\"Pr [event type k by t | any event]\") +\n  facet_wrap(~ Group) +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  guides(linetype = guide_legend(nrow = 2))"
  },
  {
    "objectID": "posts/simulating-and-modelling-competing-risk-data/index.html#implement-the-simulation-steps",
    "href": "posts/simulating-and-modelling-competing-risk-data/index.html#implement-the-simulation-steps",
    "title": "Simulating data from the cause-specific hazard approach to competing risks",
    "section": "Implement the simulation steps",
    "text": "Implement the simulation steps\nThe key idea behind simulating CR data from the CSH approach is summarized in the following steps (here within each level of covariate \\(x_1\\)):\n\nspecify the cause-specific hazards \\(h_k(t | x_1)\\)\nsimulate survival time \\(T_i\\) for the \\(i^\\text{th}\\) individual with the all-cause cumulative hazard \\(\\sum_{k=1}^K H_k(t | x_1)\\)\nfor a simulated \\(T_i\\), simulate the event type \\(D_i\\) from a multinomial distribution with \\(k\\)-dimensional probability vector from the scaled hazards \\(h_k(t | x_1) / \\sum_{k = 1}^K h_k(t | x_1)\\). This gives the pair \\((T_i, D_i)\\) for the \\(i^\\text{th}\\) individual\nsimulate censoring time \\(C_i\\) and set \\(T_i  = C_i\\) if \\(C_i &lt; T_i\\) (and set \\(D_i = 0\\)).\n\nThe event time \\(T_i\\) is generated by inverting the survivor function. The CDF \\(F(t)\\) has a \\(\\text{Uniform}[0, 1]\\) distribution where \\(F(t_i) = u_i \\sim U[0, 1]\\). We can use \\(F(t) = 1 - S(t)\\) to equate \\(S(t) = 1 - u\\) which gives\n\\[\n1 - u = \\text{exp}\\Bigg[- \\sum_{k = 1}^K H_k(t) \\Bigg]\n\\]\nTaking logs on both sides and rearranging gives the objective function\n\\[\n\\text{log}(1 - u) + \\sum_{k = 1}^K H_k(t) = 0\n\\]\nFor a randomly drawn \\(u_i \\sim U[0,1]\\) (and given \\(x_{i, 1}\\)), numerical root finding methods can be employed to estimate the value of \\(t_i\\) which satisfies this equation. This process of using \\(u_i\\) to estimate \\(t_i\\) is known as inverse transform sampling.\n\nOverall hazard\nThis function implements the sum of the cumulative hazards \\(H_k(t | x_1)\\) which is 0 at \\(t = 0\\) but increases without bound beyond that\n\nall_cause_cumulative_hazards &lt;- function(t, x1) {\n  sum(\n    Hweibull(x = t, shape = 1.6, scale = exp(log(5) + log(1.4) * x1), log = FALSE),\n    Hllogis(x = t, shape = 5, scale = 5, log = FALSE),\n    Hweibull(x = t, shape = 1, scale = 5, log = FALSE)\n  )\n}\n\nall_cause_cumulative_hazards(t = 0, x1 = 0)\n\n[1] 0\n\nall_cause_cumulative_hazards(t = 10, x1 = 0)\n\n[1] 8.527941\n\nall_cause_cumulative_hazards(t = 10, x1 = 1)\n\n[1] 7.265977\n\n\n\n\nEvent type probabilities\nThis next one calculates the event probabilities (conditional on an event at \\(t\\)) \\(\\text{Pr}[D = k | t \\le T &lt; t + \\Delta t, T \\ge t, x_1]\\) given \\(t\\) and \\(x_1\\)\n\ncause_specific_probabilities &lt;- function(t, x1) {\n  hazards &lt;- c(\n    hweibull(x = t, shape = 1.6, \n             scale = exp(log(5) + log(1.4) * x1), log = FALSE),\n    hllogis(x = t, shape = 5, scale = 5, log = FALSE),\n    hweibull(x = t, shape = 1, scale = 5, log = FALSE)\n  )\n\n  hazards / sum(hazards)\n}\n\ncause_specific_probabilities(t = 0, x1 = 0)\n\n[1] 0 0 1\n\ncause_specific_probabilities(t = 10, x1 = 0)\n\n[1] 0.4145983 0.4144437 0.1709580\n\ncause_specific_probabilities(t = 10, x1 = 1)\n\n[1] 0.2924853 0.5008953 0.2066193\n\n\nThe function returns a \\(K\\)-dimensional vector which sums to 1.\n\n\nObjective function\nImplement the objective function\n\nobj_fun &lt;- function(t, u, x1) {\n  log(1 - u) + all_cause_cumulative_hazards(t, x1)\n}\n\nobj_fun(t = 3.771, u = 0.8, x1 = 0)\n\n[1] -0.0001226652\n\ntibble(\n  time = time_seq[time_seq &lt; 10.0], \n  obj_val = map_dbl(\n    .x = time, \n    .f = ~ obj_fun(.x, u = 0.8, x1 = 0)\n    )\n  ) %&gt;% \n  ggplot(aes(x = time, y = obj_val)) + \n  geom_line() + \n  geom_hline(yintercept = 0, linetype = \"dotted\") + \n  geom_vline(xintercept = 3.771, linetype = \"dashed\") + \n  xlab(\"Time (t)\") + \n  ylab(\"Objective function\")\n\n\n\n\n\n\n\n\nThis plot corresponding to a grid search on \\(t \\in (0, 10)\\) shows that for \\(u_i = 0.8\\), \\(t_i \\approx 3.771\\) (dashed vertical line) is very close to the root since the objective function evaluates to approximately -0.00012.\nDoing grid search is kinda tedious, but we can use stats::uniroot to find the root \\(t_i\\) for a given \\(u_i\\) (and \\(x_{i, 1}\\)) and return the pair \\((T_i, D_i)\\).\n\n\nSimulate single outcome\n\nsimulate_single_outcome &lt;- function(u, x1, admin_censor_time = 5, lower_bound_time = 1e-10) {\n\n  # check whether simulated time would lie outside the time interval we want to solve for\n  # in this case the function would have the same sign at the interval endpoints\n  # if the function has different signs at the endpoints then a root is within the interval\n  same_sign &lt;- (obj_fun(u = u, t = 0.00001, x1 = x1) * obj_fun(u = u, t = admin_censor_time, x1 = x1)) &gt; 0\n\n  if (same_sign) {\n    time &lt;- admin_censor_time\n    cause &lt;- 0\n    iter &lt;- NA_real_\n    fn_value_at_root &lt;- NA_real_\n  } else {\n    # for a handful of cases (out of 100,000s) we can end up with t = 0 from a distribution that doesn't support it\n    # https://stats.stackexchange.com/questions/176376/invalid-survival-times-for-this-distribution\n    # so we can set a lower bound on time &gt; 0\n    uniroot_obj &lt;- uniroot(\n      f = obj_fun, \n      interval = c(lower_bound_time, admin_censor_time), \n      # pass arguments to the objective function\n      u = u, x1 = x1\n    )\n    time &lt;- uniroot_obj$root\n    cause &lt;- which.max(\n      rmultinom(1, 1, cause_specific_probabilities(t = time, x1 = x1))\n    )\n    iter &lt;- uniroot_obj$iter\n    fn_value_at_root &lt;- uniroot_obj$f.root\n  }\n\n  tibble(u, x1, admin_censor_time, iter, fn_value_at_root, time, cause)\n}\n\nbind_rows(\n  # non-censored\n  simulate_single_outcome(u = 0.2, x1 = 1, admin_censor_time = 5),\n  # censored\n  simulate_single_outcome(u = 0.95, x1 = 1, admin_censor_time = 5),\n  # same u but larger admin censored time means not censored\n  simulate_single_outcome(u = 0.95, x1 = 1, admin_censor_time = 12)\n)\n\n# A tibble: 3 × 7\n      u    x1 admin_censor_time  iter fn_value_at_root  time cause\n  &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.2      1                 5     6     -0.00000118  0.920     3\n2  0.95     1                 5    NA     NA           5         0\n3  0.95     1                12     7     -0.000000103 5.76      2\n\n\nA lower bound of \\(t\\) close to 0 is applied to avoid ending up with estimates of exactly 0, which can cause problems with estimation for some parametric distributions. If the drawn \\(u_i\\) is large enough that the objective function has the same sign at both endpoints of the interval, then we skip the root finding as that observation would be treated as censored anyway.\nAdministrative censoring at \\(t = 5\\) is assumed, which should lead to about 7% censored individuals in \\(x_1 = 0\\) and 10% in \\(x_1 = 1\\) (and about 8-9% overall) as seen in this zoomed-in version of the CIF plot (from a few sections above)\n\n\nCode\ncumulative_incidence_curves_plot + \n  geom_vline(xintercept = 5.0, color = \"forestgreen\") +\n  geom_hline(\n    data = tibble(Group = c(\"x1 = 0\", \"x1 = 1\"), y = c(0.068, 0.102)),\n    aes(yintercept = y), color = \"purple\"\n  ) +\n  coord_cartesian(xlim = c(4.5, 6), ylim = c(0.05, 0.12))\n\n\n\n\n\n\n\n\n\n\n\nSimulate single dataset\nFinally, the next function stitches together the steps for simulating a full dataset with \\(n = 1,000\\) individuals and their group indicator \\(x_{i, 1} \\in \\{0, 1\\}\\), event time \\(T_i\\), and event indicator \\(D_i \\in \\{0, 1, 2, 3\\}\\).\n\nsimulate_single_dataset &lt;- function(n_rows = 1000, prop_x1 = 0.5, seed = 12345) {\n  if(!is.null(seed)) set.seed(seed)\n  x1 &lt;- rbinom(n = n_rows, size = 1, prob = prop_x1)\n  u &lt;- runif(n_rows)\n  1:n_rows %&gt;% \n    map(.f = ~ simulate_single_outcome(u = u[.x], x1 = x1[.x])) %&gt;%\n    list_rbind()\n}\n\nsimulate_single_dataset(n_rows = 5)\n\n# A tibble: 5 × 7\n      u    x1 admin_censor_time  iter fn_value_at_root  time cause\n  &lt;dbl&gt; &lt;int&gt;             &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.166     1                 5     6     -0.00000122  0.765     3\n2 0.325     1                 5     5     -0.000000279 1.52      1\n3 0.509     1                 5     6     -0.00000489  2.47      3\n4 0.728     1                 5     6     -0.000000505 3.70      2\n5 0.990     0                 5    NA     NA           5         0"
  },
  {
    "objectID": "posts/simulating-and-modelling-competing-risk-data/index.html#test-the-implementation",
    "href": "posts/simulating-and-modelling-competing-risk-data/index.html#test-the-implementation",
    "title": "Simulating data from the cause-specific hazard approach to competing risks",
    "section": "Test the implementation",
    "text": "Test the implementation\n\nParameter estimates\nTo check that the simulation function works, a bunch of functions are defined in the next code chunk that take a single simulated dataset, convert it into cause-specific datasets (for each event type \\(k\\), update the event indicator by censoring competing events), fit a (correctly specified) parametric model separately to each cause-specific dataset, extract the coefficients from each model, and return them as a data frame.\n\ncreate_cause_specific_datasets &lt;- function(data) {\n  map(\n    .x = 1:3,\n    .f = ~ {\n      data %&gt;%\n        select(x1, time, cause) %&gt;%\n        mutate(event = as.numeric(cause == .x))\n    }\n  )\n}\n\nfit_cause_specific_models &lt;- function(cause_specific_datasets, distributions = c(\"weibull\", \"llogis\", \"weibull\")) {\n  map2(\n    .x = cause_specific_datasets,\n    .y = distributions,\n    .f = ~ flexsurvreg(Surv(time = time, event = event) ~ x1, data = .x, dist = .y)\n  )\n}\n\nextract_coefs &lt;- function(list_models) {\n  map2_dfr(.x = list_models, .y = 1:length(list_models), .f = ~ {\n    .x %&gt;%\n      # calls flexsurv:::tidy.flexsurvreg()\n      flexsurv::tidy() %&gt;%\n      select(term, estimate, se = std.error) %&gt;%\n      mutate(cause = .y, .before = term)\n  })\n}\n\nrun_pipeline &lt;- function(seed) {\n  seed %&gt;%\n    simulate_single_dataset(seed = .) %&gt;%\n    create_cause_specific_datasets() %&gt;%\n    fit_cause_specific_models() %&gt;%\n    extract_coefs() %&gt;%\n    mutate(rep = seed, .before = cause)\n}\n\n# wrap it up into purrr::safely() since it fails \n# for some seeds (e.g. 125)\nsafely_run_pipeline &lt;- safely(run_pipeline)\n\nsafely_run_pipeline(seed = 3)\n\n$result\n# A tibble: 9 × 5\n    rep cause term  estimate     se\n  &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1     3     1 shape   1.66   0.0801\n2     3     1 scale   4.93   0.246 \n3     3     1 x1      0.297  0.0724\n4     3     2 shape   5.50   0.359 \n5     3     2 scale   4.88   0.153 \n6     3     2 x1      0.0281 0.0391\n7     3     3 shape   1.07   0.0418\n8     3     3 scale   4.99   0.333 \n9     3     3 x1     -0.0972 0.0851\n\n$error\nNULL\n\n\nThe safely_run_pipeline() function returns the point estimates and standard errors for the shape, scale, and coefficient for \\(x_1\\) from each cause-specific model.\nThis process is repeated 5,000 times. A single run takes about 1.6 seconds on my machine, so to save time, the runs are parallelized. Using the recently added purrr::in_parallel function cuts down the total execution time from a little over 2 hours to about 20 minutes (it still took 3x longer than I expected, so I’d like to look into that later — overhead is expected from parallelization but not 3x. Maybe it would have been better to use fewer cores.).\nUsing the version of purrr at the time of writing, in_parallel requires that all these functions defined above are nested into one big function which can then be shipped off to the executors. All the package functions should be explicitly namespaced via package::fun() as well. The full script for running this pipeline is here.\nSaved results can be read back in. About 44 or 0.9% of the simulations (with the following seeds) failed at some step.\n\n\nCode\nn_reps &lt;- 5000\nfailed_parallel_runs &lt;- read_rds(\"saved_objects/failed_parallel_runs.rds\")\nfailed_parallel_runs\n\n\n [1]    5   39  124  125  315  319  445  510  662  672  765 1058 1105 1238 1494\n[16] 1673 1832 1844 2152 2176 2277 2399 2521 2526 2888 2944 2995 3065 3574 3657\n[31] 3754 3818 4222 4234 4246 4259 4326 4414 4465 4481 4482 4742 4873 4978\n\n\nCode\nlength(failed_parallel_runs) / n_reps\n\n\n[1] 0.0088\n\n\nFor the seeds that ran without error, the estimates can be plotted with the true parameter values overlaid for comparison. No transformation is needed when using flexsurv:::tidy.flexsurvreg() (except for the \\(x_1\\) coefficient which is reported on the log scale)\n\n\nCode\ntrue_params &lt;- tribble(\n  ~ cause, ~ term, ~ estimate,\n  1L, \"shape\", 1.6,\n  1L, \"scale\", 5,\n  1L, \"x1\", log(1.4),\n  2L, \"shape\", 5,\n  2L, \"scale\", 5,\n  2L, \"x1\", 0,\n  3L, \"shape\", 1,\n  3L, \"scale\", 5,\n  3L, \"x1\", 0\n)\n\nparameter_estimates_parallel &lt;- read_rds(\"saved_objects/parameter_estimates_parallel.rds\")\n\nparameter_estimates_parallel %&gt;% \n  ggplot(aes(x = estimate, group = interaction(cause, term))) +\n  geom_density() +\n  geom_vline(data = true_params, aes(xintercept = estimate)) +\n  xlab(glue::glue(\"Sampling distribution of simulation parameters \", \n                  \"(5,000 datasets with n = 1,000)\")) +\n  ggh4x::facet_grid2(rows = vars(cause), cols = vars(term), \n                     scales = \"free\", independent = TRUE) +\n  theme_classic() + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nIt’s reassuring but not surprising, given the ubiquity of this approach to CSH CR analysis, to see the density estimates centered at the true values indicating unbiasedness of the CSH approach.\n\n\nCIF estimates\nSince the parameters are estimated correctly, any downstream quantities like the cause-specific CIFs and overall survival should be unbiased too. In the next figure, I’m visualizing these for the four states within \\(x_1 = 0\\). The true curves and the average of 100 curves from randomly simulated datasets (restricted to \\(t \\in (0, 10]\\)) are pretty close\n\n\nCode\nsampling_distribution_CIFs &lt;- read_rds(\"saved_objects/sampling_distribution_CIFs.rds\")\n\nsampling_distribution_CIFs_mean &lt;- sampling_distribution_CIFs %&gt;%\n  summarise(CIF = mean(CIF), .by = c(Event, time))\n\nsampling_distribution_CIFs %&gt;%\n  ggplot(aes(x = time, y = CIF, group = rep)) +\n  geom_line(color = \"gray80\") +\n  geom_line(\n    data = cumulative_incidence_curves_plot_data %&gt;%\n    filter(Group == \"x1 = 0\", time &lt;= 10.0) %&gt;% \n    mutate(Event = fct_relevel(factor(Event), \"Event-free\", after = 0)),\n    aes(x = time, y = CIF),\n    color = \"gray20\", linewidth = 1.5,\n    inherit.aes = FALSE\n  ) +\n  geom_line(\n    data = sampling_distribution_CIFs_mean,\n    aes(x = time, y = CIF),\n    color = \"gray20\", linewidth = 1.5, linetype = \"dotted\",\n    inherit.aes = FALSE\n  ) +\n  geom_vline(xintercept = 5, linetype = \"dashed\", color = \"gray20\") +\n  xlab(glue::glue(\"Time (t)\\nSampling distribution of curves \", \n                  \"estimated from 100 datasets with \", \n                  \"n = 1,000 (light gray); \\ntrue curve \", \n                  \"(gray, solid); averaged curve (gray, dotted);\",\n                  \"\\nmaximum observed time (gray, dashed)\")) +\n  ylab(\"Probability of being in state\") +\n  facet_wrap(~ Event, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\nIntuitively, the variability (or lack of) boils down to the number of events of each type \\(k\\). Events that are more likely to occur first (in this case \\(k=3\\)) will have more events and more precise estimates. The estimate for \\(S(t|x_1 = 0)\\) is the most precise here (all the gray lines are very close to the black line) because it uses information on all the events.\nEach individual curve is produced using this function\n\nestimate_CIF &lt;- function(seed, time_seq, covariate_level = data.frame(x1 = 0)) {\n  list_models &lt;- seed %&gt;%\n    simulate_single_dataset(seed = .) %&gt;%\n    create_cause_specific_datasets() %&gt;%\n    fit_cause_specific_models()\n\n  transition_matrix &lt;- matrix(data = c(c(NA, 1, 2, 3), rep(NA, 12)), \n                              nrow = 4, ncol = 4, byrow = TRUE)\n\n  multi_state_object &lt;- fmsm(list_models[[1]], \n                             list_models[[2]], \n                             list_models[[3]], \n                             trans = transition_matrix)\n\n  multi_state_object %&gt;% \n    pmatrix.fs(\n      trans = transition_matrix, t = time_seq, \n      newdata = covariate_level, tidy = TRUE\n    ) %&gt;%\n    as_tibble() %&gt;%\n    filter(start == 1) %&gt;%\n    select(-start) %&gt;%\n    pivot_longer(cols = -time, names_to = \"Distribution\", values_to = \"CIF\")\n}\n\nsafely_estimate_CIF &lt;- safely(estimate_CIF)\n\nand the next code runs the pipeline for producing these 100 CIF estimates\n\n\nCode\ntime_seq_subset &lt;- time_seq %&gt;% \n  keep(.p = ~ .x &lt;= 10.0)\n\nsampling_distribution_CIFs &lt;- 1:100 %&gt;%\n  map(\n    .f = ~ {\n      print(glue::glue(\"rep: {.x}\"))\n      safely_estimate_CIF(\n        seed = .x, \n        time_seq = time_seq_subset, \n        covariate_level = data.frame(x1 = 0)\n      )\n    }\n  ) %&gt;%\n  # remove the seeds where the estimation failed\n  imap(\n    .f = ~ {\n      res &lt;- pluck(.x, \"result\")\n      if (!is.null(res)) {\n        res %&gt;%\n          mutate(rep = .y, .before = time)\n      } else {\n        res\n      }\n    }\n  ) %&gt;%\n  compact() %&gt;%\n  list_rbind() %&gt;%\n  mutate(\n    Event = case_when(\n      Distribution == \"V1\" ~ \"Event-free\",\n      Distribution == \"V2\" ~ \"Weibull(shape = 1.6, scale = 5)\",\n      Distribution == \"V3\" ~ \"Log-logistic(shape = 5, scale = 5)\",\n      Distribution == \"V4\" ~ \"Exponential(rate = 5)\"\n    )\n  )\n\n# write_rds(sampling_distribution_CIFs, \"saved_objects/sampling_distribution_CIFs.rds\")"
  },
  {
    "objectID": "posts/simulating-and-modelling-competing-risk-data/index.html#comparison-with-survsimcrisk.ncens.sim",
    "href": "posts/simulating-and-modelling-competing-risk-data/index.html#comparison-with-survsimcrisk.ncens.sim",
    "title": "Simulating data from the cause-specific hazard approach to competing risks",
    "section": "Comparison with survsim::crisk.ncens.sim",
    "text": "Comparison with survsim::crisk.ncens.sim\nI initially started on this project by attempting to use the function from survsim, but I was getting very different results from fitting the models using flexsurv. It was also taking quite long (~6x longer) to produce simulated datasets mostly because of me picking simulation parameters without giving it much thought1. Thinking through the process in detail is what led to this post being much longer than anticipated. This section compares the parameterizations for the hazard functions and simulated draws from survsim and flexsurv.\n1 I picked a large value for administrative censoring, which meant that uniroot would take much longer to run due to increased number of iterations to converge to the root, and increased time due to multiple integrate() calls.\nConverting flexsurv parameterization to survsim\nThe hazard function code for Weibull and log-logistic is taken from survsim:::crisk.ncens.sim() here\n\n# code from survsim::crisk.ncens.sim()\n#\nif (dist.ev[k] == \"llogistic\") {\n  a.ev[k] &lt;- 1/exp(beta0.ev[k] + suma[k])\n  b.ev[k] &lt;- anc.ev[k]\n  cshaz[[k]] &lt;- function(t, r) {\n    par1 &lt;- eval(parse(text=\"a.ev[r]\"))\n    par2 &lt;- eval(parse(text=\"b.ev[r]\"))\n    z    &lt;- eval(parse(text=\"az1[r]\"))\n    return(z*(par1*par2*(t^(par2-1)))/(1+par1*(t^par2)))}\n}\n\nif (dist.ev[k] == \"weibull\") {\n  a.ev[k] &lt;- beta0.ev[k] + suma[k]\n  b.ev[k] &lt;- anc.ev[k]\n  cshaz[[k]] &lt;- function(t, r) {\n    par1 &lt;- eval(parse(text=\"a.ev[r]\"))\n    par2 &lt;- eval(parse(text=\"b.ev[r]\"))\n    z    &lt;- eval(parse(text=\"az1[r]\"))\n    return(z * ((1/par2)/((exp(par1))^(1/par2)))*\n           t^((1/par2) - 1))}\n}\n\nIt’s a bit confusing since the user function survsim::crisk.sim() takes anc.ev and beta0.ev as inputs, but these get transformed to a.ev and b.ev and then to par1 and par2 (ok that last transformation is straightforward).\nHere’s the Weibull AFT programmed in flexsurv\n\n# Weibull AFT parameterization hazard\nflexsurv::hweibull\n\nfunction (x, shape, scale = 1, log = FALSE)\n{\n    h &lt;- dbase(\"weibull\", log=log, x=x, shape=shape, scale=scale)\n    for (i in seq_along(h)) assign(names(h)[i], h[[i]])\n    if (log)\n        ret[ind] &lt;- log(shape) + (shape-1)*log(x/scale) - log(scale)\n    else\n        ret[ind] &lt;- shape * (x/scale)^(shape - 1)/scale\n    ret\n}\n&lt;bytecode: 0x62fd9b6e0820&gt;\n&lt;environment: namespace:flexsurv&gt;\n\n\nIf the shape parameter is \\(a\\) and scale parameter is \\(\\mu\\), the hazard function is\n\\[\nh_{\\text{wei}}^{\\text{flexsurv}}(t; a, \\mu) = a\\Bigg(\\frac{t}{\\mu}\\Bigg)^{a - 1} \\frac{1}{\\mu} = a \\mu^{-a}t^{a - 1}\n\\]\nand for Weibull AFT from survsim\n\\[\nh_{\\text{wei}}^{\\text{simsurv}}(t; p_1, p_2) = \\frac{1}{p_2} \\Bigg(\\Big(e^{p_1}\\Big)^{1 / p_2}\\Bigg)^{-1} t^{\\Big(\\frac{1}{p_2} - 1\\Big)}\n\\]\nwith beta0.ev = par1 = \\(p_1\\) and anc.ev = par2 = \\(p_2\\). This gives the corresponding transformations for Weibull in simsurv, anc.ev = \\(1 / a\\) (so inverse of flexsurv shape), and beta0.ev = \\(\\text{log}(\\mu)\\) (so log of flexsurv scale).\nLog-logistic implemented in flexsurv\n\n# log-logistic hazard\nflexsurv::hllogis\n\nfunction(x, shape=1, scale=1, log = FALSE) \n{\n    h &lt;- dbase(\"llogis\", log=log, x=x, shape=shape, scale=scale)\n    for (i in seq_along(h)) assign(names(h)[i], h[[i]])\n    if (log) ret[ind] &lt;- log(shape) - log(scale) + (shape-1)*(log(x) - log(scale)) - log(1 + (x/scale)^shape)\n    else ret[ind] &lt;- (shape/scale) * (x/scale)^{shape-1} / (1 + (x/scale)^shape)\n    ret\n}\n&lt;bytecode: 0x62fd99288f80&gt;\n&lt;environment: namespace:flexsurv&gt;\n\n\nif shape is \\(a\\) and scale is \\(b\\), the hazard function is\n\\[\nh_{\\text{ll}}^{\\text{flexsurv}}(t; a, b) = \\frac{(a / b) (t / b)^{a - 1}}{1 + (t / b)^a}\n\\]\ncompared with the one from simsurv (with par1 = \\(p_1\\) and par2 = \\(p_2\\))\n\\[\nh_{\\text{ll}}^{\\text{simsurv}}(t; p_1, p_2) = \\frac{p_1 p_2 t^{p_2 - 1}}{1 + p_1 t^{p_2}}\n\\]\n\\(p_2 = \\text{anc.ev} = a\\) (shape from flexsurv) and \\(p_1 = 1 / \\text{exp}(\\text{beta0.ev}) = a \\text{log}(b)\\) (shape times log(scale) from flexsurv).\n\n\nsurvsim function\n\nsurvsim_sim_fn &lt;- function(n = 1000) {\n  survsim::crisk.sim(\n    n = n,\n    foltime = 5,\n    # TTE distr\n    nsit = 3,\n    dist.ev = c(\"weibull\", \"llogistic\", \"weibull\"),\n    # anc.ev is 1 / shape, exp(beta0) is scale in flexsurv weibull AFT\n    # anc.ev is shape, beta0.ev is shape * log(scale) in flexsurv llogis\n    anc.ev = c(1 / 1.6, 5, 1),\n    beta0.ev = c(log(5), 5 * log(5), log(5)),\n    # covariate process\n    x = list(c(\"bern\", 0.5)),\n    beta = list(c(1.4), c(0), c(0)),\n    # censoring process, assume constant here to only apply administrative censoring\n    dist.cens = \"unif\", beta0.cens = 500, anc.cens = 500\n  ) %&gt;%\n    as_tibble()\n}\n\nsurvsim_sim_fn(n = 10)\n\n# A tibble: 10 × 8\n     nid cause   time status start stop      z     x\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1    NA 5           0 NA    NA        1     0\n 2     2     1 3.79        1 NA    NA        1     0\n 3     3     3 0.321       1 NA    NA        1     0\n 4     4     3 1.02        1 NA    NA        1     1\n 5     5     1 2.38        1 NA    NA        1     1\n 6     6     3 2.45        1 NA    NA        1     1\n 7     7     3 0.0557      1 NA    NA        1     0\n 8     8     3 0.0681      1 NA    NA        1     0\n 9     9     3 0.0741      1 NA    NA        1     1\n10    10     1 2.34        1 NA    NA        1     0\n\n\nThe \\(z\\) column is for individual frailties.\n\n\nCompare run times\nThe next figure shows a dot plot of the execution time distribution by repeating the process 10 times for both functions (again previously run results read back in)\n\n\nCode\n# survsim_bench &lt;- bench::mark(\n#   survsim_sim_fn(), iterations = 10, \n#   check = FALSE, time_unit = \"s\", memory = FALSE\n# )\n# write_rds(survsim_bench, file = \"saved_objects/survsim_bench.rds\")\n\nsurvsim_bench &lt;- read_rds(\"saved_objects/survsim_bench.rds\")\n\n# my_implementation_bench &lt;- bench::mark(\n#   run_pipeline(sample(1:100, size = 1, replace = TRUE)),\n#   iterations = 10, check = FALSE, time_unit = \"s\", memory = FALSE\n# )\n# write_rds(my_implementation_bench, file = \"saved_objects/my_implementation_bench.rds\")\n\nmy_implementation_bench &lt;- read_rds(\"saved_objects/my_implementation_bench.rds\")\n\n# dot plot of the run times\ntibble(\n  time = c(survsim_bench %&gt;% pluck(\"time\", 1), \n           my_implementation_bench %&gt;% pluck(\"time\", 1)),\n  group = rep(c(\"survsim\", \"handrolled\"), each = 10)\n) %&gt;%\n  ggplot(aes(x = time, y = group, fill = group, color = group)) +\n  geom_dotplot(binwidth = 0.05,  binaxis = \"x\", \n               method = \"histodot\", binpositions = \"bygroup\", \n               stackdir = \"centerwhole\") +\n  #geom_point(size = 3, position = position_jitter(height = 0.1, seed = 4)) +\n  #theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  theme(legend.position = \"none\") +\n  scale_x_continuous(breaks = seq(1, 4, 1), \n                     labels = seq(1, 4, 1), \n                     limits = c(1, 4)) +\n  xlab(\"Time (in seconds)\") +\n  ylab(\"\") + \n  scale_color_manual(values = c(\"orange\", \"gray30\")) + \n  scale_fill_manual(values = c(\"orange\", \"gray30\"))\n\n\n\n\n\n\n\n\n\nshows about 2x speedup; I’m assuming most of this comes from not having to use numerical integration but using the cumulative hazards programmed within flexsurv.\n\n\nCompare samples\nThe next plot shows the empirical distribution functions for 100 samples of size 100 from the two implementations\n\n\nCode\n# eCDFs for a large draw overlaid to compare the draws - they should be overlapping\n# set.seed(23)\n# survsim_sample &lt;- survsim_sim_fn(n = 10000)\n# set.seed(23)\n# handrolled_sample &lt;- simulate_single_dataset(n_rows = 10000, seed = NULL)\n#\n# write_rds(survsim_sample, file = \"saved_objects/survsim_sample.rds\")\n# write_rds(handrolled_sample, file = \"saved_objects/handrolled_sample.rds\")\n\nsurvsim_sample &lt;- read_rds(\"saved_objects/survsim_sample.rds\")\nhandrolled_sample &lt;- read_rds(\"saved_objects/handrolled_sample.rds\")\n\nbind_rows(\n  survsim_sample %&gt;%\n    select(time) %&gt;%\n    mutate(group = \"survsim\"),\n  handrolled_sample %&gt;%\n    select(time) %&gt;%\n    mutate(group = \"handrolled\")\n) %&gt;%\n  mutate(rep = sample(1:100, size = 20000, replace = TRUE)) %&gt;%\n  ggplot(aes(x = time, group = interaction(group, rep), color = group)) +\n  stat_ecdf(pad = FALSE, alpha = 0.8) +\n  xlab(\"Simulated Times\") +\n  ylab(\"Empirical CDF\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  scale_color_manual(values = c(\"orange\", \"gray30\"))\n\n\n\n\n\n\n\n\n\nIt seems that the gray curves (survsim) are a bit higher than the ones in orange (my implementation). I haven’t looked into why, but it seems likely to be due to sampling variation."
  },
  {
    "objectID": "posts/simulating-and-modelling-competing-risk-data/index.html#references",
    "href": "posts/simulating-and-modelling-competing-risk-data/index.html#references",
    "title": "Simulating data from the cause-specific hazard approach to competing risks",
    "section": "References",
    "text": "References\nThe main refs for the material in this post\n\nBeyersmann, J., Latouche, A., Buchholz, A., & Schumacher, M. (2009). Simulating competing risks data in survival analysis. Statistics in medicine, 28(6), 956-971.\nAllignol, A., Schumacher, M., Wanner, C. et al. Understanding competing risks: a simulation point of view. BMC Med Res Methodol 11, 86 (2011). https://doi.org/10.1186/1471-2288-11-86 - what I found cool is that this paper uses the non-parametric estimate of the baseline hazard function to simulate proportional hazard data from the cox model\nCrowther, M. J., & Lambert, P. C. (2012). Simulating Complex Survival Data. The Stata Journal, 12(4), 674-687. https://doi.org/10.1177/1536867X1201200407 (Original work published 2012)\nPutter, H., Fiocco, M. and Geskus, R.B. (2007), Tutorial in biostatistics: competing risks and multi-state models. Statist. Med., 26: 2389-2430. https://doi.org/10.1002/sim.2712 - key paper on competing risk analyses\nHinchliffe, S.R., Lambert, P.C. Flexible parametric modelling of cause-specific hazards to estimate cumulative incidence functions. BMC Med Res Methodol 13, 13 (2013). https://doi.org/10.1186/1471-2288-13-13\nflexsurv package vignette on multi-state modelling\nflexsurv package introduction vignette\nflexsurv vignette on distributions\nsurvsim package docs\n\nSome other papers / package vignettes I came across and read (parts of if not entirely)\n\nsurvival package multi-state vignette\nsurvival package vignette reproducing Putter et al analysis\nAndersen PK, Geskus RB, de Witte T, Putter H. Competing risks in epidemiology: possibilities and pitfalls. Int J Epidemiol. 2012 Jun;41(3):861-70. doi: 10.1093/ije/dyr213. Epub 2012 Jan 9. PMID: 22253319; PMCID: PMC3396320.\nAustin PC, Lee DS, Fine JP. Introduction to the Analysis of Survival Data in the Presence of Competing Risks. Circulation. 2016 Feb 9;133(6):601-9. doi: 10.1161/CIRCULATIONAHA.115.017719. PMID: 26858290; PMCID: PMC4741409.\nFine, J. P., & Gray, R. J. (1999). A Proportional Hazards Model for the Subdistribution of a Competing Risk. Journal of the American Statistical Association, 94(446), 496–509. https://doi.org/10.2307/2670170\nLatouche A, Allignol A, Beyersmann J, Labopin M, Fine JP. A competing risks analysis should report results on all cause-specific hazards and cumulative incidence functions. J Clin Epidemiol. 2013 Jun;66(6):648-53. doi: 10.1016/j.jclinepi.2012.09.017. Epub 2013 Feb 14. PMID: 23415868.\nScheike, T. H., & Zhang, M.-J. (2011). Analyzing Competing Risk Data Using the R timereg Package. Journal of Statistical Software, 38(2), 1–15. https://doi.org/10.18637/jss.v038.i02\nThomas H. Scheike, Mei-Jie Zhang, Thomas A. Gerds, Predicting cumulative incidence probability by direct binomial regression, Biometrika, Volume 95, Issue 1, March 2008, Pages 205–220, https://doi.org/10.1093/biomet/asm096\nEdouard F Bonneville, Liesbeth C de Wreede, Hein Putter, Why you should avoid using multiple Fine–Gray models: insights from (attempts at) simulating proportional subdistribution hazards data, Journal of the Royal Statistical Society Series A: Statistics in Society, Volume 187, Issue 3, August 2024, Pages 580–593, https://doi.org/10.1093/jrsssa/qnae056"
  },
  {
    "objectID": "posts/spelling-bee/index.html",
    "href": "posts/spelling-bee/index.html",
    "title": "Setup and test Python by programming a spelling bee solver",
    "section": "",
    "text": "All the previous posts on this blog have been in R. But sometimes I want to program in another language – Python, Scala, etc. So in the process of setting up and testing Python with Quarto, I decided to write this test post where I implement a set of simple functions to solve the word blossom / spelling bee1 game given a set of letters along with some constraints (min / max / total word length, etc).\n1 Twenty years of being familiar with this game and I never knew what it was called until I looked it up for this post. I could only find “word blossom” while Googling, but a friend I sent this to mentioned that it’s called spelling bee. Which makes sense since it looks like a honeycomb.Before proceeding, let’s load some packages.\n\n# for generating list of letters\nimport string \nimport random\n\n# for plotting\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import RegularPolygon\nimport numpy as np\n\n# for loading the word list\nimport json\n\n# for typing, because why not\nfrom typing import List\n\nThe variants I’ve encountered online – this, this, or this – consist of a sequence of 6 letters arranged around a central letter.\n\ndef generate_letters(seed: int, nchar: int = 7) -&gt; List[str]:\n    random.seed(seed)\n\n    central_letter = random.choice([\"A\", \"E\", \"I\", \"O\", \"U\"])\n    alphabet = string.ascii_uppercase.replace(central_letter, \"\")\n    non_central_letters = random.sample(population=alphabet, k=nchar-1)\n\n    return [central_letter] + non_central_letters\n\nletters = generate_letters(seed=95)\n\nprint(letters)\n\n['U', 'Q', 'Y', 'R', 'E', 'P', 'A']\n\n\nVisually, it looks something like this (using a modified version of code from this stackexchange post)2\n2 Not having written any Python for more than a year, I’d forgotten how much fun it was to customize matplotlib plots.\n\nCode\ndef plot_word_blossom(letters: List[str]) -&gt; None:\n  coord = [[0,0,0],[0,1,-1],[-1,1,0],[-1,0,1],[0,-1,1],[1,-1,0],[1,0,-1]]\n  colors = [[\"orange\"]] + [[\"white\"]] * 6\n  labels = [[l] for l in letters]\n  \n  # Horizontal cartesian coords\n  hcoord = [c[0] for c in coord]\n  \n  # Vertical cartersian coords\n  vcoord = [2. * np.sin(np.radians(60)) * (c[1] - c[2]) /3. for c in coord]\n  \n  fig, ax = plt.subplots(1)\n  ax.set_aspect(\"equal\")\n  \n  # Add some coloured hexagons\n  for x, y, c, l in zip(hcoord, vcoord, colors, labels):\n      color = c[0]\n      hex = RegularPolygon((x, y), numVertices=6, radius=2. / 3., \n                           orientation=np.radians(30), \n                           facecolor=color, alpha=1, edgecolor='k')\n      ax.add_patch(hex)\n      # Also add a text label\n      ax.text(x, y, l[0], ha=\"center\", va=\"center\", size=20)\n  \n  # Also add scatter points in hexagon centres\n  # setting alpha = 0 to not show the points\n  ax.scatter(hcoord, vcoord, c=[c[0].lower() for c in colors], alpha=0)\n  \n  plt.axis(\"off\")\n  plt.show();\n\nplot_word_blossom(letters)\n\n\n\n\n\n\n\n\n\nThe goal is to make as many words as possible that meet the following conditions\n\neach word must be an actual word (say present in a British English dictionary)\nword length between 4-7\neach word must contain the central letter\n\nThe version I used to play as a kid had the additional constraint that each letter could be used only once.\nTo solve this, I load a dictionary (obtained from here) and just look up words that match the requirements.\n\ndef get_list_of_eligible_words(min_length: int = 4, \n                               max_length: int = 7, \n                               no_duplicates: bool = True) -&gt; List[str]:\n      with open(\"words_dictionary.json\") as f:\n            words_dict = json.load(f)\n\n      words = list(words_dict.keys())\n      n_all = len(words)\n\n      if no_duplicates:\n            # keep words with no duplicate characters, \n            # e.g., set turns 'coop' into {'c', 'o', 'p'}\n            words = [w for w in words if len(w) == len(set(w))]\n\n      words = [w for w in words if len(w) &gt;= min_length and len(w) &lt;= max_length]\n\n      print(f\"Found {len(words):,} words out of {n_all:,} that meet the criteria.\")\n      \n      return words\n      \nwords = get_list_of_eligible_words()\n\nFound 43,239 words out of 370,101 that meet the criteria.\n\n\nThis next function takes the list of dictionary words and filters the subset of words that match the criteria.\n\ndef get_words(words: List[str], letters: List[str]) -&gt; List[str]:\n    central_letter = letters[0].lower()\n    letters_set = set([l.lower() for l in letters])\n    result = sorted([word for word in words if set(word.lower()).issubset(letters_set) and central_letter in word.lower()])\n    print(f\"Found {len(result)} words for the given letters {letters!r} with {central_letter!r} as the central letter.\")\n    return result\n\nprint(get_words(words=words, letters=letters), sep=\",\")\n\nFound 21 words for the given letters ['U', 'Q', 'Y', 'R', 'E', 'P', 'A'] with 'u' as the central letter.\n['aperu', 'paque', 'pareu', 'perau', 'peru', 'prau', 'prue', 'pure', 'purey', 'puya', 'quae', 'quar', 'quare', 'quay', 'query', 'quey', 'rupa', 'urea', 'yaru', 'yaup', 'yauper']\n\n\nHmm, some of these words are pretty uncommon. For example, ‘paque’ – pronounced ‘pack’ – means Easter (similar to the French word ‘Pâques’). ‘Perau’ is an alternate spelling for ‘perahu’ which means boat and comes from Indonesian / Malay. These are both new to me.\nI was tempted to add a little self-contained Shiny application within this quarto document using shinylive (github link) which I discovered while writing this post. However, I decided against this for the moment since the shinylive examples hosted on github-pages took a noticeable amount of time to load on my fast laptop + browser + internet connection.\nTrying to use both RStudio and VS Code, qmd and ipynb files together seems a bit clunky at the moment. I guess with time I’ll end up finding workarounds to reduce friction in this workflow."
  }
]