[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a data scientist / statistician / autodidact currently living in the magical city of Gent, Belgium. I have degrees in mathematics and statistics, as well as more than six years of experience working as a data scientist across multiple industries – advertising, marketing, telecommunications, and vaccine epidemiology.\nMy professional interests revolve around statistics / machine learning, programming, distributed computing, writing clean code, and using models to make real-world decisions.\nIn my free time, I like to read, cook, cycle, play tennis, and learn languages.\nThe primary reason for starting this (mostly) technical blog is to work through some (technical) concepts or challenges I encounter. Additional reasons include having my R / Python code in a clean and easy-to-navigate format, and to share recipes once in a while. I’ve been talking about wanting to have a blog since 2012, so I’m glad that I finally got around to having one in 2022.\nYou can reach out to me via LinkedIn or via email (first name [dot] last name [at] gmail)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Akshat Dwivedi",
    "section": "",
    "text": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method\n\n\n\n\n\n\n\nCausal Inference\n\n\nInstrumental Variables\n\n\nG-Estimation\n\n\nG-computation\n\n\nGEE\n\n\nAdvertising\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\n\n\n\n\n  \n\n\n\n\nSetup and test Python by programming a spelling bee solver\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHow should the untreated in the treatment group from an experiment be analyzed?\n\n\n\n\n\n\n\nCausal Inference\n\n\nInstrumental Variables\n\n\nAdvertising\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSpecifying an offset in a Tweedie model with identity link\n\n\n\n\n\n\n\nR\n\n\nTweedie\n\n\nOffset\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2023\n\n\n\n\n\n\n  \n\n\n\n\nOne reason why a (g)lm coefficient is NA\n\n\n\n\n\n\n\nR\n\n\nGLM\n\n\nMulticollinearity\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2023\n\n\n\n\n\n\n  \n\n\n\n\nGEE on aggregated data?\n\n\n\n\n\n\n\nR\n\n\nGLM\n\n\nGEE\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nDetermining the volume of my (cocktail) jigger\n\n\n\n\n\n\n\nMiscellaneous\n\n\nMeasurement\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHow Many Five-Star Ratings Do I Need?\n\n\n\n\n\n\n\nAnalysis\n\n\nSimulation\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHello, World!\n\n\n\n\n\n\n\nMiscellaneous\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/g-estimation-of-MSMM-with-an-IV/index.html",
    "href": "posts/g-estimation-of-MSMM-with-an-IV/index.html",
    "title": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method",
    "section": "",
    "text": "In a previous post, I used the ratio estimator and the method of instrumental variables (IV) to estimate the effect of showing an ad on sales from a fictitious ad campaign.\nThis post uses the lesser-known method of g-estimation1 to estimate the effect of a continuous exposure variable in a multiplicative structural mean model (MSMM) by using the randomization indicator as an instrumental variable to obtain valid treatment effect estimates in the presence of unmeasured confounding.1 less popular compared to IPTW and PSM, hence the clickbait-y title\nAs mentioned towards the end of the previous post, analyzing the treated individuals by collapsing them into a single ‘treated’ group was a simplification. The observed treatment variable would in reality be the number of impressions of the ad — naturally varying between individuals in the treated group, but zero among the individuals assigned to the control group and among the never-takers2.2 i.e., people who would not take the treatment if assigned to the treatment group. Defiers and always-takers aren’t possible as individuals in the control group cannot see an ad, because compliance in the control arm is ensured by design."
  },
  {
    "objectID": "posts/g-estimation-of-MSMM-with-an-IV/index.html#introduction",
    "href": "posts/g-estimation-of-MSMM-with-an-IV/index.html#introduction",
    "title": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method",
    "section": "",
    "text": "In a previous post, I used the ratio estimator and the method of instrumental variables (IV) to estimate the effect of showing an ad on sales from a fictitious ad campaign.\nThis post uses the lesser-known method of g-estimation1 to estimate the effect of a continuous exposure variable in a multiplicative structural mean model (MSMM) by using the randomization indicator as an instrumental variable to obtain valid treatment effect estimates in the presence of unmeasured confounding.1 less popular compared to IPTW and PSM, hence the clickbait-y title\nAs mentioned towards the end of the previous post, analyzing the treated individuals by collapsing them into a single ‘treated’ group was a simplification. The observed treatment variable would in reality be the number of impressions of the ad — naturally varying between individuals in the treated group, but zero among the individuals assigned to the control group and among the never-takers2.2 i.e., people who would not take the treatment if assigned to the treatment group. Defiers and always-takers aren’t possible as individuals in the control group cannot see an ad, because compliance in the control arm is ensured by design."
  },
  {
    "objectID": "posts/g-estimation-of-MSMM-with-an-IV/index.html#parameter-of-interest",
    "href": "posts/g-estimation-of-MSMM-with-an-IV/index.html#parameter-of-interest",
    "title": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method",
    "section": "Parameter of interest",
    "text": "Parameter of interest\nIf \\(U\\) indicates the (possibly multivalued) set of unmeasured confounders, \\(Z\\) the binary randomization variable indicating group assignment (treatment or control), \\(A\\) the integral exposure variable indicating observed treatment level \\(a\\) in \\(\\{0, 1, 2, … \\}\\), then an estimand or parameter of interest is the marginal causal risk ratio (CRR)\n\\[\n\\text{ATE} = \\frac{\\text{Pr}[Y^{a + 1} = 1]}{\\text{Pr}[Y^{a} = 1]}\n\\]\nThis is usually the target of inference in an experiment with perfect compliance, and corresponds to setting the treatment level to \\(A = a + 1\\) vs \\(A = a\\) for the full population. Another estimand of interest may be the average treatment effect in the treated (ATT) or the effect of the treatment on the treated (ETT). This compares the treated individuals to the counterfactual scenario had they not been treated. This corresponds to \\[\n\\text{ATT} = \\frac{\\text{Pr}[Y^a = 1 | A = a]}{\\text{Pr}[Y^{a = 0} = 1 | A = a]}\n\\]\nIn a setting with noncompliance (possibly due to unmeasured confounding), the estimand can be further conditioned on the IV \\(Z\\)\n\\[\n\\text{ATT}_{IV} = \\frac{\\text{Pr}[Y^a = 1 | A = a, Z]}{\\text{Pr}[Y^{a = 0} = 1 | A = a, Z]}\n\\]\nThe main MSMM used in this post estimates the ATE if the unmeasured confounders \\(U\\) are not effect-measure modifiers (i.e., the impact of treatment \\(A\\) on the ratio scale is the same on \\(Y\\) at all levels of \\(U\\)). If \\(U\\) is an effect modifier, but \\(Z\\) is not, then the MSMM estimate can be interpreted as the \\(\\text{ATT}_{IV}\\).\nPages 2-3 of this paper provide a concise yet illuminating discussion of the different effect measures that can be of interest.\nSince risk ratios are collapsible (see this excellent paper), the (confounder-adjusted) marginal and conditional estimands are identical3.3 which is only the case for risk differences and risk ratios.\nBefore writing any code, the main packages used in this post are attached.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(broom, include.only = \"tidy\")\n\n# functions from the following packages are called via package::fun()\n# to reduce namespace conflicts\n# library(geeM)\n# library(mgcv)\n# library(ivtools)\n# library(marginaleffects)\n# library(glue)\n# library(gridExtra)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "posts/g-estimation-of-MSMM-with-an-IV/index.html#g-estimation-in-a-nutshell",
    "href": "posts/g-estimation-of-MSMM-with-an-IV/index.html#g-estimation-in-a-nutshell",
    "title": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method",
    "section": "G-estimation in a nutshell",
    "text": "G-estimation in a nutshell\nThe first time I read the chapter on g-estimation in the what-if book, it went over my head completely4. Conceptually, it seemed harder than the more popular (g-)methods of g-computation and inverse probability of treatment weighting (IPTW). However, its superiority to g-computation in terms of not being prone to bias due to extrapolation, superiority to IPTW5 in terms of having less variability (and bias), and relative ease of handling continuous exposures (and continuous instruments) piqued my curiosity6.4 May have had something to do with the fact that I was attempting this at 3 AM in between sleep cycles.5 I think this isn’t an issue if overlap (ATO) weights are used instead of the usual ATE weights, as g-estimation gives higher weight to strata with greater overlap, and empty strata don’t contribute to the final estimate.6 I’m basing this on the Dukes, Vansteelandt paper.\nAnother advantage is that there’s no need to correctly model the relationship between the confounders \\(U\\) and the outcome \\(Y\\) for the correct estimation of the treatment effect. This would not be the case when using (log-)binomial or poisson regression, where misspecification of the \\(U-Y\\) relationship can bias the estimate for \\(A\\).\nThe essence of g-estimation is simple7 – once the true effect of the treatment \\(\\psi\\) is shaved off from the response variable \\(Y\\), the residuals \\(H(\\psi) = Y - \\psi A = Y^{a = 0}\\) should be uncorrelated with the treatment \\(A\\) in the confounder-adjusted propensity model, i.e., \\(\\text{Cov}(A, H(\\psi) | U) = 0\\)8. This is the same as picking the value of \\(\\psi\\) that leads to \\(\\alpha_1 = 0\\) in \\(\\mathbb{E}[A | Y^{a=0}, U] = \\alpha_0 + \\alpha_1 H(\\psi) + \\sum_{j=2}^k \\alpha_j U_{j-1}\\)9. For MSMMs, \\(H(\\psi) = Y \\text{exp}(- \\psi A)\\) is used where \\(\\psi\\) is the risk ratio on the log scale.7 after having grasped it that is8 Cov denotes covariance.9 For a binary exposure variable, the \\(\\mathbb{E}[A]\\) is replaced by \\(\\text{logit(Pr[A = 1])}\\), where \\(\\text{logit}(p) = \\text{log}\\Big(\\frac{p}{1-p}\\Big)\\) and \\(p\\) is the probability \\(\\text{Pr}[A = 1]\\).\nThis is works because of the conditional mean independence assumption — that the exposed and unexposed individuals are exchangeable within levels of \\(U\\) (i.e., \\(Y^{a = 0} \\perp\\!\\!\\!\\perp A|U\\)) as would be the case for a conditionally randomized experiment. In a marginally randomized experiment, the treatment assigned and received is independent of the potential outcomes, which would not be the case if people with (say) higher levels of \\(U\\) were more likely to take the treatment (\\(A &gt; 0\\)) and have a positive outcome \\(Y = 1\\). In this scenario, it wouldn’t be possible to say whether it was \\(A\\) or \\(U\\) causing better outcomes. This lack-of-identification is due to confounding between the effect of \\(U\\) on \\(Y\\) and the effect of \\(A\\) on \\(Y\\).\nSuppose the following dataset is available (with a true CRR value of 1.02, 70% treatment-control split, and 40% non-compliance (never-takers))\n\n\nCode\nsimulate_data &lt;- function(seed = 24, n = 1e5) {\n  \n  confounder_values &lt;- seq(0, 1e5, 50) / 1e4\n  \n  set.seed(seed)\n  tibble(\n    id = 1:n,\n    # randomization indicator / instrument, indicates group assignment\n    Z = rbinom(n, 1, prob = 0.7),\n    # simulate continuous confounder value\n    U = sample(x = confounder_values, size = n, replace = TRUE),\n    # simulate (conditional) compliance indicator\n    prob_C = plogis(2 - 0.5 * U),\n    C = rbinom(n, 1, prob_C),\n    # simulate observed treatment\n    # potential continuous exposure value is simulated \n    # for each individual on [1, 100]\n    # but for those that have Z = 0 (control) or C = 0 (never-takers), A = 0\n    A = Z * C * pmin(pmax(round(rgamma(n, shape = 5, scale = 4)), 1), 100),\n    # response variable simulated with risk ratios specified\n    # baseline log probabilities\n    logpY0 = log(0.002) + log(1.4) * U,\n    # impact of treatment\n    logpY1 = logpY0 + log(1.02) * A,\n    # simulated binary responses from log-binomial model\n    Y = rbinom(n, 1, exp(logpY1))\n  )\n}\n\ndf_large &lt;- simulate_data() %&gt;%\n  glimpse()\n\n\nRows: 100,000\nColumns: 9\n$ id     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, …\n$ Z      &lt;int&gt; 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, …\n$ U      &lt;dbl&gt; 2.795, 5.120, 4.930, 1.090, 3.675, 7.735, 8.770, 8.085, 4.780, …\n$ prob_C &lt;dbl&gt; 0.64622806, 0.36354746, 0.38580036, 0.81076675, 0.54053584, 0.1…\n$ C      &lt;int&gt; 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, …\n$ A      &lt;dbl&gt; 23, 0, 0, 21, 20, 0, 0, 0, 0, 24, 0, 24, 0, 0, 0, 0, 10, 0, 21,…\n$ logpY0 &lt;dbl&gt; -5.274168, -4.491870, -4.555800, -5.847853, -4.978073, -3.61199…\n$ logpY1 &lt;dbl&gt; -4.818708, -4.491870, -4.555800, -5.431998, -4.582020, -3.61199…\n$ Y      &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\nFor exploring g-estimation with no-unmeasured confounding (since \\(U\\) is measured) in the simple model represented by the directed acyclic graph (DAG) \\(A \\leftarrow U \\rightarrow Y\\) and \\(A \\rightarrow Y\\), one way of estimating the treatment effect \\(\\psi\\) is using grid search which visually looks like this when the estimated coefficient for \\(\\alpha_1\\) is plotted on the Y-axis\n\n\nCode\ngrid_search_coefficient &lt;- seq(-1, 1, 0.05) %&gt;%\n  # get the coefficient from the correct and incorrect propensity models\n  map_dfr(.f = ~ {\n\n    temp_df &lt;- df_large %&gt;%\n      mutate(H = Y * exp(-.x * A))\n\n    list(`No unmeasured confounding` = \"A ~ H + U\", \n         `Unmeasured confounding` = \"A ~ H\") %&gt;%\n      map_dfc(.f = ~ {\n        temp_df %&gt;%\n          lm(formula(.x), data = .) %&gt;%\n          tidy() %&gt;%\n          filter(term == \"H\") %&gt;%\n          pull(estimate)\n      }) %&gt;%\n      mutate(psi = .x, .before = `No unmeasured confounding`)\n  }) \n\ngrid_search_coefficient %&gt;%\n  pivot_longer(-psi) %&gt;%\n  ggplot(aes(x = exp(psi), y = value, color = name)) +\n  geom_point() +\n  geom_line() +\n  geom_vline(xintercept = 1.02, linetype = \"dotdash\") +\n  geom_hline(yintercept = 0, linetype = \"dotdash\") +\n  geom_hline(yintercept = -2.5, linetype = \"dotdash\") +\n  xlab(\"Risk ratio (true value: 1.02)\") +\n  ylab(\n    glue::glue(\"Estimated coefficient for H(\\U1D713)\",\n               \" \\nfrom the propensity model\")\n  ) +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  coord_cartesian(xlim = c(0, 3), ylim = c(-6, 4))\n\n\n\n\n\nor like this when the \\(p\\)-value is plotted on the Y-axis (on a narrower grid)\n\n\nCode\ngrid_search_pvalue &lt;- seq(-0.05, 0.05, 0.001) %&gt;%\n  # get the coefficient from the correct and incorrect propensity models\n  map_dfr(.f = ~ {\n\n    temp_df &lt;- df_large %&gt;%\n      mutate(H = Y * exp(-.x * A))\n\n    list(`No unmeasured confounding` = \"A ~ H + U\", \n         `Unmeasured confounding` = \"A ~ H\") %&gt;%\n      map_dfc(.f = ~ {\n        temp_df %&gt;%\n          lm(formula(.x), data = .) %&gt;%\n          tidy() %&gt;%\n          filter(term == \"H\") %&gt;%\n          pull(p.value)\n      }) %&gt;%\n      mutate(psi = .x, .before = `No unmeasured confounding`)\n  }) \n\ngrid_search_pvalue %&gt;%\n  pivot_longer(-c(psi)) %&gt;%\n  ggplot(aes(x = exp(psi), y = value, color = name)) +\n  geom_point() +\n  geom_line() +\n  geom_vline(xintercept = 1.02, linetype = \"dotdash\") +\n  geom_hline(yintercept = 0, linetype = \"dotdash\") +\n  xlab(\"Risk ratio (true value: 1.02)\") +\n  ylab(\"P-value for the test of alpha1 = 0\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\nFrom both the graphs, it’s evident that the estimate from the correctly specified propensity model is much closer to the true value of 1.02 (or 2% increase when going from \\(A = a\\) to \\(A = a + 1\\)) compared to the estimate from the propensity model without \\(U\\). Recovering the true effect in the presence of unmeasured confounding would require knowing which non-zero value of \\(\\alpha_1\\) (here about -2.5) corresponds to the true value of \\(\\psi\\).\nGrid search can be kinda tedious10 and computationally inefficient when there are multiple treatment parameters11, but this paper shows a neat trick using log-link Gamma Generalized Estimating Equations (GEE) to estimate risk ratios using regression adjustment for the propensity score10 It was annoying to use grid-search with p-values for this setting - if the grid was too coarse so there weren’t any points close to the true value, then the p-values were very close to zero, which explains the fine grid used here. Calling optimize(gest_function, interval = c(-5, 5), maximum = TRUE) where gest_function takes \\(\\psi\\) as an input and returns the p-value as the output failed to find the correct value, until the interval was narrowed to (0, 2). The coefficient estimate function wouldn’t work too well either, since risk ratio values less than 1 lead to estimates of \\(\\alpha_1\\) to be very close to 0.11 to account for effect-measure modification or heterogeneity of treatment effect\n\npropensity_scores &lt;- mgcv::gam(A ~ s(U, k = 10), data = df_large) %&gt;%\n  fitted()\n\npropensity_scores %&gt;% summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.625   2.061   5.345   5.847   9.565  12.438 \n\ngest_confounder &lt;- df_large %&gt;%\n  mutate(PS = propensity_scores) %&gt;%\n  geeM::geem(Y ~ PS + A, data = ., family = Gamma(link = \"log\"),\n             corstr = \"independence\", sandwich = TRUE)\n\nsummary(gest_confounder)\n\n            Estimates Model SE Robust SE    wald     p\n(Intercept)  -3.02200 0.061950  0.041350 -73.080 0e+00\nPS           -0.25020 0.009526  0.011600 -21.580 0e+00\nA             0.01937 0.003600  0.003923   4.936 8e-07\n\n Estimated Correlation Parameter:  0 \n Correlation Structure:  independence \n Est. Scale Parameter:  117.9 \n\n Number of GEE iterations: 2 \n Number of Clusters:  100000    Maximum Cluster Size:  1 \n Number of observations with nonzero weight:  100000 \n\n\nUnfortunately there’s no tidier for objects of class geem in the broom package, but it’s easy enough to write one to extract information from the fitted model as a data frame — see the following code block\n\n\nCode\ntidy.geem &lt;- function(x, conf.int = FALSE, \n                      exponentiate = FALSE, \n                      conf.level = 0.95, \n                      robust = TRUE, ...) {\n  # works only when broom is attached\n  # as it's added as an S3 method for broom::tidy(x, ...)\n  # could also do library(broom, include.only = \"tidy\")\n  #\n  # arguments are the same as broom::tidy.geeglm()\n  stopifnot(is.logical(robust))\n  s &lt;- summary(x)\n  ret &lt;- c(\"term\" = \"coefnames\", \"estimate\" = \"beta\",\n           \"std.error\" = if (robust) \"se.robust\" else \"se.model\",\n           \"statistic\" = \"wald.test\", \"p.value\" = \"p\") %&gt;%\n    map(.x = ., .f = ~ pluck(s, .x)) %&gt;%\n    as_tibble()\n\n  if (conf.int) {\n    p &lt;- (1 + conf.level) / 2\n    ret &lt;- ret %&gt;%\n      mutate(\n        conf.low = estimate - (qnorm(p = p) * std.error),\n        conf.high = estimate + (qnorm(p = p) * std.error),\n      )\n  }\n\n  if (exponentiate) {\n    # exponentiate the point estimate and the CI endpoints (if present)\n    ret &lt;- ret %&gt;%\n      mutate(\n        across(\n          .cols = -c(term, std.error, statistic, p.value),\n          .fns = exp\n        )\n      )\n  }\n\n  return(ret)\n}\n\ngest_confounder %&gt;%\n  tidy(exponentiate = TRUE, conf.int = TRUE, robust = TRUE) %&gt;%\n  filter(term == \"A\")\n\n\n# A tibble: 1 × 7\n  term  estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 A         1.02   0.00392      4.94 0.0000008     1.01      1.03\n\n\nThis can also be compared with the usual regression estimate\n\nglm_model &lt;- glm(Y ~ U + A, data = df_large, family = binomial(link = \"log\"))\n\nglm_model %&gt;% \n  tidy(exponentiate = TRUE, conf.int = TRUE) %&gt;% \n  mutate(across(.cols = where(is.numeric), .fns = ~ round(.x, 4)))\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   0.0022    0.0833    -73.3        0   0.0019    0.0026\n2 U             1.38      0.0104     31.1        0   1.36      1.41  \n3 A             1.02      0.0025      8.52       0   1.02      1.03  \n\n\nBut what if \\(U\\) is unmeasured?"
  },
  {
    "objectID": "posts/g-estimation-of-MSMM-with-an-IV/index.html#g-estimation-with-an-iv",
    "href": "posts/g-estimation-of-MSMM-with-an-IV/index.html#g-estimation-with-an-iv",
    "title": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method",
    "section": "G-estimation with an IV",
    "text": "G-estimation with an IV\nIn a randomized trial with perfect compliance, \\(A\\) and \\(Z\\) will be identical so a simple analysis using either one of these will result in an unbiased estimate of the average treatment effect. On the other hand, in the presence of imperfect compliance due to unmeasured confounding, an unadjusted analysis of \\(A - Y\\) will lead to biased estimates of the effect of treatment, and an unadjusted analysis of \\(Z - Y\\) will lead to an estimate of assigning treatment12 instead of receiving the treatment. Analyzing only \\(A-Y\\), the estimated treatment effect concentrates around 0.98 on average instead of 1.0212 i.e., the intention-to-treat effect (ITT)\n\n\nCode\n# specify the seed\nseq(1, 100, 1) %&gt;% \n  map_dbl(.f = ~ {\n    .x %&gt;% \n      simulate_data(seed = ., n = 1e4) %&gt;% \n      glm(Y ~ A, data = ., family = binomial(link = \"log\")) %&gt;%\n      tidy(exponentiate = TRUE) %&gt;%\n      filter(term == \"A\") %&gt;% \n      pull(estimate)\n  }) %&gt;% \n  summary()\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.9578  0.9741  0.9815  0.9810  0.9864  1.0035 \n\n\nIf the DAG with relationships \\(A \\leftarrow U \\rightarrow Y\\) and \\(Z \\rightarrow A \\rightarrow Y\\) holds but only \\((Z, A, Y)\\) are observed, and \\(Z\\) is a variable that meets the IV conditions13, the effect of \\(A\\) (\\(\\text{ATE}\\) or \\(\\text{ATT}_{IV}\\)) can be estimated under some additional assumptions.13 Associated with \\(A\\) (relevance), not associated with any variables in the set \\(U\\) (ignorability), and no direct effect on \\(Y\\) (exclusion restriction)\nThe key idea is captured by this (estimating) equation\n\\[\n\\sum_{i = 1}^n (z_i - \\bar{z})\\ y_i\\ \\text{exp}(-\\psi\\ a_i) = 0\n\\]\nwhere \\(\\bar{z}\\) denotes the proportion of individuals assigned to the treatment arm.\nSince \\(Z\\) is randomly assigned, it should be balanced with respect to all measured and unmeasured confounders \\(U\\). But \\(A\\) may not be. So the correct estimate of \\(\\psi\\) is the one that leads to zero covariance between the residuals \\(Y^{a = 0}\\) (or \\(H(\\psi)\\)) and \\(Z\\)14. Pretty magical, if you ask me.14 If \\((a_i - \\bar{a})\\) were to be used instead of \\((z_i - \\bar{z})\\), then the estimate would be the same one as from the \\(A- Y\\) analysis.\nThis equation can be coded up as a function which visually looks like this\n\n\nCode\nMSNMM_function &lt;- function(psi, data = df_large, positive = TRUE) {\n  result &lt;- data %&gt;%\n    mutate(\n      Zbar = mean(Z),\n      s = (Z - Zbar) * Y * exp(-psi * A)\n    ) %&gt;%\n    pull(s) %&gt;%\n    sum()\n\n  if (positive) result &lt;- abs(result)\n\n  return(result)\n}\n\nIV_grid &lt;- seq(-0.05, 0.5, 0.01) %&gt;%\n  tibble(psi = .) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    `Raw value` = MSNMM_function(psi, positive = FALSE),\n    `Absolute value` = abs(`Raw value`)\n  ) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(-psi) %&gt;%\n  mutate(name = fct_rev(factor(name))) \n\nIV_grid %&gt;%\n  ggplot(aes(x = exp(psi), y = value, color = name)) +\n  geom_point() + geom_line() +\n  geom_hline(yintercept = 0, linetype = \"dotdash\") +\n  geom_vline(xintercept = 1.02, linetype = \"dotdash\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  scale_x_continuous(n.breaks = 10) +\n  xlab(\"Risk ratio (true value: 1.02)\") +\n  ylab(\"Covariance between Z and H(\\U1D713)\")\n\n\n\n\n\nInstead of using grid search, the optimize() function can be used with the absolute value of the covariance to pick the value of \\(\\psi\\) that leads to 0 covariance1515 if the function is flat in the specified interval, the optimization step would return incorrect results (passing `interval = c(3, 5)` returns the value of 3 and an objective function value of 83.8 which is far away from 0)\n\noptimize(f = MSNMM_function, interval = c(-2, 2), maximum = FALSE)\n\n$minimum\n[1] 0.01308194\n\n$objective\n[1] 0.03266255\n\n\nThis is a slight underestimate (1.013 vs 1.02) — but it seems to be a sampling artifact as the median of 100 simulated values concentrates around 1.02\n\n\nCode\nIV_estimates &lt;- map_dbl(\n  .x = 1:100,\n  .f = ~ {\n    .x %&gt;%\n      simulate_data(seed = .) %&gt;%\n      optimize(\n        f = MSNMM_function, interval = c(-2, 2), \n        maximum = FALSE, data = .\n      ) %&gt;%\n      pluck(\"minimum\")\n  })\n\nIV_estimates %&gt;% exp() %&gt;% summary()\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.9966  1.0136  1.0206  1.0219  1.0304  1.0536 \n\n\nIt might just be simpler to use the ivglm() function from the ivtools package (see this paper) to accomplish the same thing though1616 which has other advantages like handling multiple instruments and different effect measures\n\n\nCode\nIV_model &lt;- glm(Z ~ 1, family = binomial(link = \"logit\"), data = df_large)\n\nIV_gest &lt;- df_large %&gt;%\n  # need to cast tibble as a data frame to avoid cryptic errors\n  as.data.frame() %&gt;%\n  ivtools::ivglm(estmethod = \"g\",\n                 X = \"A\", Y = \"Y\",\n                 fitZ.L = IV_model,\n                 link = \"log\", data = .)\n\nIV_gest %&gt;% summary()\n\n\n\nCall:  \nivtools::ivglm(estmethod = \"g\", X = \"A\", Y = \"Y\", fitZ.L = IV_model, \n    data = ., link = \"log\")\n\nCoefficients: \n  Estimate Std. Error z value Pr(&gt;|z|)\nA  0.01310    0.01065    1.23    0.219\n\n\n\n\nCode\n# get the coefficient and CI\nglue::glue(\n  \"Mean: {round(exp(c(summary(IV_gest)$coefficients[1, 1])), 4)}\", \n  \" with 95% CI: ({round(exp(confint(IV_gest))[1], 4)},\", \n  \" {round(exp(confint(IV_gest))[2], 4)})\"\n)\n\n\nMean: 1.0132 with 95% CI: (0.9923, 1.0346)\n\n\nwhere it produces nearly the same estimate as above and additionally has the option of plotting the shape of the estimating equations in a neighborhood of the estimate\n\n\nCode\n# can use ggplot on the output from estfun()\n# ivtools::estfun(IV_gest, lower = -0.1, upper = 0.1, step = 0.01) %&gt;% \n#   pluck(\"f\") %&gt;%  \n#   pluck(\"A\") %&gt;% \n#   data.frame() %&gt;% \n#   ggplot(aes(x = exp(psi), y = Hsum)) + \n#   geom_point() +\n#   geom_line() \n\nplot(ivtools::estfun(IV_gest, lower = -0.1, upper = 0.1, step = 0.01))\n\n\n\n\n\nA caveat is that this approach can sometimes fail when this function has a weird shape in the presence of weak instruments (see this paper)."
  },
  {
    "objectID": "posts/g-estimation-of-MSMM-with-an-IV/index.html#estimating-probabilities",
    "href": "posts/g-estimation-of-MSMM-with-an-IV/index.html#estimating-probabilities",
    "title": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method",
    "section": "Estimating probabilities",
    "text": "Estimating probabilities\nIf we had the model which included \\(U\\) as a covariate, conditional probabilities \\(\\text{Pr}[Y = 1 | A = a, U = u]\\) as well as marginal probabilities \\(\\text{Pr}[Y = 1 | A = a]\\) (averaged over levels of \\(U\\)) could be generated using the amazing marginaleffects package to perform g-computation with the resulting dose-response curves visualized.\nThe following plot with conditional dose-response curves at quartiles of \\(U\\) shows that the effect of the treatment with increasing \\(A\\) is increasing across levels of \\(U\\) — which makes sense because the treatment effect is a multiplicative (percent) change relative to the previous level1717 the effects of \\(A\\) and \\(U\\) are linear on the log scale but not on the transformed probability scale\n\n\nCode\n# get predicted probabilities from the model at quartiles of U\nglm_model %&gt;%\n  marginaleffects::plot_predictions(condition = list(\"A\", \"U\" = \"fivenum\")) +\n  ylab(\"Predicted probability of Y = 1\") + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\nWhen \\(U\\) is unmeasured, only the marginal dose-response curve can be estimated, and the following plot compares g-computation and g-estimation estimates and their CIs, with the true curve plotted for comparison\n\n\nCode\n# simulate 100 datasets and take the mean of probabilities under A = 0\nbaseline_conversion_probability &lt;- map_dbl(\n  .x = 1:100,\n  .f = ~ {\n    simulate_data(seed = .x, n = 1e5) %&gt;%\n      pull(logpY0) %&gt;%\n      exp() %&gt;%\n      mean()\n    }) %&gt;%\n  mean()\n\n# true dose-response curve\ntrue_dose_response_curve &lt;- tibble(\n  A = 0:73,\n  estimate = baseline_conversion_probability * exp(log(1.02) * A)\n)\n\n# g-computation by creating counterfactual predictions\n# by setting A = a and averaging the predicted probabilities\n# doing this over a coarser grid to get faster run times\ng_computation_dose_response_extrapolated_levels &lt;- glm_model %&gt;%\n  marginaleffects::predictions(\n    by = \"A\", type = \"response\",\n    newdata = marginaleffects::datagrid(\n      A = c(0, seq(5, 70, 5), 73), grid_type = \"counterfactual\"\n    )\n  ) %&gt;%\n  select(A, estimate, conf.low, conf.high) %&gt;%\n  as_tibble() %&gt;% \n  mutate(type = \"G-computation\")\n\n# get estimates of Pr[Y = 1 | A = 0]\n# by shaving off the effect of the treatment\n# using the point estimate and CI endpoints from ivglm\ng_estimation_pY0 &lt;- c(\n  IV_gest %&gt;% pluck(\"est\") %&gt;% unname(),\n  IV_gest %&gt;% confint() %&gt;% unname()\n) %&gt;%\n  set_names(nm = c(\"estimate\", \"conf.low\", \"conf.high\")) %&gt;%\n  map_dfr(\n    .f = ~ {\n      df_large %&gt;%\n        mutate(H = Y * exp(-.x * A)) %&gt;%\n        pull(H) %&gt;%\n        mean() %&gt;%\n        tibble(logRR = .x, pY0 = .)\n    }, \n    .id = \"name\"\n  )\n\n# using logRR estimate + CI and Pr[Y = 1 | A = 0]\ng_estimation_dose_response &lt;- g_estimation_pY0 %&gt;%\n  expand_grid(., A = 0:73) %&gt;%\n  # get P[Y = 1 | A = a]\n  mutate(pY = pY0 * exp(logRR * A)) %&gt;%\n  select(-pY0, -logRR) %&gt;%\n  pivot_wider(id_cols = A, names_from = \"name\", values_from = \"pY\") %&gt;%\n  mutate(type  = \"G-estimation\")\n\nbind_rows(\n  g_computation_dose_response_extrapolated_levels,\n  g_estimation_dose_response\n) %&gt;%\n  ggplot(aes(x = A, y = estimate, color = type)) +\n  geom_line() +\n  # plot the true curve\n  geom_line(\n    data = true_dose_response_curve, \n    aes(x = A, y = estimate), \n    color = \"gray20\", linewidth = 1.2,, linetype = \"dotdash\",\n    inherit.aes = FALSE\n  ) + \n  geom_ribbon(\n    aes(ymin = conf.low, ymax = conf.high, fill = type), alpha = 0.2\n  ) +\n  scale_y_continuous(breaks = seq(0, 0.25, 0.05)) +\n  scale_x_continuous(breaks = seq(0, 80, 10)) +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  ylab(\"Predicted probability of Y = 1\") + \n  xlab(\"A (true curve in gray)\")\n\n\n\n\n\nAssuming no-effect modification by \\(U\\), The point estimate and the CI endpoints from ivglm are used to first estimate \\(\\text{Pr}[Y = 1 | A = 0]\\) (which is the same as \\(\\text{Pr}[Y^{a = 0} = 1]\\)18) by using the following relation18 assuming consistency\n\\[\n\\text{Pr}[Y^{a = 0} = 1] = \\text{Pr}[Y = 1 | A = 0] = \\text{E}[Y\\ \\text{exp}(\\psi A)]\n\\]\nand then using these to get the dose response curves and the following CIs\n\\[\n\\text{Pr}[Y = 1 | A] = \\text{Pr}[Y = 1 | A = 0]\\ \\text{exp}[\\psi A]\n\\]\nThe g-computation estimate is far narrower as the model has access to the information provided by \\(U\\), compared to the MSMM. The MSMM estimate for this sample underestimates the true value, which shows up as a large difference between the true and the MSMM curves at values of \\(A\\) higher than 60.\nThese probabilities can be mapped to total sales / profit given the number of individuals within each treatment level. Whether these numbers can be transported or extrapolated to future campaigns or other populations depends on how similar the distribution of \\(U\\) is across these groups. In that sense, these marginal estimates are actually conditional on the distribution of confounders and effect modifiers. I came across this paper which would be interesting to read in the future."
  },
  {
    "objectID": "posts/g-estimation-of-MSMM-with-an-IV/index.html#references",
    "href": "posts/g-estimation-of-MSMM-with-an-IV/index.html#references",
    "title": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method",
    "section": "References",
    "text": "References\nThis post is based on the following references\n\nHernán, Robins - Causal Inference: What If? - Chapters 14 (g-estimation), and 16 (instrumental variables)\nVansteelandt et al. 2011 - On Instrumental Variables Estimation of Causal Odds Ratios\nPalmer et al. 2011 - Instrumental Variable Estimation of Causal Risk Ratios and Causal Odds Ratios in Mendelian Randomization Analyses\nDukes, Vansteelandt 2018 - A Note on G-Estimation of Causal Risk Ratios\nBurgess et al. 2014 - Lack of Identification in Semiparametric Instrumental Variable Models With Binary Outcomes\nSjolander, Martinussen 2019 - Instrumental Variable Estimation with the R Package ivtools"
  },
  {
    "objectID": "posts/g-estimation-of-MSMM-with-an-IV/index.html#simulated-data",
    "href": "posts/g-estimation-of-MSMM-with-an-IV/index.html#simulated-data",
    "title": "Using an obscure causal inference method to estimate treatment effects with the help of another less obscure causal inference method",
    "section": "Simulated data",
    "text": "Simulated data\nThe four figures — going clockwise from top left — show 1) the probability of compliance as a function of the confounder \\(U\\) (where \\(\\text{Pr}[C = 1 | U] = 1 / (1 + \\text{exp}(-2 + 0.5)) u,\\ u \\in [0, 10]\\)), 2) the realized distribution of the treatment levels among those who received the treatment19 (simulated from the truncated Gamma distribution \\(x_i \\sim \\Gamma(\\text{shape} = 5,\\ \\text{scale} = 4) \\cdot I(1 \\leq x_i \\leq 100)\\)), 3) balance of \\(U\\) across levels of \\(Z\\) (which is balanced because of randomization), and 4) balance of \\(U\\) across levels of \\(A\\) (which is not balanced because of confounding by \\(U\\))19 a value is sampled for each individual, but is realized only if \\(Z = 1\\) and \\(C = 1\\) (where \\(C\\) is the compliance indicator)\n\n\nCode\n# probability of compliance as a function of the confounder\np_compliance &lt;- df_large %&gt;%\n  ggplot(aes(x = U, y = prob_C)) +\n  geom_line() + \n  ylab(\"Compliance probability\") +\n  coord_cartesian(y = c(0, 1))\n\n# observed distribution of exposure levels\nA_dist &lt;- df_large %&gt;%\n  filter(A &gt; 0) %&gt;%\n  ggplot(aes(x = A)) +\n  geom_bar() +\n  scale_x_continuous(breaks = seq(0, 100, 10), \n                     labels = seq(0, 100, 10))\n\n# check balance of U across Z\n# balanced as expected, because Z is a randomization indicator\nUZ_plot &lt;- df_large %&gt;%\n  mutate(Z = factor(Z, levels = c(1, 0), \n                    labels = c(\"Treatment\", \"Control\"))) %&gt;%\n  ggplot(aes(x = U, color = Z)) +\n  geom_density() +\n  theme(legend.position = \"bottom\")\n\n# check balance across (dichotomized version of) A\n# not balanced because U and A are associated\nUA_plot &lt;- df_large %&gt;%\n  mutate(\n    `A (binary)` = factor(as.numeric(A &gt; 0), \n                          levels = c(1, 0), \n                          labels = c(\"Treated\", \"Control\"))) %&gt;%\n  ggplot(aes(x = U, color = `A (binary)`)) +\n  geom_density() +\n  theme(legend.position = \"bottom\")\n\ngridExtra::grid.arrange(p_compliance, A_dist, \n                        UZ_plot, UA_plot, ncol = 2)"
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html",
    "href": "posts/gee-on-aggregated-data/index.html",
    "title": "GEE on aggregated data?",
    "section": "",
    "text": "TL;DR: The geepack R package doesn’t do what it doesn’t claim to do"
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html#model",
    "href": "posts/gee-on-aggregated-data/index.html#model",
    "title": "GEE on aggregated data?",
    "section": "Model",
    "text": "Model\nThe code below simulates data from the following mixed-effects model2 with random-intercept terms where person \\(j\\) is nested within cluster \\(i\\). Each cluster has a different proportion of treatment uptake \\(p_{treated,j}\\) varying between 40-60% where \\(X_{ij}\\) indicates whether person \\(j\\) in cluster \\(i\\) received the treatment or not.2 The model syntax is inspired by the statistical rethinking book\n\\[\n\\begin{gather}\nY_{ij} \\sim \\text{Bernoulli}(p_{ij}) \\\\\n\\text{logit}(p_{ij}) = -1.4 + 0.3 \\times X_{ij} + \\alpha_i \\\\\n\\alpha_i \\sim \\text{Normal}(0, 1) \\\\\nX_{ij} \\sim \\text{Bernoulli}(p_{treated,i}) \\\\\np_{treated,i} \\sim \\text{Uniform(0.4, 0.6)}\n\\end{gather}\n\\]\nThe time taken to train the model on the raw as well as the aggregate data, the point estimates, and standard errors for the coefficients are stored for analysis."
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html#code",
    "href": "posts/gee-on-aggregated-data/index.html#code",
    "title": "GEE on aggregated data?",
    "section": "Code",
    "text": "Code\n\nsimulate_data_and_get_estimates &lt;- function(n, n_clus, seed) {\n  set.seed(seed)\n  # cluster random intercept\n  rand_intercept &lt;- rnorm(n_clus, mean = 0, sd = 1)\n\n  set.seed(seed)\n  sim_data &lt;- map_dfr(.x = 1:n_clus, .f = ~ {\n    tibble(\n      id = .x,\n      x = rbinom(round(n / n_clus), size = 1, prob = runif(1, 0.4, 0.6)),\n      y = rbinom(round(n / n_clus), size = 1, prob = plogis(-1.4 + 0.3 * x + rand_intercept[[.x]]))\n    )\n  })\n\n  # fit model to raw data\n  t0 &lt;- Sys.time()\n  mod1_sim &lt;- geeglm(\n    y ~ x,\n    data = sim_data,\n    family = \"binomial\", id = id, corstr = \"exchangeable\"\n  )\n  t1 &lt;- Sys.time()\n  t1 &lt;- as.numeric(difftime(t1, t0, units = \"secs\"))\n\n  # fit GLM model to raw data\n  mod1_sim_glm &lt;- glm(y ~ factor(id) + x - 1,\n                      data = sim_data, family = \"binomial\")\n\n  # fit model to aggregated data\n  agg_data &lt;- sim_data %&gt;%\n    group_by(id, x) %&gt;%\n    summarize(n = n(), y = sum(y), .groups = \"drop\") %&gt;%\n    mutate(y_prob = y / n)\n\n  t2 &lt;- Sys.time()\n  mod2_sim &lt;- geeglm(\n    y_prob ~ x,\n    data = agg_data, weights = n,\n    family = \"binomial\", id = id, corstr = \"exchangeable\"\n  )\n  t3 &lt;- Sys.time()\n  t3 &lt;- as.numeric(difftime(t3, t2, units = \"secs\"))\n\n  # fit GLM model to aggregated data\n  mod2_sim_glm &lt;- glm(y_prob ~ factor(id) + x - 1, data = agg_data,\n                      weights = n, family = \"binomial\")\n\n  results &lt;- bind_rows(\n    # GEE results\n    broom::tidy(mod1_sim) %&gt;%\n      mutate(data = 'raw data', time_secs = t1, estimator = \"GEE\"),\n    broom::tidy(mod2_sim) %&gt;%\n      mutate(data = 'aggregated data', time_secs = t3, estimator = \"GEE\"),\n    # GLM results\n    broom::tidy(mod1_sim_glm) %&gt;%\n      mutate(data = 'raw data', time_secs = NA_real_, estimator = \"GLM\"),\n    broom::tidy(mod2_sim_glm) %&gt;%\n      mutate(data = 'aggregated data', time_secs = NA_real_, estimator = \"GLM\")\n  ) %&gt;%\n    mutate(n = n, n_clus = n_clus, seed = seed) %&gt;%\n    filter(!stringr::str_detect(term, pattern = \"id\"))\n\n  return(results)\n}\n\nHere’s what the output from this function looks like:\n\nsimulate_data_and_get_estimates(n = 100, n_clus = 5, seed = 3)\n\n# A tibble: 6 × 11\n  term       estim…¹ std.e…² stati…³ p.value data  time_s…⁴ estim…⁵     n n_clus\n  &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercep…  -1.59    0.409 15.0    1.07e-4 raw …  0.00748 GEE       100      5\n2 x           -0.144   0.383  0.141  7.07e-1 raw …  0.00748 GEE       100      5\n3 (Intercep…  -1.73    0.395 19.1    1.27e-5 aggr…  0.00326 GEE       100      5\n4 x           -0.107   0.402  0.0702 7.91e-1 aggr…  0.00326 GEE       100      5\n5 x           -0.266   0.596 -0.446  6.55e-1 raw … NA       GLM       100      5\n6 x           -0.266   0.596 -0.446  6.55e-1 aggr… NA       GLM       100      5\n# … with 1 more variable: seed &lt;dbl&gt;, and abbreviated variable names ¹​estimate,\n#   ²​std.error, ³​statistic, ⁴​time_secs, ⁵​estimator\n\n\n\nsimulation_parameters &lt;- expand_grid(\n  n = c(100, 500, 1000, 2500, 5000, 7500, 10000),\n  n_clus = c(5, 50, 250),\n  seed = 1:20\n) %&gt;% \n  # remove runs where number of clusters is larger than total sample size\n  filter(n_clus &lt; n)\n\n# set up multicore processing\nplan(multisession, workers = 15)\n\n# parallelize purrr::pmap_dfr by using the {furrr} package\nsimulation_results &lt;- future_pmap_dfr(\n  .l = simulation_parameters,\n  # have to pass args as ..1 or ..2, else it fails\n  .f = ~ simulate_data_and_get_estimates(n = ..1, n_clus = ..2, seed = ..3),\n  .options = furrr_options(seed = NULL)\n)\n\nplan(sequential) # turn off multicore\n\nThis chunk can take a while to run for some of the parameter combinations, so pre-computed results are loaded and analyzed further.\n\nsimulation_results &lt;- read_csv(file = \"gee_glm_sim.csv\", show_col_types = FALSE) %&gt;% \n  mutate(\n    n_clus_raw = n_clus,\n    n_clus = factor(paste0(\"# clusters: \", n_clus),\n                    levels = paste0(\"# clusters: \", c(5, 50, 250))),\n    term = case_when(\n      term == \"(Intercept)\" ~ \"Intercept\",\n      term == \"x\" ~ \"Slope\", \n      TRUE ~ term\n  ))\n\nglimpse(simulation_results)\n\nRows: 2,400\nColumns: 12\n$ term       &lt;chr&gt; \"Intercept\", \"Slope\", \"Intercept\", \"Slope\", \"Slope\", \"Slope…\n$ estimate   &lt;dbl&gt; -7.17e-01, -7.38e-02, -5.35e-01, -1.14e-01, -5.92e-02, -5.9…\n$ std.error  &lt;dbl&gt; 4.69e-01, 1.75e-01, 4.09e-01, 1.53e-01, 4.86e-01, 4.86e-01,…\n$ statistic  &lt;dbl&gt; 2.3402, 0.1772, 1.7056, 0.5518, -0.1216, -0.1216, 7.1290, 4…\n$ p.value    &lt;dbl&gt; 1.26e-01, 6.74e-01, 1.92e-01, 4.58e-01, 9.03e-01, 9.03e-01,…\n$ data       &lt;chr&gt; \"raw data\", \"raw data\", \"aggregated data\", \"aggregated data…\n$ time_secs  &lt;dbl&gt; 0.01389, 0.01389, 0.00420, 0.00420, NA, NA, 0.00928, 0.0092…\n$ estimator  &lt;chr&gt; \"GEE\", \"GEE\", \"GEE\", \"GEE\", \"GLM\", \"GLM\", \"GEE\", \"GEE\", \"GE…\n$ n          &lt;dbl&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,…\n$ n_clus     &lt;fct&gt; # clusters: 5, # clusters: 5, # clusters: 5, # clusters: 5,…\n$ seed       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4,…\n$ n_clus_raw &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…"
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html#computation-time",
    "href": "posts/gee-on-aggregated-data/index.html#computation-time",
    "title": "GEE on aggregated data?",
    "section": "Computation time",
    "text": "Computation time\nFirst, the time taken to fit the model to the raw dataset is shown as a function of total sample size \\(n\\) where there are \\(n / n_{clus}\\) observations per cluster. Each panel has a different y-axis to allow reading values directly from the plot.\n\nsimulation_results %&gt;%\n  filter(data == \"raw data\", estimator == \"GEE\") %&gt;%\n  # distinct here since the data frame contains a row for\n  # each of the intercept and slope terms but the runtimes are the same\n  # for both these terms\n  distinct(seed, time_secs, n, n_clus) %&gt;%\n  ggplot(aes(x = n, y = time_secs, group = n_clus)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, degree = 3)) +\n  geom_smooth(method = \"lm\", formula = y ~ I(x ^ 3), color = \"forestgreen\") +\n  stat_summary(aes(group = interaction(n_clus, n)), geom = \"point\", \n               fun = mean, color = \"darkorange\", size = 2) + \n  xlab(\"Total sample size (n)\") +\n  ylab(\"Runtime (in seconds)\") +\n  facet_wrap(~ n_clus, scales = \"free\")\n\n\n\n\nThe blue line fits a cubic polynomial \\[time = \\beta_0 + \\beta_1 n + \\beta_2 n^2 + \\beta_3 n^3 + \\varepsilon\\] to the run times as a function of sample size. The green line fits the same model but without the linear and quadratic terms, i.e, \\(time = \\alpha_0 + \\alpha_1 n^3 + \\varepsilon\\). The yellow points are the mean run times for each level of \\(n\\) within each panel.\nWhen the number of clusters is very small, an increase in the sample size within the largest cluster leads to a cubic increase in expected run times. The closeness of the blue and green lines in the leftmost panel indicates that the cubic term in the execution time model dominates the lower order terms. In the second and third panels, the \\(O(n^3)\\) limit hasn’t hit yet, as the green and the blue lines are in disagreement.\nFor 10000 samples split into 2000 samples in each of the five clusters, it takes approximately 15 minutes per run. The same total sample size split into 250 samples in each of the 40 clusters, each model takes about 7 seconds to run, and even less time when there are 250 clusters. This is a massive difference in time between the different settings."
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html#convergence-issues",
    "href": "posts/gee-on-aggregated-data/index.html#convergence-issues",
    "title": "GEE on aggregated data?",
    "section": "Convergence issues",
    "text": "Convergence issues\nHowever, unfortunately some of the models run into convergence issues, where the point estimates and the standard errors explode\n\nsimulation_results %&gt;% \n  filter(estimator == \"GEE\") %&gt;% \n  arrange(desc(std.error)) %&gt;% \n  head(n = 10)\n\n# A tibble: 10 × 12\n   term      estimate std.e…¹ stati…² p.value data  time_…³ estim…⁴     n n_clus\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;fct&gt; \n 1 Intercept  4.24e16 1.22e16  12.1   5.12e-4 aggr… 0.00516 GEE     10000 # clu…\n 2 Intercept  1.13e17 8.91e15 161.    0       aggr… 0.00621 GEE      7500 # clu…\n 3 Intercept -1.99e17 7.88e15 641.    0       aggr… 0.0175  GEE      1000 # clu…\n 4 Intercept  2.32e15 7.25e15   0.102 7.49e-1 aggr… 0.00667 GEE      7500 # clu…\n 5 Intercept  5.32e15 6.59e15   0.650 4.20e-1 aggr… 0.00582 GEE       100 # clu…\n 6 Slope     -1.69e15 2.25e15   0.562 4.53e-1 aggr… 0.0110  GEE      2500 # clu…\n 7 Intercept  9.69e14 2.12e15   0.210 6.47e-1 aggr… 0.00558 GEE     10000 # clu…\n 8 Slope     -1.03e15 2.09e15   0.241 6.23e-1 aggr… 0.00763 GEE      7500 # clu…\n 9 Slope     -1.94e15 2.05e15   0.896 3.44e-1 aggr… 0.00503 GEE     10000 # clu…\n10 Slope      3.82e15 1.88e15   4.13  4.23e-2 aggr… 0.00817 GEE      5000 # clu…\n# … with 2 more variables: seed &lt;dbl&gt;, n_clus_raw &lt;dbl&gt;, and abbreviated\n#   variable names ¹​std.error, ²​statistic, ³​time_secs, ⁴​estimator\n\n\nThis seems to be a problem when training GEE models on aggregated data when the number of clusters is low irrespective of the total sample size, which leads to about 21% of the estimates from the aggregated data being convergence failures. An estimate is flagged as convergence failure when the stdandard error of the coefficients &gt; 5 on the logit scale.\n\nsimulation_results &lt;- simulation_results %&gt;%\n  mutate(high_se = as.numeric(std.error &gt; 5)) \n\nsimulation_results %&gt;%\n  filter(estimator == \"GEE\") %&gt;% \n  select(-estimate, -std.error, -statistic, -p.value, -time_secs) %&gt;%\n  # put intercept and slope estimates in one row\n  pivot_wider(everything(), names_from = term, values_from = high_se) %&gt;%\n  # if at least one of the intercept or slope terms have a very high se\n  mutate(high_se = pmin(Intercept + Slope, 1)) %&gt;%\n  group_by(data, n_clus, n) %&gt;%\n  summarize(n_total = n(), n_failed = sum(high_se), .groups = \"drop_last\") %&gt;%\n  #print(n = Inf) %&gt;%\n  summarize(n_total = sum(n_total), n_failed = sum(n_failed), .groups = \"drop\") %&gt;%\n  mutate(percent_failed = 100 * n_failed / n_total)\n\n# A tibble: 6 × 5\n  data            n_clus          n_total n_failed percent_failed\n  &lt;chr&gt;           &lt;fct&gt;             &lt;int&gt;    &lt;dbl&gt;          &lt;dbl&gt;\n1 aggregated data # clusters: 5       140       29           20.7\n2 aggregated data # clusters: 50      140        0            0  \n3 aggregated data # clusters: 250     120        0            0  \n4 raw data        # clusters: 5       140        0            0  \n5 raw data        # clusters: 50      140        0            0  \n6 raw data        # clusters: 250     120        0            0"
  },
  {
    "objectID": "posts/gee-on-aggregated-data/index.html#raw-vs-aggregated-data-estimates",
    "href": "posts/gee-on-aggregated-data/index.html#raw-vs-aggregated-data-estimates",
    "title": "GEE on aggregated data?",
    "section": "Raw vs aggregated data estimates",
    "text": "Raw vs aggregated data estimates\nThese rows with convergence failures can be removed and the agreement between the estimates from the raw vs the aggregate datasets can be assessed visually\n\nslope_plot_data &lt;- simulation_results %&gt;%\n  filter(term == \"Slope\", high_se == 0) %&gt;%\n  select(seed, term, data, estimate, n_clus, n, estimator) %&gt;%\n  tidyr::pivot_wider(id_cols = c(seed, term, n_clus, n, estimator),\n                     names_from = data, values_from = estimate) %&gt;%\n  mutate(`Total sample size` = factor(n), \n         id = interaction(estimator, n_clus, sep = \", \", lex.order = TRUE)) \n\nslope_plot_data %&gt;% \n  filter(estimator == \"GEE\") %&gt;% \n  ggplot(aes(x = `raw data`, y = `aggregated data`, color = `Total sample size`)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  # geom_point(data = tibble(x = 0.3, y = 0.3), \n  #            aes(x = x, y = y), \n  #            color = \"black\", size = 3, inherit.aes = FALSE) +\n  xlab(\"Slope coefficient from the full dataset\") +\n  ylab(\"Slope coefficient from the aggregated dataset\") +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 1)) + \n  facet_wrap(~ id, ncol = 3, scales = \"fixed\")\n\nWarning: Removed 29 rows containing missing values (geom_point).\n\n\n\n\n\nThis plot compares the estimated slope coefficient (true value 0.3) from the GEE model on aggregated data vs the estimated slope coefficient from the model on the raw data while varying the number of clusters as well as the total sample size. The agreement between estimates increases as the number of clusters increases (going from the top left panel to the top right panel).\nOn the other hand, the estimates from running GLMs on the same aggregated and raw datasets are identical, as indicated by the points lying precisely on the black line in the following plot\n\nslope_plot_data %&gt;% \n  filter(estimator == \"GLM\") %&gt;% \n  ggplot(aes(x = `raw data`, y = `aggregated data`, color = `Total sample size`)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  # geom_point(data = tibble(x = 0.3, y = 0.3), \n  #            aes(x = x, y = y), \n  #            color = \"black\", size = 3, inherit.aes = FALSE) +\n  xlab(\"Slope coefficient from the full dataset\") +\n  ylab(\"Slope coefficient from the aggregated dataset\") +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 1)) + \n  facet_wrap(~ id, ncol = 3, scales = \"fixed\")\n\n\n\n\nAs expected, the estimates for \\(n = 10,000\\) are tightly clustered around the true value compared to \\(n = 100\\) in both sets of plots.\nOk so the estimates aren’t identical for the GEE models unfortunately, but are they approximately similar across the raw and aggregated datasets? This is assessed visually via box plots below for the case of 5 clusters in the data\n\ndist_plot_data &lt;- simulation_results %&gt;% \n  mutate(\n    ci_lower = estimate - (qnorm(0.975) * std.error),\n    ci_upper = estimate + (qnorm(0.975) * std.error), \n    ci_width = ci_upper - ci_lower, \n    ci_upper_half_width = ci_upper - estimate\n  ) %&gt;% \n  filter(term == \"Slope\", high_se == 0, estimator == \"GEE\") %&gt;% \n  select(\n    Estimate = estimate, \n    `Std. Error` = std.error, \n    `95% CI LL` = ci_lower, \n    `95% CI UL` = ci_upper,\n    `95% CI Width` = ci_width,\n    `95% CI Upper HW` = ci_upper_half_width,\n    n, n_clus_raw, n_clus, data, seed) %&gt;% \n  tidyr::pivot_longer(cols = Estimate:`95% CI Upper HW`, \n                      names_to = \"statistic\", \n                      values_to = \"values\") %&gt;% \n  mutate(\n    statistic = factor(statistic,\n                       levels = c(\"Estimate\", \"Std. Error\", \n                                  \"95% CI LL\", \"95% CI UL\", \n                                  \"95% CI Width\", \"95% CI Upper HW\")), \n    n = factor(n, ordered = TRUE), \n    data = stringr::str_to_title(data)\n  )\n\nglimpse(dist_plot_data)\n\nRows: 4,626\nColumns: 7\n$ n          &lt;ord&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,…\n$ n_clus_raw &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ n_clus     &lt;fct&gt; # clusters: 5, # clusters: 5, # clusters: 5, # clusters: 5,…\n$ data       &lt;chr&gt; \"Raw Data\", \"Raw Data\", \"Raw Data\", \"Raw Data\", \"Raw Data\",…\n$ seed       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3,…\n$ statistic  &lt;fct&gt; Estimate, Std. Error, 95% CI LL, 95% CI UL, 95% CI Width, 9…\n$ values     &lt;dbl&gt; -0.0738, 0.1753, -0.4174, 0.2698, 0.6871, 0.3436, -0.1137, …\n\n\n\ndist_plot_data %&gt;% \n  filter(n_clus_raw == 5) %&gt;% \n  ggplot(aes(x = n, y = values, color = data)) + \n  #geom_point(position = position_dodge(width = 0.2)) +\n  geom_boxplot(position = position_dodge(width = 1)) +\n  xlab(\"Total sample size (n)\") + \n  ylab(\"\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  facet_wrap(~ statistic, ncol = 2, scales = \"free\")\n\n\n\n\nSo based on these box plots, it seems like most of the sampling distributions for these statistics – point estimate, standard errors, CI width, etc. – are pretty comparable but not identical."
  },
  {
    "objectID": "posts/hello-world/index.html",
    "href": "posts/hello-world/index.html",
    "title": "Hello, World!",
    "section": "",
    "text": "This first post is to check whether the features I want for this blog work as desired.\nThese can be summarized in a non-exhaustive list as:"
  },
  {
    "objectID": "posts/hello-world/index.html#math",
    "href": "posts/hello-world/index.html#math",
    "title": "Hello, World!",
    "section": "Math",
    "text": "Math\nThe OLS estimator is given by the equation \\(\\hat\\beta_\\text{OLS} = (X^\\mathsf{T} X)^{-1} X^\\mathsf{T} y\\).\nOn the other hand, the ridge estimator is given by the following formula\n\\[\\hat\\beta_\\text{ridge} = (X^\\mathsf{T} X + \\lambda I)^{-1} X^\\mathsf{T} y\\]\nwhere \\(\\lambda \\in [0, \\infty)\\) controls the amount of shrinkage applied to the coefficients."
  },
  {
    "objectID": "posts/hello-world/index.html#r-code",
    "href": "posts/hello-world/index.html#r-code",
    "title": "Hello, World!",
    "section": "R code",
    "text": "R code\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\np &lt;- iris %&gt;% \n  ggplot(aes(x = Petal.Length, y = Petal.Width, color = Species)) + \n  geom_point() + \n  theme_classic()\n\nvanilla ggplot\n\nplot(p)\n\n\n\n\nplotly plot\n\nplotly::ggplotly(p)"
  },
  {
    "objectID": "posts/how-many-ratings-do-i-need/index.html",
    "href": "posts/how-many-ratings-do-i-need/index.html",
    "title": "How Many Five-Star Ratings Do I Need?",
    "section": "",
    "text": "In the Fall of 2021, a friend who runs her own business sent me this picture with the accompanying question:\n\nThe screenshot indicates an average1 rating of 4.5 stars out of a total of \\(n = 363\\) reviews, and additionally lists the percent of time a rating between 1-5 was given.1 Unweighted, I assume.\nIt seemed like an easy enough problem – perfect to explore on a rainy, gray Saturday in November – so I decided to have a crack at it.\nAfter doing some trivial algebra, somewhat successfully writing a for-loop, and adequately pleased with the solution, I posted the write-up for my initial approach on RPubs and sent off the answer2 to my friend.2 Spoiler: she needed between 47-50 5-star ratings to pull up the average rating to 4.6. Apologies if you were eagerly waiting for the end to find out what the answer was.\nHowever, while going through this document to clean it up for the blog (in 2022), I realized there’s a simpler way of solving this problem. Before I describe it further, I’m going to load some R packages and extract the data from the image into R objects.\n\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nn &lt;- 363\n\n# percent ratings scaled to [0,1]\np &lt;- c(0.06, 0.01, 0.02, 0.12, 0.79) %&gt;%\n  set_names(nm = 1:5) %&gt;% \n  print()\n\n   1    2    3    4    5 \n0.06 0.01 0.02 0.12 0.79 \n\n\nWhat made me rethink my approach was that I previously ended up with the wrong ratings vector after converting the (rounded) percentages into counts3.3 Although I mostly worked around it afterwards via simulation.\nSo for example, 79% out of 363 ratings were 5-star ratings, which translates to 286.77 and rounded to the nearest integer becomes 287. But this isn’t the only integer that rounds to 79% when divided by 363. Any integer \\(k\\) when divided by 363 that ends up in the (open) interval (78.5%, 79.5%) would be a possible candidate.\n\nseq(284, 289, 1) %&gt;% \n  set_names(nm = ~ .x) %&gt;% \n  map_dbl(.f = ~ round(100 * .x / 363))\n\n284 285 286 287 288 289 \n 78  79  79  79  79  80 \n\n\nSo any one of 285-288 5-star ratings are compatible with the information in the screenshot. We can get the same range for the other ratings (i.e., 1-4).\n\nplausible_counts_per_rating &lt;- p %&gt;% map(.f = function(prop) {\n  approx_val &lt;- round(prop * n)\n  possible_vals &lt;- seq(from = approx_val - 10, to = approx_val + 10, by = 1) %&gt;% \n    set_names(nm = ~ .x) %&gt;% \n    map_dbl(.f = ~ round(.x / n, 2)) %&gt;% \n    keep(.p = ~ .x == prop) %&gt;% \n    names()\n}) %&gt;% \n  as_tibble(.name_repair = ~ paste('x', .x, sep = \"\"))\n\nplausible_counts_per_rating\n\n# A tibble: 4 × 5\n  x1    x2    x3    x4    x5   \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 20    2     6     42    285  \n2 21    3     7     43    286  \n3 22    4     8     44    287  \n4 23    5     9     45    288  \n\n\nEach column shows the plausible values for the number of times a rating was provided. We can create all possible combinations of these values to identify the subset of \\(4 ^ 5 = 1024\\) possible combinations that are compatible with the information in the screenshot, i.e., a mean of 4.5 when rounded to 1 decimal place and a total of 363 ratings.\n\nrating_combinations &lt;- plausible_counts_per_rating %&gt;%\n  # create all combinations of all values in all columns\n  expand(crossing(x1, x2, x3, x4, x5)) %&gt;%\n  mutate(across(.fns = as.integer), \n         total_ratings = x1 + x2 + x3 + x4 + x5, \n         sum_ratings = x1 + (2 * x2) + (3 * x3) + (4 * x4) + (5 * x5), \n         mean_rating = round(sum_ratings / 363, 1)) %&gt;% \n  print(n = 10)\n\n# A tibble: 1,024 × 8\n      x1    x2    x3    x4    x5 total_ratings sum_ratings mean_rating\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;         &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1    20     2     6    42   285           355        1635         4.5\n 2    20     2     6    42   286           356        1640         4.5\n 3    20     2     6    42   287           357        1645         4.5\n 4    20     2     6    42   288           358        1650         4.5\n 5    20     2     6    43   285           356        1639         4.5\n 6    20     2     6    43   286           357        1644         4.5\n 7    20     2     6    43   287           358        1649         4.5\n 8    20     2     6    43   288           359        1654         4.6\n 9    20     2     6    44   285           357        1643         4.5\n10    20     2     6    44   286           358        1648         4.5\n# … with 1,014 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\npossible_ratings &lt;- rating_combinations %&gt;% \n  filter(total_ratings == 363, mean_rating == 4.5) %&gt;% \n  print(n = Inf)\n\n# A tibble: 3 × 8\n     x1    x2    x3    x4    x5 total_ratings sum_ratings mean_rating\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;         &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1    23     4     9    42   285           363        1651         4.5\n2    23     5     7    43   285           363        1651         4.5\n3    23     5     8    42   285           363        1650         4.5\n\n\nSo one of these three possible vectors is used to produce the statistics shown in the screenshot.\nThe formula for computing the (arithmetic) mean can be rearranged to easily calculate the required number of five-star ratings to bring the mean from 4.5 to 4.6.\nLet \\(n_1\\) denote the number of additional five-star ratings, \\(y\\) the (weighted) sum of the rating counts, and \\(n\\) the current number of ratings (i.e., 363) in the following equation:\n\\[\\frac{y + (n_1 \\times 5)}{n + n_1} = 4.6\\]\nThis can be rewritten as\n\\[5n_1 = 4.6 \\times (n + n_1) - y\\]\nand simplified to yield\n\\[n_1 = \\frac{(4.6 \\times n) - y}{0.4}\\]\nand coded up as an R function\n\nnum_five_star &lt;- function(sum_ratings, total_ratings) {\n  ((4.6 * total_ratings) - sum_ratings) / 0.4\n}\n\nApplying this function to the possible ratings vector leads to\n\npossible_ratings %&gt;% \n  mutate(extra_5_stars = ceiling(num_five_star(sum_ratings, total_ratings))) %&gt;% \n  select(-total_ratings, -mean_rating)\n\n# A tibble: 3 × 7\n     x1    x2    x3    x4    x5 sum_ratings extra_5_stars\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n1    23     4     9    42   285        1651            47\n2    23     5     7    43   285        1651            47\n3    23     5     8    42   285        1650            50\n\n\nso 47-50 additional 5-star ratings are needed to pull up the average4 to 4.6, assuming that the future ratings are all five-star ratings.4 exact average, not an average resulting from rounding 4.57 (say) to 4.6. In the latter case, fewer than 47 five-star ratings would be required."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html",
    "href": "posts/instrumental-variable-in-RCT/index.html",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "",
    "text": "This (longer-than-expected) post compares intention-to-treat, per-protocol, as-treated, and instrumental variable analyses on a simulated dataset. Along the way, it goes off on fun tangents like 1) comparing results from different estimators for risk difference (a.k.a. uplift), and 2) comparing the bootstrap distribution with the Bayesian posterior distribution and the normal approximation for the risk difference. Finally, it uses the estimated probabilities to get the posterior predictive distribution for the total profit under different scenarios. This was written largely for me to learn about IV methods in order to deal with the problem of estimating the effect of a treatment under non-compliance in randomized experiments."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#introduction",
    "href": "posts/instrumental-variable-in-RCT/index.html#introduction",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "Introduction",
    "text": "Introduction\nOne of the first problems I ever cut my teeth on as a data scientist many years ago was to analyze data from a marketing / advertising campaign to assess the impact of an advertisement (ad) on sales of a certain product.\nIt was a simple experimental dataset from a two-arm1 randomized controlled trial. The most challenging part of that analysis was how to analyze individuals – the so-called non-compliers2 – who were assigned to the treatment group eligible to view an ad, but who did not actually end up seeing a single ad.1 i.e., an experiment with two groups – treatment and control2 in the usual terminology used in the instrumental variable methods literature\nAll details below are of a simplified version of the problem similar to the one I worked on, and have no resemblance to the real constraints / numbers / settings from the actual problem. This exact same problem shows up in pretty much every field (marketing, advertising, pharma, tech, psychology, economics, etc.) where experiments are regularly conducted, and the analyses in this post are applicable to those situations as well."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#problem-description",
    "href": "posts/instrumental-variable-in-RCT/index.html#problem-description",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "Problem description",
    "text": "Problem description\nLet’s say a company has been working on updating an existing version of a product, and wants to know whether they should invest in an advertising campaign on a specific channel (say YouTube / Instagram ads) to promote the new product. Since buying ads cost money, the campaign would have to be profitable enough in terms of increased sales to justify the ad spend3.3 The advertising company, on the other hand, is more interested in assessing the effectiveness of advertising, so would be happier to see a strong positive effect of advertising.\nThe main interest is in (functions of) two quantities – \\(\\text{Pr}[Y^{a=0} = 1]\\), which is the proportion of sales in a population not shown the ad; and \\(\\text{Pr}[Y^{a=1} = 1]\\), which is the proportion of sales in a population which is shown the ad. These can be used to calculate the risk difference / uplift (\\(\\text{Pr}[Y^{a=1} = 1] - \\text{Pr}[Y^{a=0} = 1]\\)), the risk ratio / relative risk / lift (\\(\\text{Pr}[Y^{a=1} = 1] / \\text{Pr}[Y^{a=0} = 1]\\)), or any other quantity listed in the tables here. The risk difference can then be used to calculate the additional profit from showing the ad compared to doing nothing.\nThe cleanest way of estimating these probabilities is through an experiment. First, a target population – say, users of a given product – is identified, and then, this group is randomly split into a treatment and a control group. Individuals assigned to the treatment group are eligible to see an ad for the newer, updated version of the product, and the control group is ineligible to view the ad. The campaign runs for a two week period, where individuals in the treatment group see an ad multiple times. The outcome for this campaign is sales of the newer product in each of the two groups in the six-week period from the start of the campaign.\nBefore writing any code, the tidyverse packages is loaded.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nA DAG, or a directed acyclic graph for this problem can be visualized as follows\n\n\nCode\ndag &lt;- ggdag::dagify(\n  A ~ Z + U, \n  Y ~ A + U, \n  exposure = \"A\", outcome = \"Y\", \n  coords = list(x = c(Z = 1, A = 2, U = 2.5, Y = 3), \n                y = c(Z = 1, A = 1, U = 1.2, Y = 1))\n) %&gt;% \n  ggdag::ggdag() +\n  ggdag::theme_dag()\n\nplot(dag)\n\n\n\n\n\nwhere \\(Y\\) is a binary outcome (bought the newer version of the product or not during the follow-up period), \\(A\\) is a binary treatment variable (saw the ad or not), \\(Z\\) is a binary variable indicating group assignment (i.e., randomly assigned to the treatment or control group), and U is the set of (unmeasured) confounder(s), i.e., one or more variables that both impact whether an individual ends up seeing the ad, and whether they end up buying the newer product or not.\nThis DAG encodes several assumptions:\n\nGroup assignment \\(Z\\) only affects whether someone sees an ad or not (variable \\(A\\))\nThere are some unmeasured common causes (confounders) \\(U\\) that make our life difficult by potentially distorting the relationship between \\(A\\) and \\(Y\\)\n\\(Z\\) has no direct effect on Y; only an indirect effect entirely (mediated) via \\(A\\)\n\\(Z\\) has no relationship with \\(U\\), since \\(Z\\) is randomly assigned and should be balanced with respect to \\(U\\) (unless we’re unlucky and there’s some imbalance by chance)\n\nAfter the campaign is over, it is observed that a subset of the treatment group never received the treatment, i.e., didn’t see the ad at all. The main question for the analysis is then – how should these non-compliers be analyzed? In this post I look at three possible choices:\n\nShould they be analysed as part of the treatment group, despite not receiving the treatment?\nShould they be excluded from the analysis altogether?\nShould they be included in the control group since they were untreated?"
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#simulate-data",
    "href": "posts/instrumental-variable-in-RCT/index.html#simulate-data",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "Simulate data",
    "text": "Simulate data\nBefore the question in the previous section can be answered, we need to generate hypothetical data4 for this fictional ad campaign.4 Inspired by a comment on this post, and the Imbens, Rubin 1997 paper linked therein.\nWhat the following code does is it first partitions the population of 100,000 individuals into an 80-20% mix of compliers and non-compliers. Compliers are the individuals that would watch the ad if they were assigned to the target group. Non-compliers are the individuals who would not comply with the group they’re randomized to. More on this in the IV section below. Then, the potential outcomes for each individual under the two treatments depending on their compliance status are simulated.\nIf \\(C_i = 1\\) denotes whether individual \\(i\\) is a complier, and \\(C_i = 0\\) otherwise, then \\(\\text{Pr}[Y^{a = 0}_i = 1| C_i = 0] = \\text{Pr}[Y^{a = 1}_i = 1| C_i = 0] = 0.1\\%\\), where the probability of purchasing the new product is very small among the non-compliers independent of whether they’re assigned to the treatment or the control group.\nAmong the compliers, \\(\\text{Pr}[Y^{a = 0}_i = 1| C_i = 1] = 1\\%\\), i.e., 1% of the individuals would buy the new product if nobody was shown the ad. If the ad is rolled out to the full population, then \\(\\text{Pr}[Y^{a = 1}_i = 1| C_i = 1] = 11\\%\\), which leads to an average treatment effect (ATE) among the compliers of +10 percentage points.\nThese individuals are randomly assigned to the treatment (70%) or control (30%) group. Their actual treatment status (i.e., treated or untreated) is a product of the group they’re assigned to and their compliance. Their realized outcome is the outcome corresponding to the group they’re assigned to.\n\n\nCode\nsimulate_data &lt;- function(n = 100000L,\n                          pY0 = 0.01,\n                          risk_diff = 0.1, \n                          seed = 23, \n                          p_compliers = 0.8, \n                          p_treatment = 0.7,\n                          pY_non_compliers_factor = 0.1) {\n  \n  pY1 &lt;- pY0 + risk_diff\n  pY_nc &lt;- pY_non_compliers_factor * pY0\n  \n  set.seed(seed)\n  data &lt;- tibble(\n    id = 1:n,\n    # the underlying population can be stratified into \n    # never-takers and compliers\n    complier = rbinom(n, 1, prob = p_compliers), \n    # generate individual potential outcomes\n    # under control and treatment, i.e., Pr[Y^0 = 1]\n    Y0 = case_when(\n      complier == 0 ~ rbinom(n, 1, pY_nc),\n      complier == 1 ~ rbinom(n, 1, pY0),\n    ),\n    # assuming a constant effect of +10 percentage points\n    # among the compliers, and no average effect under the never-takers\n    Y1 = case_when(\n      complier == 0 ~ rbinom(n, 1, pY_nc),\n      complier == 1 ~ rbinom(n, 1, pY1)\n    ), \n    # treatment assigned at random\n    # 70-30 split into treatment / control\n    Z = rbinom(n, 1, prob = p_treatment),\n    # treatment uptake depends on \n    # being assigned to treatment (Z = 1)\n    # AND being a complier (C = 1)\n    A = Z * complier,\n    # generate observed response using the \n    # consistency equation\n    Y = (1 - Z) * Y0 + Z * Y1\n  )\n\n  return(data)\n}\n\n# creating these variables as they'll be helpful later on\n# population size\nn &lt;- 100000L\n\n# P[Y^0 = 1]\npY0 &lt;- 0.01\n# P[Y^1 = 1], ATE of +10 pct. pts.\npY1 &lt;- pY0 + 0.1\n\ndata &lt;- simulate_data(n = n, pY0 = pY0, risk_diff = 0.1)\n\nglimpse(data)\n\n\nRows: 100,000\nColumns: 7\n$ id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ complier &lt;int&gt; 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1…\n$ Y0       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Y1       &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ Z        &lt;int&gt; 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1…\n$ A        &lt;int&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1…\n$ Y        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n\n\nIn this simulated dataset, we’ve got information on compliance and the potential outcome under each treatment at the individual level. However, from a real experiment, only the \\(Z, A, Y\\) columns would be observed.\nThe exposition pipe %$% operator – similar to the pipe %&gt;% operator – is exported from the magrittr package and used with base::table() to expose the variables in the data frame to the table function to produce contingency tables.\n\nlibrary(magrittr, include.only = \"%$%\")\n\ndata %$% \n  table(complier, Z) %&gt;% \n  prop.table(margin = 2)\n\n        Z\ncomplier         0         1\n       0 0.1974162 0.2003373\n       1 0.8025838 0.7996627\n\n\nSince the treatment \\(Z\\) is randomly assigned, the proportion of compliers in each group is nearly the same as expected.\nThere are several effects that can be estimated for this problem each with its own advantages and disadvantages."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#intention-to-treat-itt",
    "href": "posts/instrumental-variable-in-RCT/index.html#intention-to-treat-itt",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "Intention-to-treat (ITT)",
    "text": "Intention-to-treat (ITT)\nThe intention-to-treat (ITT) effect is the effect of being assigned to the treatment instead of the effect of the treatment itself. These are identical when treatment compliance / adherence is perfect, i.e, when all the individuals only take the treatment they are assigned to, but not otherwise.\nFor this problem, the ITT analysis would analyse individuals based on the group they were assigned to, and not the treatment they ultimately received. Those in the treatment group who didn’t see a single ad would be analysed as part of the treatment group rather than being excluded from the analysis, or being analysed as part of the control group.\nAn advantage of an ITT analysis is that randomization preserves the balance of confounders in both the treatment and control groups, so the \\(Z-Y\\) association remains unconfounded and a valid, albeit conservative5 effect of assigning the treatment at the population level. However, this validity would be affected6 if this ad is rolled out to another target population – a different period in time, or another geographical location – with a different proportion of (non-)compliers7.5 Section 22.1 of the what-if book mentions that while the ITT estimate is usually conservative, it’s not guaranteed to be attenuated compared to the per-protocol effect (described in the next section) for 1) non-inferiority trials, or 2) if the per-protocol effect is not monotonic for all individuals and non-compliance is high. It also lists other counterarguments (e.g., lack of transportability) against the ITT effect.6 Rerunning the data simulation chunk above with simulate_data(p_compliers = 0.4) %&gt;% mutate(diff = Y1 - Y0) %&gt;% pull(diff) %&gt;% mean() instead of 0.8 leads to an ITT estimate of +4 instead of +8 percentage points, even though the effect of the treatment itself is still +10 percentage points.7 For a garden-variety ad campaign, the proportion of compliers could be effectively random, unless the ad in question is visually appealing, or contentious. On the contrary, for something like a vaccine in a global pandemic, compliance could vary wildly between the trial and rollout at the population level. Source: reading internet discussions between 2021-2022.\n\nEstimators / models\nSince both \\(Y\\) and \\(Z\\) are binary variables, an estimate of the ITT effect can be obtained by fitting a simple logistic regression model, and using the marginaleffects package to perform g-computation / (marginal / model-based) standardization to get the risk difference (and the associated CIs)\n\ndata %&gt;% \n  glm(Y ~ Z, family = binomial(link = \"logit\"), data = .) %&gt;%\n  marginaleffects::avg_comparisons(variables = \"Z\")\n\n\n Term Contrast Estimate Std. Error  z Pr(&gt;|z|)   S  2.5 % 97.5 %\n    Z    1 - 0   0.0807     0.0012 67   &lt;0.001 Inf 0.0783  0.083\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n\nor by fitting an identity-link logistic regression where the estimated coefficient for \\(Z\\) can be directly interpreted as the risk difference\n\ndata %&gt;% \n  glm(Y ~ Z, family = binomial(link = \"identity\"), data = .) %&gt;% \n  broom::tidy(conf.int = TRUE) %&gt;% \n  mutate(across(.cols = where(is.numeric), .fns = ~ round(.x, 4)))\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   0.0087    0.0005      16.2       0   0.0077    0.0098\n2 Z             0.0807    0.0012      67.0       0   0.0783    0.083 \n\n\nThe use of glm + g-computation here is a bit overkill, as the same estimate can be obtained by looking at the 2x2 table of proportions scaled to sum to 1 within each column\n\ndata %$%\n  table(Y, Z) %&gt;% \n  print() %&gt;% \n  prop.table(margin = 2)\n\n   Z\nY       0     1\n  0 29772 63715\n  1   261  6252\n\n\n   Z\nY             0           1\n  0 0.991309559 0.910643589\n  1 0.008690441 0.089356411\n\n\nand taking the difference of \\(P[Y = 1 | Z]\\) in the two groups to give the risk difference \\(\\hat{p}_1 - \\hat{p}_0\\), where \\(\\hat{p}_0 = \\text{Pr}[Y = 1 | Z = 0]\\) and \\(\\hat{p}_1 = \\text{Pr}[Y = 1 | Z = 1]\\)\n\nround(0.089356411 - 0.008690441, 4)\n\n[1] 0.0807\n\n\nAdditionally, a wald-type CI can be obtained using the formula for variance mentioned (using counts) here or (using proportions) here\n\\[\nSE(\\hat{p}_1 - \\hat{p}_0) = \\sqrt{\\frac{\\hat{p}_0 (1 - \\hat{p}_0)}{n_0} + \\frac{\\hat{p}_1 (1 - \\hat{p}_1)}{n_1}}\n\\]\nwhich results in the same standard error\n\n\nCode\nvar_p0 &lt;- (0.008690441 * (1 - 0.008690441)) / (261 + 29772)\nvar_p1 &lt;- (0.089356411 * (1 - 0.089356411)) / (6252 + 63715)\n\nround(sqrt(sum(var_p0, var_p1)), digits = 4)\n\n\n[1] 0.0012\n\n\nThis paper mentions other ways of estimating the risk difference from different models, e.g. linear regression + sandwich estimator for the standard errors, which are again identical to the ones from the other methods above\n\nlm_mod &lt;- lm(Y ~ Z, data = data) \n\nlm_mod %&gt;% \n  broom::tidy() %&gt;% \n  select(term, estimate, model_SE = std.error) %&gt;% \n  mutate(robust_SE = sqrt(diag(sandwich::vcovHC(lm_mod))))\n\n# A tibble: 2 × 4\n  term        estimate model_SE robust_SE\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  0.00869  0.00141  0.000536\n2 Z            0.0807   0.00168  0.00120 \n\n\nHowever, getting a distribution of this effect as opposed to just a 95% CI can be more informative from a decision making point of view. This distribution can be obtained three ways – 1) plugging the estimate and its standard error into a normal distribution and simulating effect sizes from that; 2) using the (nonparametric) bootstrap to resample the data and getting the bootstrap distribution of effect sizes; 3) using a Bayesian model to get the posterior distributions of the parameters from the model.\nFor the rest of the analyses, I pick option 3, where the simple (Bayesian) Beta-Binomial model is fit to this data separately within the treatment and the control groups. Taking the difference of these two (Beta) posterior distributions produces the posterior distribution of the risk difference. The bayesAB package in R can quickly fit Beta-Binomial models, or the brms package can be used for more general models.\n\n\nBack to ITT analysis\nA \\(\\text{Beta}(\\alpha = 1, \\beta = 9)\\) prior is specified for the probability of success in each arm reflecting the knowledge that the we can expect the conversion in each group to be somewhere between 0%-70%. Normally this depends on the product in question – expensive and / or infrequently bought products are not expected to lead to very large conversion rates.\nThe following shows a summary of 10,000 draws from the prior distribution for the parameters\n\nsummary(rbeta(1e5, 1, 9))\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0000017 0.0313705 0.0740948 0.1002186 0.1430362 0.7405391 \n\n\nwhich implies the following prior distribution for the risk difference\n\nsummary(rbeta(1e5, 1, 9) - rbeta(1e5, 1, 9))\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.6858270 -0.0709874 -0.0000165  0.0000020  0.0699090  0.6885754 \n\n\nA priori, we expect the difference to be pretty small with mean of close to 0, but allowing for large differences of +/- 60-70%. We can also visualize these distributions\n\n\nCode\nbind_rows(\n  tibble(x = rbeta(1e5, 1, 9), cat = \"Prior conversion probability\"), \n  tibble(x = rbeta(1e5, 1, 9) - rbeta(1e5, 1, 9), \n         cat = \"Prior risk difference\")\n) %&gt;% \n  ggplot(aes(x = x, group = cat, color = cat)) + \n  geom_density() + \n  theme_classic() + \n  xlab(\"\") + \n  scale_x_continuous(labels = scales::percent) +\n  facet_wrap(~ cat, scales = \"free\") + \n  guides(color = \"none\")\n\n\n\n\n\nTo get the posterior distribution for conversion in each arm, the counts from the 2x2 contingency table can be plugged into the formula for Beta posterior distribution \\(\\text{Beta}(y + \\alpha, n - y + \\beta)\\), where \\(y\\) corresponds to the count for \\(Y = 1\\), and \\(n-y\\) for \\(Y = 0\\).\n\ndata %$% table(Y, Z)\n\n   Z\nY       0     1\n  0 29772 63715\n  1   261  6252\n\n\nThis gives \\(p (\\alpha_0, \\beta_0|y) \\sim \\text{Beta}(261 + 1, 29772 + 9) = \\text{Beta}(262, 29781)\\) for the control group, and \\(p (\\alpha_1, \\beta_1|y) \\sim \\text{Beta}(6252 + 1, 63715 + 9) = \\text{Beta}(6253, 63724)\\) for the treatment group, visualized as follows\n\n\nCode\nposterior_categories &lt;- c(\n  \"Conversion probability \\n(control group)\", \n  \"Conversion probability \\n(treatment group)\",\n  \"Risk difference\"\n)\n\nsample_from_beta_posterior &lt;- function(theta_0, theta_1, \n                                       seed = 23, n_samples = 1e5) {\n  set.seed(seed)\n  posterior_control &lt;- rbeta(n_samples, theta_0[1], theta_0[2])\n  posterior_treatment &lt;- rbeta(n_samples, theta_1[1], theta_1[2])\n  risk_difference &lt;- posterior_treatment - posterior_control\n  \n  tibble(\n    id = rep(1:n_samples, times = 3),\n    cat = rep(posterior_categories, each = n_samples),\n    x = c(posterior_control, posterior_treatment, risk_difference)\n  )\n}\n\ntrue_effect &lt;- tibble(\n  x = c(NA_real_, NA_real_, pY1 - pY0), \n  cat = posterior_categories\n)\n\nplot_posteriors &lt;- function(posterior, effect = true_effect) {\n  posterior %&gt;% \n    ggplot(aes(x = x, group = cat, color = cat)) + \n    geom_density(trim = TRUE) + \n    geom_vline(data = effect, \n               aes(xintercept = x), \n               color = \"gold3\", \n               linetype = \"dashed\",\n               linewidth = 1.3,\n               na.rm = TRUE) + \n    theme_classic() + \n    theme(plot.margin = margin(0.5, 0.5, 0.5, 0.5, \"cm\")) + \n    xlab(\"Posterior distribution\") + \n    scale_x_continuous(labels = scales::label_percent(accuracy = 0.1)) +\n    facet_wrap(~ cat, scales = \"free\", ncol = 3) + \n    guides(color = \"none\")\n}\n\nITT_posterior &lt;- sample_from_beta_posterior(\n  theta_0 = c(262, 29781), theta_1 = c(6253, 63724)\n)\n\nplot_posteriors(ITT_posterior)\n\n\n\n\n\nThe posterior for the risk difference completely fails to capture the underlying true effect size of +10 percentage points. However, since the dataset is simulated with the potential outcomes \\(\\{Y_{i}^{z = 0},\\ Y_{i}^{z = 1}\\}\\) at the individual level, we can see that the estimate of +8 percentage points is correct for this population.\n\ntrue_ITT_effect &lt;- data %&gt;% \n  mutate(diff = Y1 - Y0) %&gt;% \n  pull(diff) %&gt;% \n  mean() %&gt;% \n  print()\n\n[1] 0.08069\n\n\nThe posterior distribution is summarized via the posterior mean, mode, and 95% (equal-tailed) credible intervals, which are nearly identical to the ones above.\n\n\nCode\nITT_risk_difference &lt;- ITT_posterior %&gt;% \n  filter(cat == \"Risk difference\") \n\n# get the posterior density to find the mode\n# based on this SE answer: \n# https://stackoverflow.com/a/13874750\nITT_risk_difference_density &lt;- density(ITT_risk_difference$x)\nITT_risk_difference_mode &lt;- ITT_risk_difference_density$x[which.max(ITT_risk_difference_density$y)]\n\nITT_risk_difference %&gt;% \n  summarize(\n    mean = mean(x), \n    lower = quantile(x, probs = 0.025),\n    upper = quantile(x, probs = 0.975)\n  ) %&gt;% \n  mutate(mode = ITT_risk_difference_mode, .after = mean)\n\n\n# A tibble: 1 × 4\n    mean   mode  lower  upper\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 0.0806 0.0808 0.0783 0.0830\n\n\nJust for fun, the Bayesian posterior distribution is visually compared to the normal approximation and the bootstrap distribution for the risk difference\n\n\nCode\nboot_ITT_fun &lt;- function(data, indx, ...) {\n  data &lt;- data[indx, ]\n  data %&gt;% \n    count(Z, Y) %&gt;% \n    # get P[Y = 1] within levels of Z\n    group_by(Z) %&gt;% \n    mutate(p = n / sum(n)) %&gt;% \n    ungroup() %&gt;% \n    # only keep P[Y = 1 | Z]\n    filter(Y == 1) %&gt;% \n    pull(p) %&gt;% \n    # get |p1 - p0|\n    reduce(.f = `-`) %&gt;% \n    abs()\n} \n\nITT_boot &lt;- boot::boot(data = data, \n                       statistic = boot_ITT_fun, \n                       R = 999, \n                       # multicore only works on linux\n                       parallel = \"multicore\")\n\n# plot the three distributions\nset.seed(23)\nbind_rows(\n  tibble(x = rnorm(999, 0.0807, 0.0012), y = \"Normal approximation\"), \n  tibble(x = ITT_boot$t[, 1], y = \"Bootstrap distribution\"), \n  ITT_posterior %&gt;% \n    filter(cat == \"Risk difference\") %&gt;% \n    rename(y = cat) %&gt;% \n    slice_sample(n = 999) %&gt;% \n    mutate(y = \"Beta posterior\")\n) %&gt;% \n  ggplot(aes(x = x, y = y, group = y, fill = y)) + \n  ggdist::stat_halfeye() +\n  theme_classic() + \n  theme(legend.position = \"none\", \n        plot.margin = margin(0.5, 0.5, 0.5, 0.5, \"cm\")) + \n  xlab(\"Distribution of risk difference / uplift (999 samples)\") + \n  ylab(\"\") + \n  scale_x_continuous(labels = scales::label_percent(accuracy = 0.1))\n\n\n\n\n\nAs expected, despite being philosophically different, all of these methods give nearly identical results (increasing the samples from 999 to 100,0008 would lead to them being virtually identical), but all of these underestimate the correct treatment effect of +10 percentage points.8 Using the boot package here complains about memory issues when trying to set R = 100,000, and I’m too lazy to write a (parallel) map here myself."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#per-protocol-pp",
    "href": "posts/instrumental-variable-in-RCT/index.html#per-protocol-pp",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "Per-protocol (PP)",
    "text": "Per-protocol (PP)\nThe per-protocol (PP) effect is generated by including all the people who adhere to the protocol, i.e., those with treatment status \\(A\\) identical to the assignment treatment \\(Z\\). Contrary to the ITT effect from the previous section, this effect is a measure of the effectiveness of the actual treatment itself, although this effect estimate is prone to bias.\nSince it’s not possible for us to measure9 an individual seeing an ad if they were assigned to the control group, i.e, individuals with \\(Z = 0, A = 1\\), this would only exclude all the individuals with \\(Z = 1\\) and \\(A = 0\\), i.e., about 14,000 individuals from the treatment group who didn’t see an ad.9 An individual in the control group cannot see an ad, if the ad is not shown to the individual.\n\ndata %$% table(A, Z)\n\n   Z\nA       0     1\n  0 30033 14017\n  1     0 55950\n\n\n\ndata %&gt;% filter(A == Z) %$% table(A, Z)\n\n   Z\nA       0     1\n  0 30033     0\n  1     0 55950\n\n\n\ndata %&gt;% \n  # only restrict analysis to those with perfect adherence / compliance\n  filter(A == Z) %$% \n  table(Y, A) %&gt;% \n  print() %&gt;% \n  sum()\n\n   A\nY       0     1\n  0 29772 49716\n  1   261  6234\n\n\n[1] 85983\n\n\nSImilar to the previous section, the posterior probability of success / conversion and the risk difference for the PP analysis can be obtained by using the same beta-binomial model from the previous section and visually assessed\n\nPP_posterior &lt;- sample_from_beta_posterior(\n  theta_0 = c(261 + 1, 29772 + 9), \n  theta_1 = c(6234 + 1, 49716 + 9)\n)\n\nplot_posteriors(PP_posterior)\n\n\n\n\nThe PP effect is exaggerated – albeit by a relatively small margin – compared to both the ITT effect and the true treatment effect of +10 percentage points."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#as-treated-at",
    "href": "posts/instrumental-variable-in-RCT/index.html#as-treated-at",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "As-treated (AT)",
    "text": "As-treated (AT)\nThe as-treated effect (AT) estimates the effect of analyzing individuals by treatment received. This analysis includes individuals in the group corresponding to the treatment actually received. So the individuals randomized to the treatment group who did not see an ad are included in the control group.\n\ndata %$% table(Y, A)\n\n   A\nY       0     1\n  0 43771 49716\n  1   279  6234\n\n\n\nAT_posterior &lt;- sample_from_beta_posterior(\n  theta_0 = c(279 + 1, 43771 + 9), \n  theta_1 = c(6234 + 1, 49716 + 9)\n)\n\nplot_posteriors(AT_posterior)\n\n\n\n\nLike the PP effect, this effect is a measure of the effectiveness of the actual treatment itself. By including the untreated from the treatment group – who had a lower probability of conversion – into the control group, the overall conversion probability in the control group is attenuated thereby leading to an exaggeration of the ATE. Thus, it overestimates the true effect and is likewise exaggerated compared to the ITT effect estimate. It is similarly prone to bias.\nWe’ve produced three estimates so far, and since we know the true effect of treatment here, we can see that they’re all biased. Great! The analysis could stop here and all the three estimates could be presented with their respective caveats.\nBut can we do better (in this experiment)?"
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#instrumental-variable-analysis-iv",
    "href": "posts/instrumental-variable-in-RCT/index.html#instrumental-variable-analysis-iv",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "Instrumental variable analysis (IV)",
    "text": "Instrumental variable analysis (IV)\nThe fourth and final method of analysis looks at the method of instrumental variables. Causal estimates for the treatment effect can be obtained by using just the three variables \\(\\{Z, A, Y\\}\\) if the assumptions behind this method are plausible for our (fictional) ad campaign.\nFor an instrumental variable analysis to be valid, we need an instrument (say \\(Z\\)) that needs to satisfy the first three conditions and at least the fourth or the fifth condition\n\nRelevance condition – Be correlated (hopefully strongly) with the treatment variable (in our case \\(A\\))\nIgnorability - Be uncorrelated with all measured and unmeasured confounders (\\(U\\)) of the \\(A \\rightarrow Y\\) relationship and share no common causes with \\(Y\\)\nExclusion restriction – have no direct impact on the outcome \\(Y\\), only an indirect effect entirely mediated via the treatment \\(A\\)\n(Weak) Homogeneity – for a binary \\(Z-A\\) system, if \\(Z\\) is not an effect modifier of the \\(A-Y\\) relationship, i.e.,\n\\(\\mathbb{E}[Y^{a = 1} - Y^{a = 0} | Z = 1, A = a] = \\mathbb{E}[Y^{a = 1} - Y^{a = 0} | Z = 0, A = a]\\) then the IV estimate can be interpreted as the average treatment effect among the treated (ATT) in the population. If the ATT is identical to the ATU(ntreated), then the IV estimate can be interpreted as the ATE in the population. Other homogeneity conditions are described in section 16.3 of the what-if book. Appendix 2.1 of the Hernán, Robins 2006 paper has some stuff on this too.\nMonotonicity - The trial population can be partitioned into four-latent subgroups / prinicipal strata defined by a combination of \\(A, Z\\) - always takers, compliers, never-takers, and defiers. Always (or never) takers are people who would always (or never) take the treatment independent of group assignment. Compliers would comply with the treatment corresponding to the group they are assigned to. Defiers would do the opposite to the group they’re assigned to. In the absence of defiers and always takers, the IV estimand is the complier average causal effect (CACE) or the local average treatment effect (LATE). This can be written as\n\\[\n\\mathbb{E}[Y^{a = 1} - Y^{a = 0} | A^{z = 1} \\gt A^{z = 0}] = \\frac{\\mathbb{E}[Y = 1 | Z = 1] - \\mathbb{E}[Y = 1 | Z = 0]}{\\mathbb{E}[A = 1 | Z = 1] - \\mathbb{E}[A = 1 | Z = 0]}\n\\]\n\nThe variable \\(Z\\) from the DAG for this problem\n\n\nCode\nplot(dag)\n\n\n\n\n\nseems to fulfill the first three conditions, since individuals are randomly assigned into treatment and control groups. This makes it a causal instrument, as an individual cannot choose to see an ad unless they’re assigned to the treatment group.\nRegarding homogeneity, it can be argued that the assignment of an individual10 to the group cannot be an effect modifier, since this is done by a random number generator and so should not affect the potential outcomes.10 In the presence of interference, e.g. individuals in the same household being assigned to different groups but affecting each others’ potential outcomes, the unit of assignment and analysis would be at the household level.\nSince the ad is randomized at the individual level, there can never be any defiers (i.e., individuals assigned to the control group can never deliberately switch over to the treatment group). Always takers are also not an option, since people assigned to the control group cannot choose to take the treatment11. This essentially leaves the population to be a mix of never-takers and compliers.11 Not that anyone would choose to see an ad.\n\nIV in action\nSince the instrument, treatment, and outcome are all binary, we can use the following ratio / Wald estimator for the IV estimand\n\\[\n\\text{IV} = \\frac{\\text{Pr}[Y = 1 | Z = 1] - \\text{Pr}[Y = 1 | Z = 0]}{\\text{Pr}[A = 1 | Z = 1] - \\text{Pr}[A = 1 | Z = 0]}\n\\]\nwhere the numerator is the ITT effect estimate defined a few sections above, and the denominator is a measure of compliance on the risk difference scale with regards to the assigned treatment, i.e., an association between treatment assignment \\(Z\\) and treatment uptake \\(A\\).\nDepending on whether we invoke one of the homogeneity assumptions or the monotonicity assumption, we end up with either the average causal effect in the treated, or in the population, or the average causal effect among the compliers. All the different assumptions for this are described in the Swanson et al 2018 article.\nTo fit this Beta-binomial model to the data, we can use the posterior distribution of the risk difference from the ITT effect for the numerator. For the denominator, we need to get a similar distribution of treatment compliance. Looking at the 2x2 \\(Z-A\\) table,\n\ndata %$% table(A, Z)\n\n   Z\nA       0     1\n  0 30033 14017\n  1     0 55950\n\n\nthe group of \\(\\text{Pr}[A = 1 | Z = 0]\\) is (expected to be) 0, since those assigned to the control group should not be able to see the ad. We can pick a degenerate (Beta) prior distribution12 with \\(\\alpha = 0,\\ \\beta &gt; 0\\), e.g., \\(\\text{Beta}(0, 1)\\), which results in a Beta posterior with shape \\(\\alpha = 0 + 0 = 0,\\ \\beta = 30033 + 1 = 30034\\). Since \\(\\alpha = 0\\), all draws from this posterior distribution are exactly zero.12 Since I’m not using Stan to fit the model here, I can get away with this. Using Stan, I’d pick something like a \\(\\text{Beta}(1, 10000)\\) distribution that puts a very small non-zero probability for \\(\\text{Pr}[A = 1 | Z = 0]\\). I don’t know for sure whether the degenerate prior would cause issues for Stan to be honest.\n\nsummary(rbeta(1e2, 0, 30034))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\n\nFor \\(\\text{Pr}[A = 1 | Z = 1]\\), we can expect the compliance to be reasonably high, so the \\(\\text{Beta}(7, 3)\\) with a mean compliance of 70%, and a range of 10%-100% might be a good choice. With the large sample sizes in the dataset, the prior is going to be dominated by the likelihood anyway.\n\nsummary(rbeta(1e4, 7, 3))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.09285 0.60786 0.71188 0.69917 0.80307 0.98952 \n\n\nUsing the counts from the \\(A-Z\\) table above, we get the \\(\\text{Beta}(55957, 14020)\\) posterior with \\(\\alpha = 7 + 55950\\) and \\(\\beta = 3 + 14017\\). Taking a difference of these two posteriors would leave this posterior unchanged, so we can produce the corresponding IV posterior distribution\n\n\nCode\nset.seed(23)\ncompliance_posterior &lt;- rbeta(nrow(ITT_risk_difference), 55957, 14020)\n\nIV_labels &lt;- c(\"Intention-to-treat effect\", \n               \"Proportion of compliers\", \n               \"Instrumental variable estimate\")\n\nIV_posterior &lt;- tibble(\n  id = rep(1:nrow(ITT_risk_difference), times = 3),\n  cat = factor(rep(IV_labels, each = nrow(ITT_risk_difference)), \n               levels = IV_labels),\n  x = c(ITT_risk_difference$x, compliance_posterior, \n        ITT_risk_difference$x / compliance_posterior)\n)\n\ntrue_effect_IV &lt;- true_effect %&gt;% \n  mutate(cat = factor(IV_labels, levels = IV_labels))\n\nplot_posteriors(IV_posterior, true_effect_IV)\n\n\n\n\n\n\nIV_posterior %&gt;% \n  filter(cat == IV_labels[3]) %&gt;% \n  summarize(mean = mean(x), \n            lower = quantile(x, probs = 0.025), \n            upper = quantile(x, 0.975))\n\n# A tibble: 1 × 3\n   mean  lower upper\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 0.101 0.0979 0.104\n\n\nCompared to the ITT, PP, and AT estimates, the IV estimate is the closest to the true treatment effect of +10 percentage points, which makes sense as all the IV conditions are valid here due to the use of a randomized experiment with the causal instrument \\(Z\\). However, if any of the conditions (monotonicity, relevance, etc.) were to weaken, this would lead to bias in the IV estimate.\nSome recommend producing bounds13 for the IV estimate since it’s usually not point identified. For the case of binary \\(Z, A, Y\\), we can use ivtools::ivbounds() to produce the natural bounds – which are between 8%-28% for the ATE (CRD in the result)13 Note to (future) self: look at the Balke, Pearl 1997 and Swanson et al 2018 papers, and the causaloptim R package.\n\ndata %&gt;% \n  ivtools::ivbounds(data = ., \n                    Z = \"Z\", X = \"A\", Y = \"Y\", \n                    monotonicity = TRUE) %&gt;% \n  summary()\n\n\nCall:  \nivtools::ivbounds(data = ., Z = \"Z\", X = \"A\", Y = \"Y\", monotonicity = TRUE)\n\nThe IV inequality is not violated\n\nSymbolic bounds:\n   lower upper  \np0 p10.0 1-p00.0\np1 p11.1 1-p01.1\n\nNumeric bounds:\n       lower    upper\np0   0.00869  0.00869\np1   0.08910  0.28944\nCRD  0.08041  0.28075\nCRR 10.25255 33.30515\nCOR 11.15758 46.46413\n\n\nThe width of the bounds is 20 percentage points, which matches the formula for the width14 – \\(\\text{Pr}[A = 1 | Z = 0] + \\text{Pr}[A = 0 | Z = 1]\\) – where the former is 0, since we have no defiers, and 20% for the second quantity, which is the proportion of never-takers.14 given in some of the references below\nThe slight difference between the true value and the estimate is due to sampling variability, which can be assessed by simulating multiple datasets with different seeds to give a mean ATE very close to +10 percentage points\n\n\nCode\nmap_dbl(\n  .x = 1:100, \n  .f = ~ {\n    .x %&gt;% \n      simulate_data(seed = .) %&gt;% \n      mutate(diff = (Y1 - Y0) / 0.8) %&gt;%\n      pull(diff) %&gt;% \n      mean()\n    }) %&gt;% mean()\n\n\n[1] 0.0999695"
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#all-the-estimates-in-a-single-plot",
    "href": "posts/instrumental-variable-in-RCT/index.html#all-the-estimates-in-a-single-plot",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "All the estimates in a single plot",
    "text": "All the estimates in a single plot\nWe can look at all four estimates in a single plot\n\n\nCode\nlist(\n  \"Instrumental variable\" = IV_posterior, \n  \"As-treated\" = AT_posterior, \n  \"Per-protocol\" = PP_posterior, \n  \"Intention-to-treat\" = ITT_posterior\n) %&gt;% \n  map2_dfr(.x = ., .y = names(.), .f = ~ {\n    .x %&gt;% \n      filter(str_detect(cat, \"Risk|Instrument\")) %&gt;% \n      mutate(cat = {{ .y }})\n    }) %&gt;% \n  mutate(\n    cat = factor(cat, levels = c(\n      \"Instrumental variable\", \"As-treated\",\n      \"Per-protocol\", \"Intention-to-treat\"\n  ))) %&gt;% \n  ggplot(aes(x = x, y = cat, group = cat, fill = rev(cat))) + \n  ggdist::stat_halfeye(\n    show.legend = FALSE\n  ) +\n  geom_vline(\n    xintercept = pY1 - pY0, \n    color = \"gray50\", \n    linetype = \"dashed\",\n    linewidth = 1.3\n  ) +\n  geom_vline(\n    xintercept = true_ITT_effect, \n    color = \"gray20\", \n    linetype = \"dotted\",\n    linewidth = 1.3\n  ) +\n  theme_classic() + \n  xlab(paste0(\n    \"Posterior distribution of risk difference / uplift\\n\", \n    \"(Dotted line: ATE in this population; dashed line: true ATE)\"\n  )) + \n  ylab(\"\") + \n  scale_x_continuous(labels = scales::label_percent(accuracy = 0.1))\n\n\n\n\n\nThe IV estimate is the closest to the true treatment effectiveness of +10 percentage points (in gray), with the AT and the PP effects both showing some bias.\nThe simulated data analyzed here are from a relatively clean setting where the treatment effectiveness, sample sizes, and strength of association between A-Z are all very high. If any of these conditions weaken, the analysis may become more challenging."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#profit-distribution",
    "href": "posts/instrumental-variable-in-RCT/index.html#profit-distribution",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "Profit distribution",
    "text": "Profit distribution\nSo we have two valid estimates – the ITT and the IV estimate. Can we use these estimates to calculate the total profit from this campaign, as well as other hypothetical campaigns with varying treatment / control splits and compliance?\nThis assumes that\n\nthe effect of treatment among the compliers and the never-takers would be the same\nthe conversion probabilities specified below would apply to these other populations\ncompliance varies randomly between populations\n\nData from other experiments, if available, can be used to get more realistic estimates by capturing the heterogeneity in the estimated probabilities (e.g., by meta-analysis).\nTo estimate the total profit, we need the following probability of conversion\n\\[\n\\text{Pr}[Y = 1] = \\sum_z \\text{Pr}[Y = 1 | Z = z] \\text{Pr}[Z = z]\n\\]\nwhere \\(\\text{Pr}[Z = z]\\) is the proportion of people assigned to treatment (\\(Z = 1\\)) and control (\\(Z = 0\\)), and the \\(\\text{Pr}[Y = 1 | Z = z]\\) is the ITT estimate15 in each arm. The ITT estimate under the monotonicity assumption can be further decomposed – using the law of total probability – into a weighted sum of the conversion probabilities among the compliers and the never-takers1615 Technically an estimand, but feels odd to type out ‘estimand’ everywhere in this and following paragraphs instead of ‘estimate’.16 The \\(\\text{Pr}[Z = z]\\) term in the numerator on the right side of the equation is constant with respect to the summation (over \\(c\\)), so can be pulled out and cancels out the same term in the denominator.\n\\[\n\\text{Pr}[Y = 1 | Z = z] = \\sum_c \\text{Pr}[Y = 1 | Z = z, C = c] \\text{Pr}[C = c | Z = z]\n\\]\nwhere \\(\\text{Pr}[C = c | Z = z] = \\text{Pr}[C = c]\\) because \\(Z\\) is randomly assigned, so \\(Z \\perp\\!\\!\\!\\perp C\\), i.e., \\(Z\\) and \\(C\\) are independent.\nOur best estimate of compliance is given by\n\\[\n\\text{Pr}[C = 1] = \\text{Pr}[A = 1 | Z = 1] - \\text{Pr}[A = 1 | Z = 0]\n\\]\nand \\(\\text{Pr}[C = 0] = 1 - \\text{Pr}[C = 1]\\) is the proportion of never-takers. To get the conditional probability in the first term in the sum, we can rewrite the previous equation – shown only for \\(Z = 1\\) but is the same for \\(Z = 0\\) – after diving each term by \\(\\text{Pr}[C = 1]\\)\n\\[\n\\begin{split}\n\\frac{\\text{Pr}[Y = 1 | Z = 1]}{\\text{Pr}[C = 1]} & = \\text{Pr}[Y = 1 | Z = 1, C = 1] \\\\ & + \\text{Pr}[Y = 1 | Z = 1, C = 0] \\frac{\\text{Pr}[C = 0]}{\\text{Pr}[C = 1]}\n\\end{split}\n\\]\nThe term on the left is the ITT estimate rescaled by the proportion of compliers. The first term on the right is the conversion probability among the compliers who received the treatment \\(\\text{Pr}[Y = 1 | Z = 1, C = 1]\\), and the second term on the right is the probability of conversion among the never-takers who were assigned to the treatment, weighted by the ratio of never-takers to compliers.\nThis also shows why subtracting the two rescaled ITT estimates (i.e., the IV estimate) is the effect of treatment among the compliers – the second term on the right cancels out (under the assumption of exclusion restriction) since \\(Z\\) is randomly assigned, so the never-takers in each arm should have the same probability of conversion\n\\[\n\\text{Pr}[Y = 1 | Z = 1, C = 0] = \\text{Pr}[Y = 1 | Z = 0, C = 0]\n\\]\nand proportion of compliers \\(\\text{Pr}[C = 1 | Z = 1] = \\text{Pr}[C = 1 | Z = 0]\\), and we’re left with\n\\[\n\\text{Pr}[Y = 1 | Z = 1, C = 1] - \\text{Pr}[Y = 1 | Z = 0, C = 1]\n\\]\nwhich is the CACE from the IV analysis.\nWhile compliance (\\(C\\)) is unobserved, we can estimate all these probabilites using the combined information present in \\(Z, A, Y\\) along with the assumptions in the previous paragraphs.\nFor the proportion of compliers (\\(\\text{Pr}[C = 1]\\)), we can use the posterior distribution we obtained for the IV analysis.\n\nprop_compliers &lt;- IV_posterior %&gt;% \n  filter(cat == \"Proportion of compliers\") %&gt;% \n  select(id, p_compliers = x)\n\nprop_compliers %&gt;%\n  mutate(p_never_takers = 1 - p_compliers) %&gt;% \n  summary()\n\n       id          p_compliers     p_never_takers  \n Min.   :     1   Min.   :0.7913   Min.   :0.1933  \n 1st Qu.: 25001   1st Qu.:0.7986   1st Qu.:0.1993  \n Median : 50000   Median :0.7997   Median :0.2003  \n Mean   : 50000   Mean   :0.7996   Mean   :0.2004  \n 3rd Qu.: 75000   3rd Qu.:0.8007   3rd Qu.:0.2014  \n Max.   :100000   Max.   :0.8067   Max.   :0.2087  \n\n\nFor the probability of conversion among the never takers, we can calculate this from information among the untreated assigned to the treatment group. A \\(\\text{Beta}(1, 1)\\) prior can be assumed here to get the following \\(\\text{Beta}(18 + 1, 13999 + 1) = \\text{Beta}(19, 14000)\\) posterior\n\ndata %&gt;% filter(A != Z) %$% table(Y)\n\nY\n    0     1 \n13999    18 \n\npY_never_takers &lt;- tibble(\n  id = 1:1e5, \n  pY_nt = rbeta(1e5, 19, 14000)\n)\n\npY_never_takers %&gt;% summary()\n\n       id             pY_nt          \n Min.   :     1   Min.   :0.0004274  \n 1st Qu.: 25001   1st Qu.:0.0011353  \n Median : 50000   Median :0.0013325  \n Mean   : 50000   Mean   :0.0013561  \n 3rd Qu.: 75000   3rd Qu.:0.0015507  \n Max.   :100000   Max.   :0.0031606  \n\n\nFor the ITT estimates, the posterior probabilities from the ITT analysis above are used\n\nITT_posterior_subset &lt;- ITT_posterior %&gt;% \n  filter(cat != \"Risk difference\") %&gt;% \n  pivot_wider(id_cols = id, names_from = cat, values_from = x) %&gt;% \n  rename(\n        \"p_control\" = \"Conversion probability \\n(control group)\", \n        \"p_treated\" = \"Conversion probability \\n(treatment group)\"\n    ) %&gt;% \n  glimpse()\n\nRows: 100,000\nColumns: 3\n$ id        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ p_control &lt;dbl&gt; 0.008838875, 0.008459186, 0.009314263, 0.009458440, 0.008553…\n$ p_treated &lt;dbl&gt; 0.09029532, 0.08987783, 0.09138095, 0.08802061, 0.09105225, …\n\n\nPutting these together gives\n\nposterior_draws &lt;- list(\n  prop_compliers, pY_never_takers, ITT_posterior_subset\n  ) %&gt;% \n  reduce(.f = ~ full_join(.x, .y, by = \"id\")) %&gt;% \n  glimpse()\n\nRows: 100,000\nColumns: 5\n$ id          &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ p_compliers &lt;dbl&gt; 0.7993178, 0.8003960, 0.7980252, 0.7976448, 0.8001245, 0.7…\n$ pY_nt       &lt;dbl&gt; 0.0010972405, 0.0010973038, 0.0017398618, 0.0014166189, 0.…\n$ p_control   &lt;dbl&gt; 0.008838875, 0.008459186, 0.009314263, 0.009458440, 0.0085…\n$ p_treated   &lt;dbl&gt; 0.09029532, 0.08987783, 0.09138095, 0.08802061, 0.09105225…\n\n\nThe conversion probabilities among the compliers are calculated, which results in the data frame with all the posterior draws for the probabilities we need.\n\n# set seed in case a subset of the posterior draws are used\n# set.seed(23)\nposterior_draws &lt;- posterior_draws %&gt;% \n  mutate(\n    across(\n      .cols = c(p_treated, p_control), \n      # rescale the ITT probabilities by p(compliance) and\n      # shave off the effect of never takers on this probability\n      # to recover the p(Y = 1 | Z = z, C = 1)\n      .fns = ~ ((.x / p_compliers) - \n                  (pY_nt * ((1 - p_compliers) / p_compliers))), \n      .names = \"{.col}_compliers\"\n    )\n  ) %&gt;% \n  # slice_sample(n = 5000) %&gt;% \n  # select(-id)  %&gt;% \n  glimpse()\n\nRows: 100,000\nColumns: 7\n$ id                  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ p_compliers         &lt;dbl&gt; 0.7993178, 0.8003960, 0.7980252, 0.7976448, 0.8001…\n$ pY_nt               &lt;dbl&gt; 0.0010972405, 0.0010973038, 0.0017398618, 0.001416…\n$ p_control           &lt;dbl&gt; 0.008838875, 0.008459186, 0.009314263, 0.009458440…\n$ p_treated           &lt;dbl&gt; 0.09029532, 0.08987783, 0.09138095, 0.08802061, 0.…\n$ p_treated_compliers &lt;dbl&gt; 0.1126900, 0.1120181, 0.1140685, 0.1099912, 0.1135…\n$ p_control_compliers &lt;dbl&gt; 0.010782542, 0.010295105, 0.011231293, 0.011498576…\n\n\nIn principle, any of these probabilities can be replaced with other priors17 when attempting to transport these estimates to the other scenarios.17 rather than using the observed posteriors as priors.\nFor the plot below, there are two dimensions\n\ncompliance with levels set to 50%, 80%, 100%\ntreatment / control split\n\n70/30% as in this experiment\n0/100% - don’t treat the population\n100/0% - treat the full population\n\n\nIt feels a bit odd to look at the combination of compliance with 0% of the population assigned to treatment, as the (non-)compliers are only identified – and the respective conditional probabilities estimated – when at least some individuals are assigned to treatment. In this case, it’s more of a hypothetical exercise using data from an experiment where &gt;0% of the population was assigned to the treatment arm. Page 41 of the Greenland et al 1999 paper mentions\n\n[…] noncompliance represents movement into a third untreated state that does not correspond to any assigned treatment (Robins, 1998).\n\nI haven’t managed to go through the Robins 1998 paper yet but it would be interesting to dig into this further.\n\nparameter_grid &lt;- expand_grid(\n  compliance = c(0.5, 0.8, 1.0),\n  split = c(0.7, 1.0, 0.0), \n  n = n\n)\n\nTotal profit distributions for each of these hypothetical experiments is calculated using the formulas specified above. To get the variability in the profit distribution, total sales \\(S \\sim \\text{BetaBin}(n, \\alpha, \\beta)\\) can be sampled from the (beta-binomial) posterior predictive distribution, where the probability of conversion \\(p \\sim \\text{Beta}(\\alpha, \\beta)\\) and \\(S \\sim \\text{Binomial}(n, p)\\) under the different interventions in different settings. The total profit can be obtained from multiplying \\(S\\) with the profit per unit, which can be fixed at (say) +100 euros.\n\nprofit_distributions &lt;- expand_grid(parameter_grid, posterior_draws) %&gt;% \n  mutate(\n    # get the ITT estimates for the hypothetical campaigns\n    # under the different parameters\n    # pY_nt is scaled by proportion of compliers\n    # as this wasn't done above\n    p_treated_hyp = ((compliance * p_treated_compliers) + \n      ((1 - compliance) * (pY_nt / p_compliers))),\n    p_control_hyp = ((compliance * p_control_compliers) + \n      ((1 - compliance) * (pY_nt / p_compliers))),\n    n_treated = round(split * n), \n    n_control = n - n_treated, \n    s_treated = rbinom(n(), n_treated, p_treated_hyp), \n    s_control = rbinom(n(), n_control, p_control_hyp), \n    total_profit = 100 * (s_treated + s_control)\n  ) %&gt;% \n  glimpse()\n\nRows: 900,000\nColumns: 17\n$ compliance          &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, …\n$ split               &lt;dbl&gt; 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, …\n$ n                   &lt;int&gt; 100000, 100000, 100000, 100000, 100000, 100000, 10…\n$ id                  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ p_compliers         &lt;dbl&gt; 0.7993178, 0.8003960, 0.7980252, 0.7976448, 0.8001…\n$ pY_nt               &lt;dbl&gt; 0.0010972405, 0.0010973038, 0.0017398618, 0.001416…\n$ p_control           &lt;dbl&gt; 0.008838875, 0.008459186, 0.009314263, 0.009458440…\n$ p_treated           &lt;dbl&gt; 0.09029532, 0.08987783, 0.09138095, 0.08802061, 0.…\n$ p_treated_compliers &lt;dbl&gt; 0.1126900, 0.1120181, 0.1140685, 0.1099912, 0.1135…\n$ p_control_compliers &lt;dbl&gt; 0.010782542, 0.010295105, 0.011231293, 0.011498576…\n$ p_treated_hyp       &lt;dbl&gt; 0.05703136, 0.05669450, 0.05812436, 0.05588362, 0.…\n$ p_control_hyp       &lt;dbl&gt; 0.006077632, 0.005833028, 0.006705751, 0.006637289…\n$ n_treated           &lt;dbl&gt; 70000, 70000, 70000, 70000, 70000, 70000, 70000, 7…\n$ n_control           &lt;dbl&gt; 30000, 30000, 30000, 30000, 30000, 30000, 30000, 3…\n$ s_treated           &lt;int&gt; 3981, 3988, 4160, 3843, 3968, 3943, 3813, 3905, 40…\n$ s_control           &lt;int&gt; 186, 172, 209, 214, 199, 189, 204, 209, 183, 159, …\n$ total_profit        &lt;dbl&gt; 416700, 416000, 436900, 405700, 416700, 413200, 40…\n\n\nThe posterior predictive distribution for the total profit is visually compared with simulated draws from the data generating process\n\n\nCode\nsimulated_profit &lt;- parameter_grid %&gt;% \n  expand_grid(id = seq(from = 20, by = 1, length.out = 20)) %&gt;% \n  pmap_dfr(\n    .f = function(compliance, split, id, n) {\n      simulate_data(n = n,\n                    p_compliers = compliance, \n                    p_treatment = split,\n                    seed = id) %&gt;%\n        summarise(total_profit = 100 * sum(Y)) %&gt;% \n        tibble(compliance, split, .)\n    }\n  )\n\nprofit_distributions_plot_data &lt;- list(\n  \"extrapolated\" = profit_distributions %&gt;% \n    select(compliance, split, total_profit),\n  \"simulated\" = simulated_profit\n  ) %&gt;% \n  map(\n    .f = ~ {\n      .x %&gt;% \n        mutate(\n          total_profit = total_profit / 1e5, \n          compliance = paste0(\"Compliers: \", compliance * 100, \"%\"), \n          compliance = factor(compliance, \n                        levels = c(\"Compliers: 50%\",\n                                   \"Compliers: 80%\",\n                                   \"Compliers: 100%\")),\n          split = paste0(\"Treatment: \", split * 100, \"%\"), \n          split = factor(split, \n                         levels = c(\"Treatment: 0%\", \n                                    \"Treatment: 70%\", \n                                    \"Treatment: 100%\"))\n        )\n    }\n  )\n\nprofit_distributions_plot_data %&gt;% \n  pluck(\"extrapolated\") %&gt;% \n  ggplot(aes(x = total_profit)) + \n  geom_density(trim = TRUE) + \n  geom_vline(\n    data = profit_distributions_plot_data %&gt;% pluck(\"simulated\"), \n    aes(xintercept = total_profit), color = \"maroon\"\n  ) + \n  theme_bw() + \n  facet_wrap(vars(split, compliance), scales = \"free\") + \n  #facet_grid(compliance ~ split, scales = \"fixed\") +\n  xlab(\"Total profit (multiples of 100,000 EUR, per 100,000 individuals)\")\n\n\n\n\n\nThe middle column shows the total profit distribution for this experiment with a 70-30% split and 80% compliance. In this fictional scenario, the ad campaign has a strong positive impact on the total profits (90,000 under 0% treated and 80% compliance vs 900,000 under 100% treated) so with the gift of hindsight, it could have been rolled out to the full population.\nIf you’re wondering why the posterior predictive distribution (black line) in each panel is not centered at the mean of the vertical lines (in maroon), it’s from using the estimated probabilities from the original dataset which differ from the true probabilities because of sampling variability18.18 This can be checked by using the true probabilities, i.e., p_compliers = 0.8, pY_nt = 0.001, p_treated_compliers = 0.1, p_control_compliers = 0.01 instead of the posterior distributions for these probabilities."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#ignored-complexities",
    "href": "posts/instrumental-variable-in-RCT/index.html#ignored-complexities",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "Ignored complexities",
    "text": "Ignored complexities\nIn this post, I ignored the complexity where the exposure among the treated – i.e., number of impressions of the ad – would be continuous, possibly something like this\n\n\nCode\nset.seed(23)\ntibble(impressions = pmax(round(rgamma(5e4, shape = 4, scale = 3)), 1)) %&gt;% \n  ggplot(aes(x = impressions)) + \n  geom_bar() +\n  #stat_ecdf() + \n  theme_classic() + \n  xlab(\"Impressions\") + \n  scale_x_continuous(breaks = seq(0, 50, 5), labels = seq(0, 50, 5))\n\n\n\n\n\nIgnoring the actual impressions effectively assumes treatment variation irrelevance, i.e, seeing 1 impression is as effective as seeing 20 impressions with regards to conversion. This might be fun to explore in a future post on binary instruments, with a continuous exposure19. Baiocchi et al. (section 12) describes an analysis with continuous treatments.19 although all references mention that this complicates the analysis\nIssue with handling partial exposures, i.e., people with one or more partial impressions are also not tackled here.\nFor building more complex / full Bayesian IV models using brms / Stan, see this, this, this, or this."
  },
  {
    "objectID": "posts/instrumental-variable-in-RCT/index.html#references",
    "href": "posts/instrumental-variable-in-RCT/index.html#references",
    "title": "How should the untreated in the treatment group from an experiment be analyzed?",
    "section": "References",
    "text": "References\nThese are the main references I’m using for this post20.20 although I can’t claim to have read and mastered all these.\n\nSimulating data for instrumental variables - Andrew Gelman blog post link, or this link to statistical rethinking models implemented using brms\nHernán, Robins - Causal Inference - What if? - Chapters 16 (instrumental variables), and 22 (sections on intention-to-treat and per-protocol analyses)\nGelman, Hill, Vehtari - Regression and other stories, Sections 21.1-21.2\nHernán, Robins 2006 - Instruments for causal inference: an epidemiologist’s dream?\nBurgess, Small, Thompson 2017 - A review of instrumental variable estimators for Mendelian randomization\nBaiocchi, Cheng, Small 2014 - Instrumental variable methods for causal inference\n\nGood review paper for different IV situations\n\nSussman, Hayward 2010 - An IV for the RCT: using instrumental variables to adjust for treatment contamination in randomised controlled trials\nNaimi, Whitcomb 2020 - Calculating risk differences and risk ratios from regression models\nImbens, Rubin 1997 - Bayesian inference for causal effects in randomized experiments with noncompliance\nBalke, Pearl 1997 - Counterfactual Probabilities: Computational Methods, Bounds and Applications\nSwanson et al 2018 - Partial Identification of the Average Treatment Effect Using Instrumental Variables: Review of Methods for Binary Instruments, Treatments, and Outcomes\n\nNote to future self: good rabbit hole to go down to calculate bounds on the ATE under different assumption sets\n\nGreenland, Pearl, Robins 1999 - Confounding and Collapsibility in Causal Inference\nRobins 1998 - Correction for non-compliance in equivalence trials (free link)"
  },
  {
    "objectID": "posts/jigger-volume-estimation/index.html",
    "href": "posts/jigger-volume-estimation/index.html",
    "title": "Determining the volume of my (cocktail) jigger",
    "section": "",
    "text": "In the fall of 2022, I was gifted a cocktail mixing set for my birthday1 which included a cocktail jigger. A jigger is an essential tool for mixing cocktails as it makes it relatively easy to measure the quantity of drinks that go into a cocktail (e.g., 40 ml of vodka, 10 ml of sugar syrup, etc.).1 It was put to use immediately and some delicious whiskey sours were made.\nAfter a couple of months of not having to use my cocktail set due to travelling, I couldn’t remember the volume of either side (i.e., basin) of the jigger, nor could I find the remains of the packaging material for reference. Googling showed that jiggers come in different sizes, so I decided to measure the volume myself."
  },
  {
    "objectID": "posts/jigger-volume-estimation/index.html#method-1-weight-based",
    "href": "posts/jigger-volume-estimation/index.html#method-1-weight-based",
    "title": "Determining the volume of my (cocktail) jigger",
    "section": "Method 1: Weight-based",
    "text": "Method 1: Weight-based\nThe digital kitchen scale I own has this nifty feature where it can account for and automatically subtract the weight of the container so that only the weight of the object of interest is shown.\nThe scale measures weight in grams (g), but I was interested in the volume in milliliters (ml). Quick Googling reminded me of the nifty fact I learned and forgot a long time ago, that the weight of 1 g of water is approximately equal to the volume occupied by 1 ml of water.\nPlacing the jigger on the scale and ensuring that the scale read zero g, I filled up the larger basin to the top with tap water. The scale showed 51g, which was odd as I was expecting it to show a number that would be a multiple of five (i.e., a number ending with a 0 or a 5)3.3 although some jigger images online show possible measurements like 22.5 ml (which is 0.75 oz)\nSo obviously4 the next logical step was to take multiple measurements. After making sure to drain the jigger, clean the scale of any spilled water droplets, and ensuring the scale read zero, I took the next measurement which read 48 g. Excellent. I love consistent results.4 obvious to a Statistician or a Data Scientist\nSo I took another 8 measurements, and repeated this process for the smaller basin of the jigger as well. These measurements are visualized below.\nEach time I filled it to what I perceived to be the top but I got different measurements. I don’t think that this variability is necessarily due to the variability of the digital scale itself but most likely due to the variability in my perception of what counts as ‘filled to the brim’.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\nvol_big &lt;- c(51, 48, 48, 52, 49, 50, 51, 52, 50, 50)\nvol_small &lt;- c(31, 28, 32, 30, 30, 29, 32, 32, 31, 31)\n\ntibble(\n  Basin = c(rep(\"Smaller basin\", 10), rep(\"Larger basin\", 10)),\n  `Volume (in ml)` = c(vol_small, vol_big)\n) %&gt;%\n  mutate(Basin = forcats::fct_rev(Basin)) %&gt;% \n  ggplot(aes(x = `Volume (in ml)`, fill = Basin, color = Basin)) +\n  geom_dotplot(binwidth = 1, stroke = 2, dotsize = 0.7) +\n  theme_classic() +\n  theme(legend.position = \"none\", \n        axis.title.x = element_text(size = 16), \n        axis.text.x = element_text(size = 16), \n        strip.text = element_text(size = 16)) + \n  scale_y_continuous(NULL, breaks = NULL) + \n  scale_fill_manual(values = c(\"#E69F00\", \"darkgreen\")) + \n  scale_color_manual(values = c(\"#E69F00\", \"darkgreen\")) + \n  facet_wrap(~ Basin, scales = 'free')\n\n\n\n\n\nAveraging these measurements gave me a volume of 50.1 ml for the big basin and 30.6 ml for the smaller basin.\nInterestingly5, it also indicates6 that I usually put anywhere between 48-52 (or 28-32) ml of a drink when I’m supposed to add 50 (or 30) ml.5 to me and literally nobody else6 assuming the digital scale is not causing this variability"
  },
  {
    "objectID": "posts/jigger-volume-estimation/index.html#method-2-dimension-based",
    "href": "posts/jigger-volume-estimation/index.html#method-2-dimension-based",
    "title": "Determining the volume of my (cocktail) jigger",
    "section": "Method 2: Dimension-based",
    "text": "Method 2: Dimension-based\nThe shape of a jigger reminded me of a (partial) cone, so I measured7 the height \\(h\\) from the mouth of each basin to the point where it joins the other basin, and the diameter \\(2R\\) of the mouth of each basin.7 using a pair of digital calipers I totally had lying around the house and did not use this exercise as an excuse to buy\nFor the smaller basin, the height \\(h_s\\) was measured as 3.48 cm, and a diameter of 3.9 cm (so a radius \\(R_s\\) of 1.95 cm).\nFor the larger basin, the height \\(h_l\\) was measured as 5.28 cm, and a diameter of 4.18 cm (so a radius \\(R_l\\) of 2.09 cm).\nThe radius \\(r\\) of the base where the two basins are attached to each other is \\(2.78 / 2 = 1.39\\).\nPlugging these into the formula for a partial cone88 here’s a derivation (and another) of the volume of a cone using calculus, which I’ve probably derived in school or university and long since forgotten\n\\[V = \\frac{1}{3} \\times \\pi \\times h \\times \\Big(R^2 + Rr + r^2\\Big)\\]\n\n\nCode\ncone &lt;- function(h, r, R) {\n  (pi * h * (R^2 + (R * r) + r^2)) / 3\n}\n\n\ngave a volume9 of 30.8 ml for the smaller basin and 50.9 for the larger basin which was very similar to the numbers from the other method, but not exactly equal to 30 and 50 ml due to (human) measurement error.9 using the conversion factor of 1 cm3 to 1 ml for volume of water\nA few days after doing all these measurements, I came across a similar (in spirit?) post on a blog I frequent, which inspired me to write this up."
  },
  {
    "objectID": "posts/one-reason-why-a-glm-coefficient-is-NA/index.html",
    "href": "posts/one-reason-why-a-glm-coefficient-is-NA/index.html",
    "title": "One reason why a (g)lm coefficient is NA",
    "section": "",
    "text": "TL;DR: Use the alias function in R to check if you have nested factors (predictors) in your data\nRecently while fitting a logistic regression model, some of the coefficients estimated by the model were NA. Initially I thought it was due to separation1, as that’s the most common issue I usually face when fitting unregularized models on data.1 see this Wikipedia article, or this stats.stackexchange.com thread (and the associated links in the sidebar)\nHowever, googling2 threw up many threads on multicollinearity and anyway, separation usually leads to nonsensical estimates like \\(1.5 \\times 10^8\\) instead of NA.2 in this day and age of ChatGPT, I know\nAfter combing through many stackexchange threads, I discovered the alias function in R from this thread, which was pretty handy at identifying the problematic column(s).\nIt’s interesting that the alias documentation doesn’t mention anything about GLMs (glm()) but this does work on glm(..., family = \"binomial\") model objects3.3 possibly since the class of a glm object is c(\"glm\", \"lm\")\nThe rest of this post explores this issue and its resolution using aggregated test data, where the city variable is intentionally nested within the country variable.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nsimulated_data &lt;- tribble(\n  ~age, ~city, ~country, ~y, ~N,\n  \"&lt; 30\", \"Paris\", \"France\", 30, 100,\n  \"&lt; 30\", \"Nice\", \"France\", 20, 100,\n  \"&lt; 30\", \"Berlin\", \"Germany\", 23, 100,\n  \"30+\", \"Paris\", \"France\", 12, 100,\n  \"30+\", \"Nice\", \"France\", 11, 100,\n  \"30+\", \"Berlin\", \"Germany\", 27, 100\n) %&gt;% \n  mutate(y = y / N)\n\nmodel &lt;- glm(y ~ age + city + country, weights = N, data = simulated_data, family = \"binomial\")\n\nsummary(model)\n\n\nCall:\nglm(formula = y ~ age + city + country, family = \"binomial\", \n    data = simulated_data, weights = N)\n\nDeviance Residuals: \n      1        2        3        4        5        6  \n 1.1459   0.3555  -1.4512  -1.4069  -0.4309   1.5443  \n\nCoefficients: (1 not defined because of singularities)\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.8733     0.1876  -4.656 3.23e-06 ***\nage30+          -0.4794     0.2062  -2.325   0.0201 *  \ncityNice        -0.6027     0.2558  -2.356   0.0185 *  \ncityParis       -0.2286     0.2395  -0.954   0.3399    \ncountryGermany       NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 19.2591  on 5  degrees of freedom\nResidual deviance:  8.0953  on 2  degrees of freedom\nAIC: 43.492\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe estimate for Germany is NA. Calling the alias function on this GLM model shows that the dummy variable of Germany is linearly dependent on (a subset of) the other columns.\n\nalias(model)\n\nModel :\ny ~ age + city + country\n\nComplete :\n               (Intercept) age30+ cityNice cityParis\ncountryGermany  1           0     -1       -1       \n\n\nThis means that the column for Germany is redundant in this design matrix (or the model matrix), as the values of Germany (the pattern of 0s and 1s) can be perfectly predicted / recreated by combining the Intercept, Nice, and Paris columns using the coefficients from the output of alias(). This is why the perfect multicollinearity in this case leads to an NA coefficient.\n\n# countryGermany and Germany are identical\nmodel.matrix(~ ., data = simulated_data) %&gt;% \n  as_tibble() %&gt;% \n  select(-y, -N, -`age30+`) %&gt;% \n  mutate(Germany = `(Intercept)` - cityNice - cityParis)\n\n# A tibble: 6 × 5\n  `(Intercept)` cityNice cityParis countryGermany Germany\n          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1             1        0         1              0       0\n2             1        1         0              0       0\n3             1        0         0              1       1\n4             1        0         1              0       0\n5             1        1         0              0       0\n6             1        0         0              1       1\n\n\nAnother interesting observation is that changing the order of the variables\n\n# here country comes before city\nglm(y ~ age + country + city, weights = N, data = simulated_data, family = \"binomial\") %&gt;% \n  alias()\n\nModel :\ny ~ age + country + city\n\nComplete :\n          (Intercept) age30+ countryGermany cityNice\ncityParis  1           0     -1             -1      \n\n\nleads to different estimates being NA, i.e., the estimate for Paris is now NA and is linearly dependent on the Intercept, country (Germany), and another city (Nice).\nThe simplest solution here is to drop one of the city or country columns, or to build two separate models – one without country, and one without city.\nIf the goal is to estimate coefficients for both the city and country variables, then a mixed model with nested effects might be the right rabbit hole to go down, assuming they both have more than 10 levels or so. See the following links:\n\nhttps://stats.stackexchange.com/questions/197977/analyzing-nested-categorical-data\nhttps://stats.stackexchange.com/questions/79360/mixed-effects-model-with-nesting\nhttps://stats.stackexchange.com/questions/372257/how-do-you-deal-with-nested-variables-in-a-regression-model\nSecond bullet point from the answer: https://stats.stackexchange.com/questions/243811/how-to-model-nested-fixed-factor-with-glmm\nhttps://stackoverflow.com/questions/70537291/lmer-model-failed-to-converge-with-1-negative-eigenvalue\nhttps://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\nhttps://stackoverflow.com/questions/40723196/why-do-i-get-na-coefficients-and-how-does-lm-drop-reference-level-for-interact"
  },
  {
    "objectID": "posts/spelling-bee/index.html",
    "href": "posts/spelling-bee/index.html",
    "title": "Setup and test Python by programming a spelling bee solver",
    "section": "",
    "text": "All the previous posts on this blog have been in R. But sometimes I want to program in another language – Python, Scala, etc. So in the process of setting up and testing Python with Quarto, I decided to write this test post where I implement a set of simple functions to solve the word blossom / spelling bee1 game given a set of letters along with some constraints (min / max / total word length, etc).1 Twenty years of being familiar with this game and I never knew what it was called until I looked it up for this post. I could only find “word blossom” while Googling, but a friend I sent this to mentioned that it’s called spelling bee. Which makes sense since it looks like a honeycomb.\nBefore proceeding, let’s load some packages.\n\n# for generating list of letters\nimport string \nimport random\n\n# for plotting\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import RegularPolygon\nimport numpy as np\n\n# for loading the word list\nimport json\n\n# for typing, because why not\nfrom typing import List\n\nThe variants I’ve encountered online – this, this, or this – consist of a sequence of 6 letters arranged around a central letter.\n\ndef generate_letters(seed: int, nchar: int = 7) -&gt; List[str]:\n    random.seed(seed)\n\n    central_letter = random.choice([\"A\", \"E\", \"I\", \"O\", \"U\"])\n    alphabet = string.ascii_uppercase.replace(central_letter, \"\")\n    non_central_letters = random.sample(population=alphabet, k=nchar-1)\n\n    return [central_letter] + non_central_letters\n\nletters = generate_letters(seed=95)\n\nprint(letters)\n\n['U', 'Q', 'Y', 'R', 'E', 'P', 'A']\n\n\nVisually, it looks something like this (using a modified version of code from this stackexchange post)22 Not having written any Python for more than a year, I’d forgotten how much fun it was to customize matplotlib plots.\n\n\nCode\ndef plot_word_blossom(letters: List[str]) -&gt; None:\n  coord = [[0,0,0],[0,1,-1],[-1,1,0],[-1,0,1],[0,-1,1],[1,-1,0],[1,0,-1]]\n  colors = [[\"orange\"]] + [[\"white\"]] * 6\n  labels = [[l] for l in letters]\n  \n  # Horizontal cartesian coords\n  hcoord = [c[0] for c in coord]\n  \n  # Vertical cartersian coords\n  vcoord = [2. * np.sin(np.radians(60)) * (c[1] - c[2]) /3. for c in coord]\n  \n  fig, ax = plt.subplots(1)\n  ax.set_aspect(\"equal\")\n  \n  # Add some coloured hexagons\n  for x, y, c, l in zip(hcoord, vcoord, colors, labels):\n      color = c[0]\n      hex = RegularPolygon((x, y), numVertices=6, radius=2. / 3., \n                           orientation=np.radians(30), \n                           facecolor=color, alpha=1, edgecolor='k')\n      ax.add_patch(hex)\n      # Also add a text label\n      ax.text(x, y, l[0], ha=\"center\", va=\"center\", size=20)\n  \n  # Also add scatter points in hexagon centres\n  # setting alpha = 0 to not show the points\n  ax.scatter(hcoord, vcoord, c=[c[0].lower() for c in colors], alpha=0)\n  \n  plt.axis(\"off\")\n  plt.show();\n\nplot_word_blossom(letters)\n\n\n\n\n\nThe goal is to make as many words as possible that meet the following conditions\n\neach word must be an actual word (say present in a British English dictionary)\nword length between 4-7\neach word must contain the central letter\n\nThe version I used to play as a kid had the additional constraint that each letter could be used only once.\nTo solve this, I load a dictionary (obtained from here) and just look up words that match the requirements.\n\ndef get_list_of_eligible_words(min_length: int = 4, \n                               max_length: int = 7, \n                               no_duplicates: bool = True) -&gt; List[str]:\n      with open(\"words_dictionary.json\") as f:\n            words_dict = json.load(f)\n\n      words = list(words_dict.keys())\n      n_all = len(words)\n\n      if no_duplicates:\n            # keep words with no duplicate characters, \n            # e.g., set turns 'coop' into {'c', 'o', 'p'}\n            words = [w for w in words if len(w) == len(set(w))]\n\n      words = [w for w in words if len(w) &gt;= min_length and len(w) &lt;= max_length]\n\n      print(f\"Found {len(words):,} words out of {n_all:,} that meet the criteria.\")\n      \n      return words\n      \nwords = get_list_of_eligible_words()\n\nFound 43,239 words out of 370,101 that meet the criteria.\n\n\nThis next function takes the list of dictionary words and filters the subset of words that match the criteria.\n\ndef get_words(words: List[str], letters: List[str]) -&gt; List[str]:\n    central_letter = letters[0].lower()\n    letters_set = set([l.lower() for l in letters])\n    result = sorted([word for word in words if set(word.lower()).issubset(letters_set) and central_letter in word.lower()])\n    print(f\"Found {len(result)} words for the given letters {letters!r} with {central_letter!r} as the central letter.\")\n    return result\n\nprint(get_words(words=words, letters=letters), sep=\",\")\n\nFound 21 words for the given letters ['U', 'Q', 'Y', 'R', 'E', 'P', 'A'] with 'u' as the central letter.\n['aperu', 'paque', 'pareu', 'perau', 'peru', 'prau', 'prue', 'pure', 'purey', 'puya', 'quae', 'quar', 'quare', 'quay', 'query', 'quey', 'rupa', 'urea', 'yaru', 'yaup', 'yauper']\n\n\nHmm, some of these words are pretty uncommon. For example, ‘paque’ – pronounced ‘pack’ – means Easter (similar to the French word ‘Pâques’). ‘Perau’ is an alternate spelling for ‘perahu’ which means boat and comes from Indonesian / Malay. These are both new to me.\nI was tempted to add a little self-contained Shiny application within this quarto document using shinylive (github link) which I discovered while writing this post. However, I decided against this for the moment since the shinylive examples hosted on github-pages took a noticeable amount of time to load on my fast laptop + browser + internet connection.\nTrying to use both RStudio and VS Code, qmd and ipynb files together seems a bit clunky at the moment. I guess with time I’ll end up finding workarounds to reduce friction in this workflow."
  },
  {
    "objectID": "posts/tweedie-with-identity-link-and-offset/index.html",
    "href": "posts/tweedie-with-identity-link-and-offset/index.html",
    "title": "Specifying an offset in a Tweedie model with identity link",
    "section": "",
    "text": "This post explores several things:\nIn this article, the words duration, offset, exposure, and weight are used interchangeably."
  },
  {
    "objectID": "posts/tweedie-with-identity-link-and-offset/index.html#introduction",
    "href": "posts/tweedie-with-identity-link-and-offset/index.html#introduction",
    "title": "Specifying an offset in a Tweedie model with identity link",
    "section": "Introduction",
    "text": "Introduction\nData coming from fields like ecology, insurance, epidemiology – such as number of new cases of a disease in multiple cities with very different population sizes, number of claims and total claim amounts from insurance policies with different durations, etc. – need to have the varying exposures accounted for as offsets while building models. These might look something like the following simulated dataset with an exposure variable (w) and a response variable (y):\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nn &lt;- 1e6\n\n# tweedie distribution parameters\np &lt;- 1.3 # variance power\nmu &lt;- 200 # mean\nphi &lt;- 350 # dispersion, so variance = phi * mu ^ p\n\nset.seed(45)\nsim_data &lt;- tibble(\n  w = runif(n, min = 0.5, max = 3),\n  y = tweedie::rtweedie(n = n, mu = mu, phi = phi / w, power = p)\n)\n\n# summary(sim_data)\n\nsim_data %&gt;% slice_head(n = 3)\n\n# A tibble: 3 × 2\n      w     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1  2.08     0\n2  1.29     0\n3  1.10     0\n\n\nFor readability, let’s pretend that this response variable is the individual claim amounts for an insurance company with a client base of 1 million customers. Additionally, for each individual the duration of how long they’ve been insured is also recorded. So the first customer has been insured for about 2 years and 1 month, and has not filed a claim at all. The third customer has been insured for a little over a year and has also not filed any claims."
  },
  {
    "objectID": "posts/tweedie-with-identity-link-and-offset/index.html#ratio-of-means-or-mean-of-ratios",
    "href": "posts/tweedie-with-identity-link-and-offset/index.html#ratio-of-means-or-mean-of-ratios",
    "title": "Specifying an offset in a Tweedie model with identity link",
    "section": "Ratio of means or mean of ratios?",
    "text": "Ratio of means or mean of ratios?\nThe quantity of interest from such datasets may be the mean per unit exposure – expected claim amount per individual per year, expected number of new cases of a disease per year per 100,000 people, etc. There are two separate quantities that can be calculated as expected values – the ratio of the means of amount and duration variables, or the mean of the individual ratios. The differences between these two quantities is explained pretty well in this stackexchange thread.\nThe ratio estimator estimates the first quantity \\(R = \\sum_i{y_i} / \\sum_i{w_i}\\). This answers the following question: given a population (or a group of individuals) who were followed-up / observed for a given amount of time, and generated a total amount of claims, how much of the total claim amount can be attributed per person per unit exposure?\nThe weighted sample mean of the individual ratios estimates the second quantity \\(\\mathbb{E} = \\sum_i{w_i y_i} / \\sum_i{w_i}\\). This answers the question: given each individual’s claim amount and exposure, what was the average individual’s claim amount per unit exposure?\nIn the latter case, the link between claim amount and duration at the individual level is preserved, whereas for the former, the numerator and denominators are totals at the population level.\nIf everyone had exactly the same weight \\(w_i\\) of 1 year, the denominator would sum to the sample size of 1 million for the data above, and both the estimates would be the same. If the weights were different, then the two results would differ.\nIn epidemiology, the ratio estimator is used for incidence rate calculations. On the other hand, the insurance papers on Tweedie models mentioned at the bottom of this post all seem to use the weighted mean approach.\nFor the simulated data above, the two estimates are quite different:\n\nsum(sim_data$y) / sum(sim_data$w)\n\n[1] 114.5185\n\n\n\nweighted.mean(sim_data$y, w = sim_data$w)\n\n[1] 200.4661\n\n\nIn this case, the second estimate matches the true mean value chosen for the simulation, which makes sense given that the data are simulated from a Tweedie model with varying exposures (\\(Y_i \\sim \\text{Tweedie}(\\mu_i, \\phi / w_i, p)\\)) but with the mean-variance relationship preserved under unit exposure. This is described in part A of the supplement to the Yang et al 2016 paper.\nHowever, both the numbers are right, as they answer different questions."
  },
  {
    "objectID": "posts/tweedie-with-identity-link-and-offset/index.html#building-a-regression-model",
    "href": "posts/tweedie-with-identity-link-and-offset/index.html#building-a-regression-model",
    "title": "Specifying an offset in a Tweedie model with identity link",
    "section": "Building a regression model",
    "text": "Building a regression model\nThis formula is easy enough to apply if there are no predictors, but a regression model is needed to estimate these quantities when there are predictors in the data.\nThere are two ways of specifying the model. The first method corresponds to passing the claim amounts unmodified, and passing the exposures as weights:\n\n# weighted mean estimator\nsummary(glm(y ~ 1, weights = w, data = sim_data,\n            family = statmod::tweedie(var.power = 1.3, link.power = 1)))\n\n\nCall:\nglm(formula = y ~ 1, family = statmod::tweedie(var.power = 1.3, \n    link.power = 1), data = sim_data, weights = w)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 200.4661     0.4431   452.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 349.4572)\n\n    Null deviance: 240499666  on 999999  degrees of freedom\nResidual deviance: 240499666  on 999999  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe estimated intercept and dispersion correspond to the same parameters (\\(\\mu = 200\\), \\(\\phi = 350\\)) picked for the simulation. This estimate also coincides with the weighted mean estimate. link.power = 1 indicates that the identity link function is used, and var.power = 1.3 or \\(p = 1.3\\) is assumed to be known here. Usually the profile likelihood approach is used to pick the best value on the interval \\(p \\in (1, 2)\\) for data with exact zeroes.\nOnce \\(p\\) is chosen, \\(\\mu\\) can be estimated independently of \\(\\phi\\), and finally dispersion \\(\\phi\\) can be estimated using an optimization algorithm, or one of the estimators described in section 6.8 of the Dunn and Smyth book. The glm function in R uses the Pearson estimator:\n\\[\n\\phi = \\frac{1}{N-p'} \\sum_{i=1}^{N} \\frac{w_i (y_i - \\hat\\mu_i) ^ 2}{\\hat\\mu_i^p}\n\\]\nwhere \\(p'\\) is the number of parameters in the model and \\(\\text{Var}(\\mu_i) = \\mu_i^p\\) is the variance function.\n\n# pearson estimate of dispersion\n(sum((sim_data$w * ((sim_data$y - 200.4661)^2)) / (200.4661^1.3))) / (n - 1)\n\n[1] 349.4573\n\n\nThe second method rescales the response variable with the weights, and passes exposures as weights to the model:\n\n# ratio estimator\nsummary(glm(y / w ~ 1, weights = w, data = sim_data,\n            family = statmod::tweedie(var.power = 1.3, link.power = 1)))\n\n\nCall:\nglm(formula = y/w ~ 1, family = statmod::tweedie(var.power = 1.3, \n    link.power = 1), data = sim_data, weights = w)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 114.5185     0.3655   313.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 492.3754)\n\n    Null deviance: 177557174  on 999999  degrees of freedom\nResidual deviance: 177557174  on 999999  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n\n\nThis estimate corresponds to the same output as the ratio estimator from the previous section. The estimate of dispersion \\(\\phi\\) is different from the true value of 350, which makes sense as dispersion is a function of the estimated value of the mean \\(\\mu\\)."
  },
  {
    "objectID": "posts/tweedie-with-identity-link-and-offset/index.html#what-does-the-math-look-like-behind-these-two-models",
    "href": "posts/tweedie-with-identity-link-and-offset/index.html#what-does-the-math-look-like-behind-these-two-models",
    "title": "Specifying an offset in a Tweedie model with identity link",
    "section": "What does the math look like behind these two models?",
    "text": "What does the math look like behind these two models?\nTo fit a Tweedie model to the data with weights to calculate the two different values in the previous section, the following log-likelihood function – taken from the Yang et al paper – can be maximized.\n\\[\nl(\\mu, \\phi, p | \\{y, x, w\\}_{i=1}^n) = \\sum_{i = 1}^{n}{ \\frac{w_i}{\\phi}} \\Bigg(y_i \\frac{\\mu_i^{1-p}}{1-p} - \\frac{\\mu_i^{2-p}}{2-p} \\Bigg) + log\\ a(y_i, \\phi / w_i, p)\n\\]\nFor this simulated dataset, \\(w_i\\) and \\(y_i\\) vary for each individual, but there are no predictors so an identity link function can be used 1, the overall mean is of interest so the \\(\\mu_i\\) term can be collapsed into a single \\(\\mu\\) parameter to be optimized, and the log a(...) term can be ignored as it’s not a function of \\(\\mu\\). Since the regression equation for an intercept only term is \\(\\mu_i = \\beta_0\\), we can replace the \\(\\mu\\) with \\(\\beta_0\\).1 which should be avoided if the model will be used for out-of-sample prediction to avoid predicting values below 0\nThis makes it easy to compute the closed form solution, which is calculated by differentiating this function to produce the score function, and setting it to 0.\n\\[\n\\frac{\\partial l}{\\partial \\beta} = \\sum_{i = 1}^{n}{ \\frac{w_i}{\\phi}} (y_i \\beta_0^{-p} - \\beta_0^{1-p}) = 0\n\\]\n\\(\\phi\\) and \\(\\beta_0^{-p}\\) are non-zero constants, so can be pulled out of the summation and absorbed into the zero on the right-hand side to give\n\\[\n\\sum_{i = 1}^{n}{ w_i (y_i - \\beta_0) } = 0\n\\]\nFor glm(y ~ 1, weights = w, ...), this equals\n\\[\n\\sum_{i = 1}^{n} w_i y_i - \\beta_0 \\sum_{i = 1}^{n} w_i = 0 \\\\\n\\Rightarrow \\beta_0 = \\frac{\\sum_{i = 1}^{n} {w_i y_i}}{\\sum_{i = 1}^{n} w_i}\n\\]\nand for glm(y / w ~ 1, weights = w, ...), this equals\n\\[\n\\sum_{i = 1}^{n} w_i \\frac{y_i}{w_i} - \\beta_0 \\sum_{i = 1}^{n} w_i = 0 \\\\\n\\Rightarrow \\beta_0 = \\frac{\\sum_{i = 1}^{n} {y_i}}{\\sum_{i = 1}^{n} w_i}\n\\]"
  },
  {
    "objectID": "posts/tweedie-with-identity-link-and-offset/index.html#a-method-that-works-for-poisson-but-not-for-tweedie",
    "href": "posts/tweedie-with-identity-link-and-offset/index.html#a-method-that-works-for-poisson-but-not-for-tweedie",
    "title": "Specifying an offset in a Tweedie model with identity link",
    "section": "A method that works for Poisson but not for Tweedie",
    "text": "A method that works for Poisson but not for Tweedie\nWhat initially led me down this rabbit hole was finding this stackexchange post for specifying the offset with identity link in a poisson model, and naively (and incorrectly) fitting the Tweedie glm like this\n\nsummary(glm(y ~ w - 1, data = sim_data,\n            family = statmod::tweedie(var.power = 1.3, link.power = 1)))\n\n\nCall:\nglm(formula = y ~ w - 1, family = statmod::tweedie(var.power = 1.3, \n    link.power = 1), data = sim_data)\n\nCoefficients:\n  Estimate Std. Error t value Pr(&gt;|t|)    \nw 121.7328     0.4064   299.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 465.8885)\n\n    Null deviance:       NaN  on 1000000  degrees of freedom\nResidual deviance: 158130565  on  999999  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n\n\nand trying to figure out why this didn’t coincide with the ratio estimate of 114.5185, which is the case for the poisson models\n\n# ratio estimator\nsum(sim_data$y) / sum(sim_data$w)\n\n[1] 114.5185\n\n# generates warnings because we're passing a non-discrete response\n# point estimates are the same though\nunname(coef(suppressWarnings(glm(y ~ w - 1, family = poisson(link = \"identity\"), data = sim_data))))\n\n[1] 114.5185\n\nunname(coef(suppressWarnings(glm(y / w ~ 1, weights = w, family = poisson(link = \"identity\"), data = sim_data))))\n\n[1] 114.5185\n\n\nThis happens because substituting \\(\\mu_i = w_i \\beta_0\\) instead of \\(\\mu_i = \\beta_0\\) in the log-likelihood function\n\\[\nl(\\beta_0, \\phi, p | \\{y, x, w\\}_{i=1}^n) = \\sum_{i = 1}^{n}{ \\frac{w_i}{\\phi}} \\Bigg(y_i \\frac{(w_i \\beta_0)^{1-p}}{1-p} - \\frac{(w_i\\beta_0)^{2-p}}{2-p} \\Bigg) + log\\ a(y_i, \\phi / w_i, p)\n\\]\nleads to the following score function\n\\[\n\\frac{\\partial l}{\\partial \\beta} = \\sum_{i = 1}^{n} \\frac{w_i}{\\phi} (y_i w_i^{1-p} \\beta_0^{-p} - w_i^{2-p} \\beta_0^{1-p}) = 0\n\\]\nThe constant \\(\\phi\\) and \\(\\beta_0^{-p}\\) terms can be dropped from the equation, and the \\(w_i\\) outside the bracket are all 1 because no weights are passed to the glm call, so the following equation is solved\n\\[\n\\sum_{i = 1}^{n} y_i w_i^{1-p} - w_i^{2-p} \\beta_0 = 0\n\\]\nwhich can be simplified by pulling \\(w_i^{1-p}\\) as a common term\n\\[\n\\sum_{i = 1}^{n} w_i^{1-p} (y_i  - w_i \\beta_0) = 0\n\\]\nThis shows where the logical error happens, as well as how the correct estimate can be obtained, i.e., by passing weights = w ^ (p - 1) to the glm call, so that the \\(w_i^{1-p}\\) term cancels out\n\nsummary(glm(y ~ w - 1, weights = I(w^(1.3 - 1)), data = sim_data,\n            family = statmod::tweedie(var.power = 1.3, link.power = 1)))\n\n\nCall:\nglm(formula = y ~ w - 1, family = statmod::tweedie(var.power = 1.3, \n    link.power = 1), data = sim_data, weights = I(w^(1.3 - 1)))\n\nCoefficients:\n  Estimate Std. Error t value Pr(&gt;|t|)    \nw 114.5185     0.3655   313.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 492.3754)\n\n    Null deviance:       NaN  on 1000000  degrees of freedom\nResidual deviance: 177557174  on  999999  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 3\n\n\nNow the point estimates, standard errors, and dispersion parameters correspond to the model where the ratio estimator is correctly specified.\nHere’s the code and the plot of the score equations for the different models:\n\n\nCode\nparameter_grid &lt;- seq(50, 300, 5)\n\nestimating_equation_curves &lt;- tibble(\n  parameter = parameter_grid,\n   `Mean of ratios` = map_dbl(.x = parameter_grid, .f = ~ {\n    sim_data %&gt;%\n      mutate(s = w * (y - .x)) %&gt;%\n      pull(s) %&gt;%\n      sum()\n  }),\n  `Ratio of means` = map_dbl(.x = parameter_grid, .f = ~ {\n    sim_data %&gt;%\n      summarize(across(.cols = c(w, y), .fns = sum)) %&gt;%\n      mutate(s = y - (.x * w)) %&gt;%\n      pull(s)\n  }),\n  Incorrect = map_dbl(.x = parameter_grid, .f = ~ {\n    sim_data %&gt;%\n      mutate(s = (w ^ (1 - 1.3)) * (y - w * .x)) %&gt;%\n      pull(s) %&gt;%\n      sum()\n  })\n)\n\nestimating_equation_curves %&gt;%\n  pivot_longer(-parameter) %&gt;%\n  ggplot(aes(x = parameter, y = value, linetype = name)) +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype = \"dotdash\") +\n  # geom_vline(xintercept = 114.5185, linetype = 2) +\n  # geom_vline(xintercept = 121.7328, linetype = 1) +\n  # geom_vline(xintercept = 200, linetype = 11) +\n  theme_bw() +\n  ylab(\"Score function\") +\n  xlab(\"Parameter\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())"
  },
  {
    "objectID": "posts/tweedie-with-identity-link-and-offset/index.html#references",
    "href": "posts/tweedie-with-identity-link-and-offset/index.html#references",
    "title": "Specifying an offset in a Tweedie model with identity link",
    "section": "References",
    "text": "References\n\nYang et al 2016 paper and supplement (link)\nDelong et al 2021 paper (link)\nChapter 12 on Tweedie models from the Dunn and Smyth book on GLMs\n\nOthers\n\nZhang 2013 (link)\nStan code for Tweedie\n\nhttps://discourse.mc-stan.org/t/tweedie-likelihood-compound-poisson-gamma-in-stan/14636\nhttps://gist.github.com/MatsuuraKentaro/952b3301686c10adcb13"
  }
]